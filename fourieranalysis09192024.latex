\documentclass[12pt,a4paper,notitlepage]{article} 
\usepackage[margin=0.5in]{geometry}
\setlength{\parindent}{0cm}% Default is 15pt.
\setlength{\parskip}{1em}% paragraph spacing (vertical)
\renewcommand{\baselinestretch}{1.0}% line spacing (vertical)
\usepackage{float}
\usepackage{soulutf8}% get \ul command, prevents underlined text from exceeding margins.
\usepackage{mathrsfs}
\usepackage{yfonts}
\usepackage{amssymb}
\usepackage{eucal}% nicer mathcal font
\usepackage{mathtools}
\usepackage[dvipsnames]{xcolor}% additional named colors
\usepackage{bm}% boldface in mathmode with \boldsymbol
\usepackage{moreverb}% verbatim preserving indentation, ie: \begin{verbatimtab}[4], setting tab to 4 spaces
\DeclareMathOperator{\sinc}{sinc}% for operator names that should be printed upright.
\DeclareMathOperator{\disc}{disc}% for operator names that should be printed upright.
\DeclareMathOperator{\sgn}{sgn}% for operator names that should be printed upright.
\DeclareFontFamily{U}{wncy}{}% Sha function...
\DeclareFontShape{U}{wncy}{m}{n}{<->wncyr10}{}
\DeclareSymbolFont{mcy}{U}{wncy}{m}{n}
\DeclareMathSymbol{\Sh}{\mathord}{mcy}{"58}%... Sha function.
\usepackage{extarrows}% get x-arrows
\usepackage{tikz}
\usetikzlibrary{arrows}%
\usetikzlibrary{intersections}% get \path
\usepackage{pgfplots}
\pgfplotsset{compat=1.16}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{decorations.pathreplacing}% curly braces
\usepackage{empheq}% for multiple boxed aligned equations
\newcommand*\widefbox[1]{\fbox{\hspace{2em}#1\hspace{2em}}}% empheq box definition
\title{No Pedagogy: The Fourier Transform}
\author{Leo B. PeBenito}
\date{\today}

\usepackage{glossaries}% ***NOT WORKING***

\makeglossaries

\newglossaryentry{bandwidth}
{
        name=bandwidth,
        description={The bandwidth is the minimum frequency for which the following is true}
}

\begin{document}

\maketitle

\printglossary%***NOT WORKING***

\section{Preface}

  % Resources
\iffalse
% Trading View id: significantOwl60346 - pebenito.leoGmail
https://libgen.rs/ % download ANY text free
https://graphics.stanford.edu/~seander/bithacks.html % Bit Twiddling Hacks
https://www.dsprelated.com/freebooks/mdft/ % Stanford U. text on DFT for audo
https://web.archive.org/web/20060512020859/http://rkb.home.cern.ch/rkb/titleA.html - Data Analysis BriefBook
dsp.stackexchange.com -
https://ccrma.stanford.edu/~jos/mdft/ % blog on DFT and audio applications
https://www.youtube.com/watch?v=zgX6S7tEScI % lect. on Dirac delta
Mike X Cohen youtube channel
http://www.dspguide.com/pdfbook.htm - Ebook by Steven W. Smith on DSP
https://www.youtube.com/user/allsignalprocessing - Barry Van Veen U. Wisconsin Madison
https://www.youtube.com/user/dLabRoboticsMIT/videos -
https://en.wikipedia.org/wiki/User:LucasVB/Gallery - great animations that teach wave concepts
http://www.industrial-electronics.com/DAQ/understngng_dsp_3.html - dsp shiz
http://www.industrial-electronics.com/DAQ/Books/index.html - amazing vault of dsp/sensor resources!!!
http://www.industrial-electronics.com/DAQ/Books/Sensors.html - Sensors!

% How to setup latex doc
https://www.youtube.com/watch?v=zqQM66uAig0

Data:
Edward Tufte (CHECK HIM OUT!!!, 9/17/2022) - Graphics Press, *tufte-book document class* (Latex package), data visualization master
wiki data
https://www.datafinnovation.com/
https://popular.info/ - Popular Information (Inde Journalism)
https://data-science.llnl.gov/open-data-initiative

Prediction Models:
tspDB - MIT based time series prediction built into postgreSQL !!!
  - https://news.mit.edu/2022/tensor-predicting-future-0328
  - https://tspdb.mit.edu/

William Wu - Wu riddles web site
- worked with Brad Osgood on sampling theory

Look Into:
- Paley-Wiener theorem

\fi

  No Pedagogy means do everything! show everything! No claims about what I know, just telling what I think and trying to be as upfront about where my knowledge ends.

  By proceeding naively you are incurring more technical debt than you may realize and only be able to implement the Fourier transform once in a specific setting but untimately have to start over when applying the Fourier transform in a new context.

  AHEAD: What is up with Finite Element Methods (Gilbert Strang's first book is on this) and the Fourier transform?...

  This document is meant to ...
  you don't have to sit through the sound of a chalk board and try to take notes from illegible penmanship (Osgood lectures).
  Math equations should never be interupted by a figure (p.24\cite{BriggsHenson}).
  I do not like punctuation in-line with fomulas and math expressions when they are given the importance of their own line, this is not standard, periods and commas often appear.
  I hate how Briggs and Henson reverse the order of frequency and position in the FT when refering to indices in the DFT, ie:
  $$\sum e^{-i2\pi \omega_k x_n} = \sum e^{-i2\pi k n /N}$$

  Prioretize consistency over aesthetics, ie:
  writing the Fourier transform as 
  $$F_k = \int f(x) e^{-i2\pi \omega x}\,dx$$
  and writing the DFT as 
  $$F_k = \sum_n f_n e^{-i2\pi n k/N}$$
  swapping the order of the variables $\omega$ and $x$ when referring to the respective indices for frequency and position in the discrete Fourier transform, $k$ and $n$. 

  Will attempt to keep things ``self-contained'', will not extend things over multiple chapters, giving too little information too soon only to reveal an anticlimactic punchline.


  This repo will live and die based on the value of its documentation - that is to say, I am no expert!
  My goal is that someone with the patience to read the documentation will be able to procede with the code with confidence, and maybe even be curious enough to look at a reference.

  No pedagogy:
  Nothing is being intentionally withheld.
  The order in which topics are introduced should be logical enough to make transitions seem obvious.
  Concepts are built up incrementally without skipping steps (if something is missing it's because I don't understand it, yet!) - let the reader skip ahead if she is impatient to see the significant results.


  The lead of to one of the most important formulas in the book (eqn. 6.6, p.191 \cite{BriggsHenson}) is ``a short calculation (problem 125) reveals that the Fourier coefficients of $f$ on $[-pA/2, pA/2]$ in the Appendix shows that the $pN$-point DFT is given by''
  This sentence lacks clarity and defers the derivation/explanation to the reader (as a problem).


\section{Fourier Series}

  {\bf Must make executive decision: must get away from Osgood notation.  Must get away from using $k$ to index spatial modes.  This leads to conflict with other notation which uses $k$ to index frequency, especially when introducing the DFT!...}
  {\bf This Tag is placed everywhere where the notation has not been updated to use $n$ in place of $k$}
  {\bf But before you go down this path... think, $k$ is being used to index frequency/wavelength here.  Later $n$ will be used to index space/time in the DFT.}

  The derivation of the Fourier transform begins with Fourier series.
  Dirichlet's Theorem\cite{DJG} stated by Bakefi and Barret: Any periodic, single valued function $f(x)$ defined in the interval $[-\pi, \pi]$ and having a periodicty of $2 \pi$ can be represented by the trigonometric series:
\begin{equation}
  f(x) = { A_0 \over 2 } + \sum_{k=1}^\infty \Bigl( A_k \cos ( k x ) + B_k \sin ( k x ) \Bigr)
\end{equation}
  where $x$ is dimensionless, and $k \in \mathbb{Z}$.

  {\bf Should I change $k$ to $n$?... $k$ is used lated to denote something completely different.}

  Fourier analysis is concerned with finding the coefficients associated with the terms in the series.
  Consider the first term $A_0 \over 2$, which amounts to a vertical shift, and corresponds to the average value of the function being represented by the Fourier series.
  To obtain $A_0$, and to understand the reason of the factor of $1 \over 2$, integrate the function $f(x)$ 

  $$\int\limits_{- \pi}^{\pi} f(x) \,d x$$

  All the sine and cosine terms will cancel because they are integrated over the full domain of their periodicity, leaving an integral over a complete period of $A_0$, which produces a factor of $2\pi$

  $$= \int\limits_{-\pi}^\pi {A_0 \over 2} \,dx = {A_0 \over 2} 2 \pi = A_0 \pi$$

  Fourier's trick, as it's referred to, amounts to integrating $f(x)$ and dividing by $\pi$ to get the Fourier coefficient, here $A_0$

  $$A_0 = { 1 \over \pi } \int\limits_{-\pi}^\pi f(x) \,dx$$

  This procedural structure is maintained when obtaining the amplitudes for the sine and cosine terms.
  For the sine terms first.  

  $$\int\limits_{- \pi}^\pi f(x) \sin ( k x ) \,d x = \int\limits_{ - \pi }^\pi \sin^2 (k x ) \,d x$$

  Note:
  $$\int\limits_{- \pi}^\pi \sin ( k x ) \sin ( m x ) \,d x = \delta_{km}$$

  Recall:
\begin{align*}
  e^{i \theta} e^{i \theta} &= e^{i 2 \theta} = \cos ( 2 \theta ) + i \sin ( 2 \theta ) \\
  &= \bigl( \cos ( \theta ) + i \sin ( \theta ) \bigr) \bigl( \cos ( \theta ) + i \sin ( \theta ) \bigr)
  = \cos^2 ( \theta ) - \sin^2 ( \theta ) + i 2 \cos ( \theta ) \sin ( \theta ) \\
\end{align*}

  Combining real parts, and using $ 1 = \sin^2 ( \theta ) + \cos^2 ( \theta ) $

  $$ \cos ( 2 \theta ) = \cos^2 ( \theta ) - \sin^2 ( \theta ) = \cos^2 ( \theta ) - \bigl( 1 - \cos^2 ( \theta ) \bigr) $$

  So,
  $$ \cos^2 ( \theta ) = \biggl( { 1 + \cos ( 2 \theta ) \over 2 } \biggr) $$

  By the same token,
  $$ \cos ( 2 \theta ) = \cos^2 ( \theta ) - \sin^2 ( \theta ) = \bigl( 1 - \sin^2 ( \theta ) \bigr) - \sin^2 ( \theta ) $$

  $$ \sin^2 ( \theta ) = \biggl( { 1 - \cos ( 2 \theta ) \over 2 } \biggr) $$
\vspace{5mm}

  Now apply this to Fourier's trick
  $$ \int\limits_{ - \pi }^\pi \,d x \, f ( x ) \sin ( k x ) = \int\limits_{ - \pi }^\pi \,d x \, \sin^2 ( k x ) = \int\limits_{ - \pi }^\pi \,d x \, \biggl( { 1 - \cos ( 2 k x ) \over 2 } \biggr) $$

  The $\cos$ term vanishes, being integrated over its domain of periodicity.  What remains is 
  $$ { 1 \over 2 } \int\limits_{ - \pi }^\pi \,d x = { 2 \pi \over 2} = \pi  $$

  In the same manner, 
  $$ \int\limits_{ - \pi }^\pi \,d x \, \cos^2 ( k x ) = \int\limits_{ - \pi }^\pi \,d x \, \biggl( { 1 + \cos ( 2 k x ) \over 2 } \biggr) $$

  The $ \cos $ term vanishes, being integrated over its domain of periodicity, leaving
  $$ { 1 \over 2 } \int\limits_{ - \pi }^\pi \,d x = { 2 \pi \over 2 } = \pi $$

  SIDE: Perhaps it is worth noting the factor of $ 1 \over 2 $ here as it helps remember the factor of $ 1 \over 2 $ of $ B_0 $, but in this case the $  1 \over 2  $ comes from the definition of $ \sin $ and $ \cos $ .

  The amplitudes of the individual terms can be gotten by 
  $$ A_m = { 1 \over \pi } \int\limits_{ - \pi }^\pi \,d x \, f(x) \cos ( m x ) \qquad
  B_m = { 1 \over \pi } \int\limits_{ - \pi }^\pi \,d x \, f(x) \sin ( m x ) $$


\subsection{Fourier Series Using Complex Exponentials}

  It is preferrable to express the infinite series in terms of complex exponentials rather than sines and cosines, given 
  $$\cos(\theta) = {e^{i \theta} + e^{- i \theta} \over 2} \qquad
  \sin(\theta) = {e^{i \theta} - e^{- i \theta} \over 2 i}$$
  Then $f(x)$ becomes,
\begin{align*}
  f(x) &= { A_0 \over 2 } + \sum_{ k \, = \, 1 }^\infty \biggl( A_k { e^{i \, k x}  + e^{- i \, k x} \over 2 } +  B_k { e^{i \, k x} - e^{ - i \, k x } \over 2 i } \biggr) \\
  &= { A_0 \over 2 } + \sum_{ k \, = \, 1 }^\infty \biggl[ \biggl( { A_k \over 2 } + { B_k \over 2 i } \biggr) e^{ i \, k x} +  \biggl( { A_k \over 2 } - { B_k \over 2 i } \biggr) e^{ - i \, k x} \biggr] \\
\intertext{Simplify the coefficients, given $i^2 = -1$, and $-i^2 = 1$, by writing
  $${1\over i} = {-i^2\over i} = -i \qquad\text{, and}\quad {-1\over i} = {i^2\over i} = i$$}
  &= { A_0 \over 2 } + {1\over2}\sum_{ k \, = \, 1 }^\infty \biggl( A_k - i B_k \biggr) e^{i \, k x} +  {1\over2}\sum_{ k \, = \, 1 }^\infty \biggl( A_k + i B_k \biggr) e^{- i \, k x } \\
\intertext{Inverting the index of the latter sum to go from $-1$ to $-\infty$}
  &= { A_0 \over 2 } + {1\over2}\sum_{ k \, = \, 1 }^\infty \biggl( A_k - i B_k \biggr) e^{i \, k x} +  {1\over2}\sum_{ k \, = \, -1 }^{-\infty} \biggl( A_{_{-k}} + i B_{_{-k}} \biggr) e^{i \, k x}\\
\intertext{allows the two sums to be combined into a single sum over their combined indices.
  Written in this way it is easy to see the familiar form of the Fourier series written as a sum from $-\infty$ to $\infty$ of complex exponentials }
  &= \sum_{ k \, = \, - \infty }^\infty C_k e^{i k x}
	\qquad \text{, where} \quad
    	\begin{cases}
			C_0 = A_0 / 2 & \text{, if $k = 0$} \\
			C_k = {1\over2}\bigl( A_k - i B_k \bigr) & \text{, if $k > 0$} \\
			C_k = {1\over2}\bigl( A_{_{-k}} + i B_{_{-k}} \bigr) & \text{, if $k < 0$}
    	\end{cases}\\
\end{align*}
  Clearly, $C_k = \overline{ C }_{ -k }$ (where the bar notation indicates complex conjugate).
  While this formula is more compact, \ul{\bf keep in mind both positive and negative indices, $k$ and $-k$ are needed to describe a given (the $k$-{\em th}) mode}.


  \subsection{Fourier Coefficients (scaling the Fourier series)}

  It is natural to consider the interval $[-\pi, \pi]$ from a mathematical perspective.
  A shift in the domain to $[0, 2 \pi]$ has no affect on the above treatment.
  From a physical perspective it is more natural to consider the interval from 0 to $\lambda$ (the implicit wavelenth associated with the spatial extent), or from 0 to $L$, the length of time sampled.
  The variable must then be scaled by $2 \pi /\lambda$ such that
  $$f(x) = \sum_{k \, = \, - \infty}^\infty c_k e^{i 2 \pi k x / \lambda}$$
  Applying Fourier's trick now involves an integral over the wavelength
\begin{align*}
  \int\limits_0^\lambda \,d x \, f ( x ) \exp \biggl(-{i 2 \pi m x \over \lambda } \biggr)
  &= \int\limits_0^\lambda \,d x \, \sum_{ k \, = \, - \infty }^\infty c_k \exp \biggl( {i2 \pi k x \over \lambda } \biggr) \exp \biggl(-{i 2 \pi m x \over \lambda } \biggr) \\
  &= \int\limits_0^\lambda \,d x \, c_k \exp \biggl( {i2 \pi (k - m) x \over \lambda } \biggr) \\
  &= \begin{cases}
		0 & \text{if $k\ne m$} \\
		\int\limits_0^\lambda c_k \,dx = c_k (\lambda)  & \text{if $k=m$} \\
	\end{cases}
\end{align*}
  For $ k \!\neq\! m $ the above equation equals 0.
  For $ k \!=\! m $ the above equation is just an integral over the wavelength.
  The Fourier coefficient for each index $k$ is obtained by
  $$\boxed{c_k = {1 \over \lambda} \int\limits_0^\lambda \,d x \, f ( x ) \exp \biggl( -{i 2 \pi k x \over \lambda } \biggr)}$$
  We can just as easily consider the symmetric interval over the wavelength
  $$c_k = {1 \over \lambda} \int\limits_{-\lambda / 2}^{\lambda / 2} \,d x \, f(x) \exp \biggl( -{i 2 \pi k x \over \lambda } \biggr)$$
  More generally, simply in terms of some length ($L$), ie: of the sample duration
  $$\boxed{c_k = {1 \over L} \int_L \,d x \, f(x) \exp \biggl( -{i 2 \pi k x \over L} \biggr)}$$


\section{Derivation of the Fourier Transform}

  The derivation of the Fourier transform is based on extending the boundaries over which the Fourier series is defined, and thereby the boundaries of the Fourier analysis integral, to $\pm\infty$.
  The trick to performing this is to consider the interval that separates the modes, ie: define
  $$\omega = {2 \pi k \over \lambda} \quad\longrightarrow\quad \triangle\omega = {2 \pi \over \lambda}$$
  $$c_k = {\triangle\omega \over 2 \pi} \int\limits_{-\pi / \triangle\omega}^{\pi / \triangle\omega} \,d x \, f(x) \exp ( - i \omega x )$$

  The Fourier series expansion can now be written as
\begin{align*}
  f(x) &= \sum_{\omega = - \infty}^{\infty} \Biggl( {\triangle\omega \over 2 \pi} \int\limits_{-\pi / \triangle\omega}^{\pi / \triangle\omega} \,d x \, f(x) \exp ( - i \omega x ) \Biggr) \exp ( i \omega x ) \\
\intertext{Taking the limit}
  &= \lim\limits_{\substack{\triangle\omega \,\rightarrow \,0 \\ (\lambda \,\rightarrow \,\infty)}} \; \sum_{\omega \,= \,-\infty}^{\infty} \Biggl( {\triangle\omega \over 2 \pi} \int\limits_{-\pi / \triangle\omega}^{\pi / \triangle\omega} \,d x \, f(x) \exp ( - i \omega x ) \Biggr) \exp ( i \omega x ) \\
\intertext{At this stage the factor of $1/ 2 \pi$ can be distributed}
  &= {1 \over \sqrt{2 \pi}} \int\limits_{- \infty}^{\infty} \underbrace{\Biggl( {1 \over \sqrt{2 \pi}} \int\limits_{-\infty}^{\infty} f(x) \exp ( - i \omega x ) \,d x \Biggr)}_{\text{\large$F(\omega)$}} \exp ( i \omega x ) \,d \omega \\
\end{align*}

  The Fourier transform of $f(x)$ and inverse Fourier transform are
\begin{subequations}
\begin{empheq}[box=\widefbox]{align}
  F(\omega) &= {1 \over \sqrt{2 \pi}} \int\limits_{-\infty}^{\infty} f(x) \exp( - i \omega x ) \,d x \\
  f(x) &= {1 \over \sqrt{2 \pi}} \int\limits_{-\infty}^{\infty} F(\omega) \exp(i \omega x) \,d \omega
\end{empheq}
\end{subequations}

  It is also possible to not distribute the factor of $1/2\pi$, such that sometimes the Fourier transform and inverse Fourier transform are written as
\begin{subequations}
\begin{empheq}[box=\widefbox]{align}
  F(\omega) &= {1 \over 2 \pi} \int\limits_{-\infty}^{\infty} f(x) \exp( - i \omega x ) \,d x \\
  f(x) &= \int\limits_{-\infty}^{\infty} F(\omega) \exp(i \omega x) \,d \omega
\end{empheq}
\end{subequations}

  A way to remember which expression gets the minus sign in the exponential is based on which expression is intended to model the original function (synthesis) and which is intended to obtain the Fourier coefficients (analysis).
  The expression for getting the Fourier coefficients has the minus sign in the exponential.
  That is, the way to get the Fourier coefficients involves multiplying by the complex conjugate.

  The treatment by Brad Osgood (Jerri also puts $2\pi$ in the exponential) in terms of time $t$ is nice too, and {\bf the factor of $2\pi$ stays in the exponential}.
  It simplifies construction of the Fourier series by considering the signal over a period of length 1, such that
  $$ f(t) = \sum_{k = -\infty}^{\infty} c_k e^{i2 \pi k t} $$

  The Fourier coefficients are got somewhat more directly without the need for any prefactor as
  $$ c_k = \int\limits_0^1 f(t) e^{- i2 \pi k t} $$

  To get to the Fourier transform cosider a period $T$ that encompases the signal.
  $$ f(t) = \sum_{k = -\infty}^{\infty} c_k e^{i2 \pi k t / T} $$
  $$ c_k = {1\over T} \int\limits_0^T f(t) e^{- i2 \pi k t / T} $$

  The latter can be rewritten over the symmetric interval
  $$ c_k = {1\over T} \int\limits_{-T/2}^{T/2} f(t) e^{- i2 \pi k t / T} $$

  Similarly as before, define
  $$ s = {k\over T} \rightarrow \triangle s = {1 \over T} $$ 

  Reasoning similarly as before
\begin{align*}
  f(t) &= \lim\limits_{\substack{\triangle s \,\rightarrow \,0 \\ (T \,\rightarrow \,\infty)}} \; \sum_{s \,= \,-\infty}^{\infty} \Biggl( \triangle s \int\limits_{-1 / \triangle s}^{1 / \triangle s} \,d t \, f(t) e^{- i2 \pi s t } \Biggr) e^{i2 \pi s t } \\
  &= \int\limits_{-\infty}^{\infty} \underbrace{\int\limits_{-\infty}^{\infty} \,d t \, f(t) e^{- i2 \pi s t }}_{\text{\large$F(s)$}} e^{i2 \pi s t } \,d s \\
\end{align*}

  The Fourier transform analyzes a signal into its component parts
  $$\boxed{(\mathscr{F}\!f)(s) = \mathscr{F}\!f = F(s) = \hat{f}(s) = \int\limits_{-\infty}^{\infty} \,d t \, f(t) e^{- i2 \pi s t }}$$
  and is often thought of in the frequency domain.
  In general it is necessary to understand the convergence of the integral.
  The set of $s \in \mathbb{R}$ for which $\mathscr{F}(f(t))$ is defined is called the spectrum.
  The inverse Fourier transform synthesizes the function from its constituent parts
  $$\boxed{\mathscr{F}^{^{-1}}\!F = f(t) = \int\limits_{-\infty}^{\infty} \,d s \, F(s) e^{i2 \pi s t }}$$
  and is often thought of in the time domain.
  However, the Fourier transform is a sufficiently flexible tool that it is often advantageous to abandon such references.

  Notice the relationship between the Fourier transform and the Fourier coefficients using scaled Fourier series for $f$ defined over the region $L$
  $${F(s_k)\over L} = {\hat{f}(s_k)\over L} = c_k = {1\over L} \int_L \,dx f(x) e^{-i2\pi s_k x}$$
  This relationship is a little subtle (can be confusing, ie: if you thought the Fourier transform gave the Fourier coefficients directly, given that the Fourier coefficients furnished the starting point and a fairly direct route for the derivation of the Fourier transform; but the relationship is clear just by comparing the formulas directly), and \ul{reoccurs with the discrete Fourier transform (DFT) and essentially shows that the DFT is actually a direct approxination to the Fourier coefficients} (despite the nomenclature).


  \subsection{The Many Defintions of the Fourier Transform}

  For time, $(t)$, in seconds and frequency, $(f)$, in cycles per second (Hertz) (p. 601, ref. Numerical Methods) the Fourier transform equations are
  $$H(f) = \int_{-\infty}^\infty h(t) e^{i2\pi f t} \,d t$$
  $$h(t) = \int_{-\infty}^\infty H(f) e^{-i2\pi f t} \,d f$$

  Using angular frequency, $\omega = 2\pi f$, in radians per second \cite{NumericalRecipes}
  $$H(f) = \int_{-\infty}^\infty h(t) e^{i \omega t} \,d t$$
  $$h(t) = {1\over 2\pi} \int_{-\infty}^\infty H(\omega) e^{-i \omega t} \,d \omega$$
  But this form is not preferred because of the need to keep track of the factor of $(2\pi)^{-1}$.

 
  \subsection{Basic Properties of the Fourier Transform}

  The Fourier transform is a linear operator.

  Prove these relationships between the function and its Fourier transform.
  These symmetries can be used to increase computational efficiency\cite{NumericalRecipes}.
\begin{center}
\begin{tabular}{| l | l |}
\hline
  $h(t)$ real			& $H(-f) = |H(f)|^*$	\\
  $h(t)$ imaginary		& $H(-f) = -\vert H(f) \vert^*$	\\
  $h(t)$ even			& $H(f)$ even			\\
  $h(t)$ odd			& $H(f)$ odd			\\
  $h(t)$ real even		& $H(f)$ real even	\\
  $h(t)$ real odd		& $H(f)$ imaginary odd	\\
  $h(t)$ imaginary even	& $H(f)$ imaginary even	\\
  $h(t)$ imaginary odd	& $H(f)$ real odd	\\
\hline
\end{tabular}
\end{center}

  \subsection{Fourier Inversion and the Dirac $\delta$}

  Recall: the integral representation of the Dirac $\delta$-function using complex exponentials
  $$\delta(x) = {1\over2\pi}\int\limits_{-\infty}^\infty e^{\pm ikx} \,dx$$

  Fourier inversion refers to the process by which the inverse Fourier transform of the Fourier transform, and the Fourier transform of the inverse Fourier transform both return the function
\begin{align*}
  \mathscr{F}^{^{-1}}(\mathscr{F}(f(t))) = f(t) &= \mathscr{F}(\mathscr{F}^{-1}(f(t))) \\
  &= \int\limits_{-\infty}^\infty e^{-i2\pi st}
     \int\limits_{-\infty}^\infty e^{+i2\pi st^\prime} f(t^\prime) \,dt^\prime \,ds \\
  &= \int\limits_{-\infty}^\infty \,dt^\prime f(t^\prime)
     \int\limits_{-\infty}^\infty \,ds \, e^{+i2\pi s(t^\prime-t)} \\
  \intertext{The right integral expression is a complex exponential form (/integral representation) of the {\bf Dirac $\delta$-function} (which is of course symmetric and the argument variables can swap possition).}
  &= \int\limits_{-\infty}^\infty \,dt^\prime f(t^\prime) \, \delta(t^\prime-t) = f(t) \\
\end{align*}


  \section{Properties of the Fourier Transform}

  \subsection{Duality}
  $$\boxed{ \mathscr{F}(f^-) = \mathscr{F}(f(-t)) = (\mathscr{F}f)^- = (\mathscr{F}f)(-s) = \mathscr{F}^{^{-1}}f }$$

  The {\em reverse} signal defined as 
  $$f^{reversed} = f^{-} = = f^{(-)} = f(-t)$$
  provides a way to express if a function is even or odd
\begin{align*}
	f &= f^{(-)} \qquad \text{``even''} \\
	{-}f &= f^{(-)} \quad \text{``odd'' }
% parentheses brings minus-sign closer to 'f'
\end{align*}
  In practice this amounts to processing the signal in reverse.

  The reverse of the Fourier transform is the inverse Fourier transform
\begin{align*}
  (\mathscr{F}f)^- = F(-s) &= \int\limits_{-\infty}^{\infty} f(t) e^{-i2\pi (-s) t} \,d t = \int\limits_{-\infty}^{\infty} f(t) e^{+i2\pi s t} \,d t = \mathscr{F}^{-1}f \\
\intertext{, which is why $\mathscr{F}$, and $\mathscr{F}^{^{-1}}$, are sometimes referred to as the forward, and reverse Frourier transform, respectively.}
\intertext{Associating the minus sign introduced by ($-s$) into the argument of the exponential with ($t$)}
  &= \int\limits_{-\infty}^{\infty} f(t) e^{-i2\pi s (-t)} \,d t \\
\intertext{and performing the $u$-substitution, $\begin{cases} u &= -t \\ \,d u &= -\,dt \end{cases}$}
  & = - \int\limits_{\infty}^{-\infty} f(-u) e^{-i2\pi s u} \,d u = \int\limits_{-\infty}^{\infty} f(-u) e^{-i2\pi s u} \,d u \\
  &= \mathscr{F}(f(-u)) = \mathscr{F}(f(-t)) \qquad\text{(since $u$ is just a dummy variable)}
\end{align*}
  shows \ul{the Fourier transform of the reverse signal is equal to the inverse Fourier transform}.
  
  The Fourier transform of the Fourier transform returns the reverse signal
  $$\boxed{ \mathscr{F} \mathscr{F} f = f^- }$$
\begin{align*}
  \mathscr{F}(\mathscr{F}(f(t)))
  &= \int\limits_{-\infty}^\infty e^{-2\pi ist} \,dt
     \int\limits_{-\infty}^\infty e^{-2\pi ist^\prime} f(t^\prime) \,dt^\prime \\
  &= \int\limits_{-\infty}^\infty f(t^\prime) \,dt^\prime
     \int\limits_{-\infty}^\infty e^{+2\pi is(-t-t^\prime)} \,dt \\
  &= \int\limits_{-\infty}^\infty f(t^\prime) \delta(-t-t^\prime) \,dt^\prime = f(-t)
\end{align*}

  This makes it simple to solve some otherwise difficult Fourier transforms.


  \subsection{The Shift (Delay) Theorem:} %(Lecture 8) Delays(shifted), stretches, and convolution

  {\bf Do this for frequency shift as well}
  $$h(t) e^{-i2\pi f_0 t} \Longleftrightarrow H(f-f_0)$$

  $$\boxed{f(t \pm b) \xlongleftrightarrow[\text{$\mathscr{F}^{^{-1}}$}]{\text{$\mathscr{F}$}} e^{\pm i2\pi s b}\,F(s)}$$

  The Fourier transform of a delayed signal is
\begin{align*}
  \mathscr{F}(f(t-b)) &= \int\limits_{-\infty}^{\infty} e^{-i2\pi s t} f(t - b) \,d t
  \intertext{Perform $u$-substitution, $u = t - b$, $\,d u = \,d t$, $t = u + b$}
  &= \int\limits_{-\infty}^{\infty} e^{-i2\pi s (u + b)} f(u) \,d u \\
  &= e^{-2\pi i s b} \int\limits_{-\infty}^{\infty} e^{-i2\pi s u} f(u) \,d u \\
  &= e^{-i2\pi s b} F(s)
\end{align*}

  The physical interpretation of this is a shift in time corresponds to a phase shift in frequency.
  Recall, the Fourier transform is a complex number with a magnitude and a phase
  $F(s) = |F(s)|e^{i2\pi \Theta (s)}$
  , so
  $e^{-2\pi i s b} F(s) = |F(s)|e^{i2\pi (\Theta (s) - s b)}$.


  \subsection{The Scaling (Stretch/Similarity) Theorem}

  {\bf Do this for frequency as well}
  $${1\over|b|} h({t\over b}) \Longleftrightarrow H(bf)$$

  $$\boxed{ f(\alpha t) \xlongleftrightarrow[\text{$\mathscr{F}^{-1}$}]{\text{$\mathscr{F}$}} {1\over|\alpha|} F\Bigl({s\over\alpha}\Bigr) }$$

  The Fourier transform of the scaled signal is
\begin{align*}
  \mathscr{F}(f(\alpha t)) &= \int\limits_{-\infty}^\infty e^{-i2\pi st} f(\alpha t) \,dt \\
  \intertext{For $\alpha > 0$, the $u$-substitution, $u = \alpha t$, gives}
  &= \int\limits_{-\infty}^{\infty} \exp\biggl(-i2\pi s \Bigl({u\over\alpha}\Bigr)\biggr) f(u) {\,du \over \alpha}
  = {1\over \alpha} \int\limits_{-\infty}^{\infty} \exp\biggl(-i2\pi \Bigl({s\over\alpha}\Bigr) u \biggr) f(u) \,du\\
  \intertext{For $\alpha < 0$, this $u$-substitution effectively swaps the limits of integration}
  &= {1\over \alpha} \int\limits_{+\infty}^{-\infty} e^{-i2\pi (s/\alpha) u} f(u) \,du \\
  \intertext{To recover the original limits of integration introduces a minus sign.  But $\alpha$ is negative in this case, so}
  &= {-1\over \alpha} \int\limits_{-\infty}^{\infty} e^{-i2\pi (s/\alpha) u} f(u) \,du
  = {1\over |\alpha|} F(s/\alpha)
\end{align*}

  The physical interpretation of this is,
  if $|\alpha| > 1$ then $f$ gets squeezed, and $|F(s)|$ gets stretched horizontally and squashed vertically.
  Whereas, if $|\alpha| < 1$ then $f$ gets stretched, and $|F(s)|$ gets squeezed horizontally and stretched vertically.
  This reciprocal relationship shows that {\em a signal cannot be simultaneously localized in time and in frequency}.

  ***DO THIS: What happens when you combine shifts and stretches?***


  \subsection{The Convolution Theorem}

  %(probably the most important operation in signal processing)
  %Warm up to convolution:
  %Basic question in all signal procesing is how to use one signal (function) to modify another.
  %Most often one looks to modify the spectrum of a signal.
  %\begin{center}
\begin{table}[H]
  \centering
  \renewcommand{\arraystretch}{1.5}% stretch vertical space in array environment
  $\begin{array}{| r c l |}
	\hline
  	\mathscr{F}(f) \cdot \mathscr{F}(g) & = & \mathscr{F} (f * g) \\ %[0.5ex]
  	\mathscr{F}(f) * \mathscr{F}(g) & = & \mathscr{F} (f \cdot g) \\ %[0.5ex]
  	\mathscr{F}^{-1}(f) \cdot \mathscr{F}^{-1}(g) & = & \mathscr{F}^{-1} (f * g) \\ %[0.5ex]
  	\mathscr{F}^{-1}(f) * \mathscr{F}^{-1}(g) & = & \mathscr{F}^{-1} (f \cdot g) \\ %[0.5ex]
	\hline
  \end{array}$
  \renewcommand{\arraystretch}{1.0}% reset array environment
  \caption{Summary of important results.  The main take away is, colloquially, the Fourier transform (and inverse Fourier transform) ``swaps convolution and multiplication''.}
\end{table}
  %\end{center}

  The linearity of the Fourier transform 
\begin{align*}
  \mathscr{F}(f + g) &= \mathscr{F}(f) + \mathscr{F}(g) \\
  \mathscr{F}(\alpha f) &= \alpha \mathscr{F}(f)
\end{align*}
  shows how addition and scalar multiplication of signals influences the Fourier transform.
  The product of two signals has a more dramatic impact on the Fourier transform.

  The best known and most usefull expression involving multiplication of signals and the Fourier transform is given by the \underline{Fourier transform of the convolution}
  $$\boxed{F(s) G(s) = \mathscr{F} (f * g)}$$
  which is the product of the Fourier transforms.
  This expression describes application of a filter according to frequency.
  %(***Would it be beneficial to think of this in the operator sense, as in multiplication from the left?***)

  Proof:

  Differentiate $t$ variables of the Fourier transforms
\begin{align*}
  (F(s))(G(s)) &= \mathscr{F}(f) \mathscr{F}(g) \\
  &= \Biggl(\int_{-\infty}^\infty e^{-2\pi i s t_1} f(t_1) \,d t_1 \Biggr)
  \Biggl(\int_{-\infty}^\infty e^{-2\pi i s t_2} g(t_2) \,d x_2 \Biggr) \\
  \intertext{There are two {\em decoupled} variables so the integrals can be combined and the components rearranged}
  &= \int_{-\infty}^\infty \int_{-\infty}^\infty
  e^{-2\pi i s (t_1+t_2)} f(t_1) g(t_2) \,d t_1 \,d t_2 \\
  \intertext{Introducing a change of variables $t = t_1 + t_2$, and $t_2 = t - t_1$
  $$\det(\boldsymbol{\text{J}}) = \Biggl|{\partial (t_1, t_2)\over \partial (t_1, t)}\Biggr|
  =
  \renewcommand{\arraystretch}{1.4}
  \begin{vmatrix}
  {\partial t_1\over \partial t_1} & {\partial t_1\over \partial t} \\
  {\partial t_2\over \partial t_1} & {\partial t_2\over \partial t}
  \end{vmatrix}
  \renewcommand{\arraystretch}{1}
  = \begin{vmatrix}
  1  & 0 \\
  -1 & 1
  \end{vmatrix} = 1$$
  The Jacobian of the transformation is $1$, and the exponential is separable, such that the integral can be rewritten as}
  &= \int_{-\infty}^\infty
	e^{-2\pi i s t}
	\underbrace{\Biggl(
		\int_{-\infty}^\infty f(t_1) \, g(t-t_1) \,dt_1
	\Biggr)}_{\text{$(f * g)$}}
	\,dt \\
  \intertext{The integral in parentheses is the convolution of $f$ and $g$, $(f * g)$}
  &= \int\limits_{-\infty}^\infty e^{-2\pi i s t} \, (f * g) \,dt = \mathscr{F}(f * g) \\
  \intertext{(Note the limits of integration range from $(-\infty, \infty)$ unlike the convolution as written in its familiar form in connection with the Laplace transform, which has limits of integration from $0$ to $t$.)}
\end{align*}
  %***...come back: time and space invariance of convolution, modulation theorem, ...
  As a means of self check, given the result
  $$F(s) \, G(s) = \mathscr{F}(f * g)$$
  the signal can be recovered by Fourier inversion
  $$\boxed{\mathscr{F}^{-1}((F(s))(G(s))) = (f * g)}$$
  as follows
  $$\mathscr{F}^{-1}((F(s))(G(s))) = \mathscr{F}^{-1}\mathscr{F}(f * g) = (f * g)$$
  To demonstrate this, write the convolution as an explicit function of $t$
  $$(f * g) = (f * g)(t) = \int\limits_{-\infty}^\infty f(t^\prime) g(t-t^\prime) \,dt^\prime$$
\begin{align*}
  \mathscr{F}^{-1}\mathscr{F}(f*g) &= \int\limits_{-\infty}^\infty e^{i2\pi st}
  \int\limits_{-\infty}^\infty e^{-i2\pi st} (f*g)\,ds \,dt \\
  \intertext{Distinguish the $t$-variable of the Fourier transform as $t^\prime$ (time in one expression need not be correlated with time in another expression)}
  &= \int\limits_{-\infty}^\infty (f*g)
  \int\limits_{-\infty}^\infty e^{i2\pi s(t-t^\prime)} \,ds \,dt^\prime\\
  &= \int\limits_{-\infty}^\infty (f*g) \, \delta(t-t^\prime) \,dt^\prime = (f*g)(t)
\end{align*}

  The \underline{convolution of Fourier transforms} is
  $$\boxed{\mathscr{F}(f) * \mathscr{F}(g) = \mathscr{F}(f\cdot g)}$$
  To see this write Fourier transforms in terms of the ($s$) variables
  $$\mathscr{F}(f) * \mathscr{F}(g) = F(s) * G(s)$$
  which by definition is
  $$= \int\limits_{-\infty}^\infty F(s') G(s-s') \,ds'$$
  $$= \int\limits_{-\infty}^\infty
	\Biggl(\, \int\limits_{-\infty}^\infty e^{-i2\pi s't} f(t) \,dt \Biggr)
	\Biggl(\, \int\limits_{-\infty}^\infty e^{-i2\pi (s-s')t} g(t) \,dt \Biggr) \,ds'$$
  Use Fubini to rearrange the order of integration as needed.
  Use one integration over ($t$) to define the delta $\delta$-function
  $$= \int\limits_{-\infty}^\infty
	\int\limits_{-\infty}^\infty e^{-i2\pi s't} f(t) g(t) \delta(s-s') \,ds' \,dt$$
  Use integration over ($s'$) to apply the delta function.
  What remains is the Fourier transform
  $$= \int\limits_{-\infty}^\infty e^{-i2\pi st} f(t) g(t) \,dt = \mathscr{F}(f \cdot g)$$
  Colloquially, the Fourier transform (as well as the inverse Fourier transform) swap multiplication for convolution.

  A similar set of statements holds for the inverse Fourier transform.
  The \ul{inverse Fourier transform of the convolution}
  $$\boxed{\mathscr{F}^{-1}(f) \cdot \mathscr{F}^{-1}(g) = \mathscr{F}^{-1}(f*g)}$$
  is the product of the inverse Fourier transforms.
\begin{align*}
  \mathscr{F}^{-1}(f) \cdot \mathscr{F}^{-1}(g)
	&= \int\limits_{-\infty}^\infty e^{+i2\pi s_1t} f(s_1) \,ds_1  
    \int\limits_{-\infty}^\infty e^{+i2\pi s_2t} g(s_2) \,ds_2 \\
  &= \int\limits_{-\infty}^\infty\int\limits_{-\infty}^\infty 
	e^{+i2\pi (s_1+s_2)t} f(s_1) g(s_2) \,ds_1 \,ds_2 \\
  \intertext{Use the substitution $s = s_1 + s_2$, again the determinant of the Jacobian is 1}
  & =\int\limits_{-\infty}^\infty e^{+i2\pi st} \Biggl( \int\limits_{-\infty}^\infty
	f(s_1) g(s-s_1) \,ds_1 \Biggr) \,ds = \mathscr{F}^{-1}(f*g)
\end{align*}

  The {\em convolution of the inverse Fourier transform} 
  $$\boxed{\mathscr{F}^{-1}(f) * \mathscr{F}^{-1}(g) = \mathscr{F}^{-1}(f \cdot g)}$$
  is the inverse Fourier transform of the product.
\begin{align*}
  \mathscr{F}^{-1}(f) * \mathscr{F}^{-1}(g)
  &= \int\limits_{-\infty}^\infty
	\Biggl(\, \int\limits_{-\infty}^\infty e^{i2\pi s t'} f(s) \,ds \Biggr)
	\Biggl(\, \int\limits_{-\infty}^\infty e^{i2\pi s (t-t')} g(s) \,ds \Biggr) \,dt' \\
  &= \int\limits_{-\infty}^\infty
	\int\limits_{-\infty}^\infty e^{i2\pi s t'} f(s) g(s) \delta(t-t') \,ds \,dt' \\
  &= \int\limits_{-\infty}^\infty e^{i2\pi s t} f(s) g(s) \,ds = \mathscr{F}^{-1} (f \cdot g) \\
\end{align*}
  
  \subsection{Filtering}
  %Lect 9: Convolution continued...

  It is clear from the convolution theorem that multiplying the spectrum by a filter function, $H(s) = \mathscr{F}(h)$, in the frequency domain corresponds to convolution of $h(t)$ with the signal in the time domain.

  A filter is a system that convolves an input (or set of inputs) with a fixed function/signal (known as the "impulse response").

  In the time domain
  $$\underset{\text{output}}{g} = \underset{\text{input signal}}{f} * \underset{\text{impulse response}}{h}$$
  In the frequency domain, via the Fourier transform
  $$G(s) = (\underset{input spectra}{F(s)}) (\underset{transfer function}{H(s)})$$

  To design a filter involves specifying $H(s)$.
  Recall, mathematically there are positive and negative frequencies to consider, hence filters are defined symmetrically about the vertical axis.
  A low-pass filter is a rectangle function with desired width and height 1 (idealization, because of the sharp cut-offs) 

\begin{figure}[H]
\centering
\begin{tikzpicture}
% Horizontal and vertical axes
\draw[<->] (-4,0) node[anchor=north] {$-v$} 
	  -- (0,0) node[anchor=north] {$0$}
	  -- (4,0) node[anchor=north] {$v$};
\draw[->] (0,0) -- (0,3) node[anchor=west] {$\left\Vert H(s)\right\Vert$};

% Low-pass filter
\draw[very thick] (-4,0)  -- (-1,0)
		-- (-1,1) -- (1,1)
		-- (1,0)  -- (4,0);
\end{tikzpicture}
\caption{The low-pass filter.}
\end{figure}

  A high-pass filter (allow high frequencies and eliminate low frequencies, useful for edge detection) is a square well.

\begin{figure}[H]
\centering
\begin{tikzpicture}
% horizontal vertical axes
\draw[<->] (-4,0) node[anchor=north] {$-v$} 
	  -- (0,0) node[anchor=north] {$0$}
	  -- (4,0) node[anchor=north] {$v$};
\draw[->] (0,0) -- (0,3) node[anchor=west] {$\left\Vert H(s)\right\Vert$};

% High-pass filter
\draw[very thick] (-4,1)  -- (-3,1)
		-- (-3,0) -- (3,0)
		-- (3,1)  -- (4,1);
\end{tikzpicture}
\caption{The high-pass filter.}
\end{figure}

  A band-pass filter consists of rectangle functions at the frequencies of interest.

\begin{figure}[H]
\centering
\begin{tikzpicture}
% horizontal vertical axes
\draw[<->] (-4,0) node[anchor=north] {$-v$} 
	  -- (0,0) node[anchor=north] {$0$}
	  -- (4,0) node[anchor=north] {$v$};
\draw[->] (0,0) -- (0,3) node[anchor=west] {$\left\Vert H(s)\right\Vert$};

% Band-pass filter
\draw[very thick] (-4,0)  -- (-3,0)
		-- (-3,1) -- (-2,1)
		-- (-2,0)  -- (2,0)
		-- (2,1)  -- (3,1)
		-- (3,0)  -- (4,0);
\end{tikzpicture}
\caption{The band-pass filter.}
\end{figure}

  It's comparatively easy to intuit filtering in the frequency domain compared to the time domain...
  To visualize the effect of filtering in time one must visualize $f * h$ (not worth the effort).
  An interpretation that is as flexible as its application is "the convolution is what the convolution does"(Brad Osgood).
  In many contexts, although not universally, convolution is associated with smoothing or averaging, ie: the low-pass filter smooths (but the high-pass filter does not).
  
  Example (DO THIS):
  $$\boldsymbol{\Pi} * \boldsymbol{\Pi} = \boldsymbol{\Lambda}$$
  Example of smoothing because the triangle function is continuous whereas the rectangle function is not.
  Furthermore, this example provides a consistency check for the Fourier transform of the triangle function is $\sinc^2$
  $$\mathscr{F}(\boldsymbol{\Pi} * \boldsymbol{\Pi}) = (\mathscr{F}\boldsymbol{\Pi}) (\mathscr{F}\boldsymbol{\Pi}) = \sinc^2$$

  (FIND EXAMPLE:) An important aspect of convolution is confering properties of one function to another, ie: turning an aperiodic function into a periodic one through convolution with a periodic function.

  Example: If $f$ is differentiable and $g$ is not, then $(f * g)$ is differentiable and 
  $$(f * g)^\prime = f^\prime * g$$
  Similarly with higher order derivatives.


  \subsection{The Derivative Theorem for Fourier Transforms} 

  $$\mathscr{F}(f^\prime) = (i2\pi s) \mathscr{F}f$$
  Similarly for higher derivatives
  $$\mathscr{F}(f^{(n)}) = (i2\pi s)^n \mathscr{F}f$$
  Aphorism: the Fourier transform turns differentiation into multiplication.
  Fundamental property of F.T. that makes it so useful in applications.
  The derivation for the case when $f(t) \rightarrow 0$ as $t \rightarrow \pm \infty$ is
\begin{align}
  \mathscr{F}f^\prime &= \int\limits_{-\infty}^\infty e^{-i2\pi s t} f^{\prime}(t) \,d t \\
  \intertext{Integrate by parts using
	$\qquad\begin{cases}
	      \;u = e^{-i2\pi s t} \\ 
	      \,d v = f^{\prime}(t) \,d t
	  \end{cases}$}
  &= \underbrace{\biggl[f(t) e^{-i2\pi s t}\biggr]_{-\infty}^\infty}_{\text{$=0$}} -
  \int\limits_{-\infty}^{\infty} f(t) (-i2\pi s e^{-i2\pi s t}) \,d t \\
  &= (i2\pi s) \int\limits_{-\infty}^\infty e^{-i2\pi s t} f(t) \,d t \\
  &= (i2\pi s) \mathscr{F} f \\
\end{align}

  Do quintessential application of F.T.: The Heat Equation (end of lect. 9)
  -For the ring: 
  -For the infinite rod:


  \subsection{The Central Limit Theorem}
  (Lect 10: Convolution and the Central Limit Theorem)
  (Lect 11: fix a mistake made at the end of Lect. 10 for expression for the exponential deduced from the truncated Taylor series expansion)
  - maybe combine this with the section above on the convolution?...

  Recall: CLT explains the universal appearance of the Gaussian in probability.

  Suppose $x_1$ and $x_2$ are independent identically distributed random variables (iid) with distributions $p(x_1) = p(x_2) = p(x)$, respectively.
  Question: How is the sum of the random variables distributed?
  Answer: by the convolution $p_1 * p_2$

  Proof:
  
  For any value $t$
  $$P(x_1 + x_2 \le t) = \int\int p(x_1) p(x_2) \,d x_1 \,d x_2$$

  %THIS IS INTERESTING AND POSSIBLY WORTH COMING BACK TO BUT NOT WORTH THE TIME NOW!!!


\subsection{The Correlation Theorem}

  Correlation is defined as
  $$Corr(g, h) \equiv \int_{-\infty}^\infty g(\tau + t) h(\tau) \,d\tau$$
  where $t$ is called ``the lag''.

  The correlation theorem is
  $$Corr(g, h) \Longleftrightarrow G(s) H^* (s)$$
  when $g$ and $h$ are real.

  And, more generally
  $$Corr(g, h) \Longleftrightarrow G(s) H(-s)$$

  The autocorrelation $corr(g, g)$, which leads to the Wiener-Khinchin theorem
  $$Corr(g, g) \Longleftrightarrow |G(s)|^2$$


\subsection{Parseval's Theorem}

  $$\text{total power} \equiv \int_{-\infty}^\infty |h(t)|^2 \,dt = \int_{-\infty}^\infty |H(s)|^2 \,ds$$

  Usually the entity of interest is the power within a frequency interval $s$ to $s + \,d s$.
  Here it is common to ingore negative frequencies.
  Then the ``one-sided power spectral density'' (PSD) of the function $f$ is
  $$P_f(s) \equiv |F(s)|^2 + |F(-s)|^2 \qquad 0 \le s < \infty$$

  Numerical Recipes p.603 ... what is the difference between the one-sided and two-sided spectral density?

  If the function/signal is infinite then in general so is the total power and power spectral density.
  Then of interest is the (1 or 2 sided) power spectral density per unit time, ie: select a portion of the signal over a long but finite duration, compute the PSD, and divide by the duration/distance.
  


\section{The Fourier Transform of Basic Signals}

\subsection{The Box Function, $\boldsymbol\Pi$}

  The square pulse (aka box, characteristic, indicator) function is
  $$\Pi(t) =
    	\begin{cases}
	    	1 & \text{if $|t| < 1/2$} \\
	    	0 & \text{if $|t| \geq 1/2$}
    	\end{cases}
  $$

  The Fourier transform of the box function is
\begin{align*}
  \mathscr{F}(\Pi(t)) &= \int\limits_{-\infty}^{\infty} \,d t \, \Pi(t) \exp (- 2 \pi i s t )
   = \int\limits_{-1/2}^{1/2} \,d t \exp (- 2 \pi i s t ) \\
  &= \Biggl[{- 1 \over 2 \pi i s} \exp (- 2 \pi i s t )\Biggr]_{t = -1/2}^{t = 1/2} \\
  &= \Biggl({- 1 \over 2 \pi i s} \exp (- \pi i s )\Biggr) - \Biggl({- 1 \over 2 \pi i s } \exp (\pi i s )\Biggr) \\
  &= {1 \over \pi s} \Biggl({ \exp(\pi i s) - \exp(-\pi i s) \over 2 i } \Biggr) \\
  &= {\sin(\pi s) \over \pi s} \\
\end{align*}
  $$\boxed{\mathscr{F}\Pi = \sinc(s)}$$
  Here, the ({\em normalized}) $\sinc()$ function is defined as $\sinc(x) = {\sin(\pi x)\over \pi x}$, which is suited for interpolation of bandlimited signals (cite: wikipedia, Sinc function, 10/26/2022).
  (it is common in Calculus text books for $\sinc(x)$ to be defined as $\sinc(x) = {\sin(x)\over x}$ (cite Strang, Calculus Vol.1, Ch.4.8, p.454)).


  The boxcar (aka rectangle) function (Osgood unabashedly calls this the ``stretched rect'' p.126 \cite{Osgood}) 
  $$
	\Pi_b(t) =
	\begin{cases}
		1 & \text{if $|t| < b/2$} \\
		0 & \text{if $|t| \ge b/2$} \\
	\end{cases}
  $$

  The Fourier transform of the boxcar function is
\begin{align*}
  \mathscr{F} \Pi_b(t) &= \int\limits_{-\infty}^{\infty} \,d t \, \,e^{- i 2 \pi s t } \,\Pi_b(t) 
   = \int\limits_{-b/2}^{b/2} \,d t \,e^{- i 2 \pi s t } \\
  &= \Biggl[{ e^{- i 2 \pi s t } \over (-i 2 \pi s)} \Biggr] \Biggr\rvert_{t = -b/2}^{t = b/2}
  = {1\over i 2 \pi s} \Bigl( -e^{- i \pi s b } - \bigl( -e^{i \pi s b } \bigr) \Bigr)\\
  &= {1 \over \pi s} \Biggl({ e^{i \pi s b} - e^{-i \pi s b} \over i 2 } \Biggr)
  = {\sin(b \pi s) \over \pi s} \\
\intertext{Technically, the $\sinc(x)$ function is $\sin(\pi x)/(\pi x)$ - {\em the denominator is the same as the argument to} $\sin()$ (\ul{this is a subtle but important point, vital to applications, which you should check using Julia}), such that multiplication by a factor of $1 = b/b$ is necessary to construct the $\sinc$ function}
  &= {b \sin(b \, \pi s) \over b \, \pi s} \\
\end{align*}
  $$\boxed{\mathscr{F} \Pi_b = b \sinc(b s)}$$

  This comes up in describing the aperture function in diffraction...
  This also comes up in sampling when performing interpolation...


  The Fourier transform of the (normalized) boxcar function of width $b$ and area one, ${1\over b}\boldsymbol{\Pi}_b(t)$, is
\begin{align*}
  \mathscr{F}\biggl({\Pi_b(t)\over b}\biggr) &= {1\over b} \int\limits_{-\infty}^{\infty} \,d t \, \Pi_b(t) \,e^{- i 2 \pi s t }
   = {1\over b} \int\limits_{-b/2}^{b/2} \,d t \,e^{- i 2 \pi s t } \\
  &= \Biggl[-{ e^{- i 2 \pi s t } \over i 2 \pi s b} \Biggr] \Biggr\rvert_{t = -b/2}^{t = b/2}
  = {1\over i 2 \pi s b} \Bigl( -e^{- i \pi s b } - \bigl( -e^{i \pi s b } \bigr) \Bigr)\\
  &= {1 \over \pi s b} \Biggl({ e^{i \pi s b} - e^{-i \pi s b} \over i 2 } \Biggr)
  = {\sin(b \pi s) \over (b \pi s)} \\
\end{align*}
  $$\boxed{\mathscr{F} \biggl({\Pi_b\over b}\biggr) = \sinc(b s)}$$


\subsection{The Hat Function, $\boldsymbol\Lambda(t)$}

  The triangle (aka hat, tent) function is
  $$ \Lambda(t) =
     \begin{cases}
	     1 - |t| & \text{if $|t| < 1$} \\
	     0 & \text{if $|t| \geq 1$}
     \end{cases}
  $$

  The Fourier transform of $\Lambda(t)$ is
\begin{align*}
  \mathscr{F}(\Lambda((t)) &= \int\limits_{-\infty}^{\infty} \,d t \,\Lambda(t) \exp (- 2 \pi i s t ) \\
\intertext{Break the integral up into two sections}
  &= \int\limits_{-1}^0 \,d t (1 + t) \exp(- 2 \pi i s t) + \int\limits_0^1 \,d t (1 - t) \exp(- 2 \pi i s t) \\
\intertext{Solve each section using integration by parts.  }
\intertext{Making the substitutions for $u$ and $v$ }
  &= \int\limits_{-1}^0 \underbrace{(1 + t)}_{\text{u}} \underbrace{\exp(- 2 \pi i s t) \,d t}_{\text{\,dv}} \\
\intertext{gives $ \,d u = \,d t $, and $ v = {-1 \over 2 \pi i s} \exp(- 2 \pi i s t) $}
  &= \biggl[(1 + t) \biggl({-1 \over 2 \pi i s} \exp(-2 \pi i s t)\biggr)\biggr]_{t = -1}^{t = 0} - \int\limits_{-1}^0 \,d t \biggl({-1 \over 2 \pi i s}\biggr) \exp(-2 \pi i s t) \\
  &= \biggl({-1 \over 2 \pi i s} \biggr) + \biggl[{-1 \over (2 \pi i s)^2} \exp(-2 \pi i s t)\biggr]_{t=-1}^{t=0} \\
  &= {-1 \over 2 \pi i s} + \biggl({-1 \over (2 \pi i s)^2} + {\exp(-2 \pi i s t)\over (2 \pi i s)^2}\biggr) \\
\intertext{The second section ...}
  &= \int\limits_{0}^1 \underbrace{(1 - t)}_{\text{u}} \underbrace{\exp(- 2 \pi i s t) \,d t}_{\text{\,dv}} \\
  &= \biggl[(1 - t) \biggl({-1 \over 2 \pi i s} \exp(-2 \pi i s t)\biggr)\biggr]_{t = 0}^{t = 1} - \int\limits_{-1}^0 (-\,d t) \biggl({-1 \over 2 \pi i s}\biggr) \exp(-2 \pi i s t) \\
  &= {- 1 \over 2 \pi i s} + \biggl(- {\exp(-2 \pi i s t)\over (2 \pi i s)^2} + {1 \over (2 \pi i s)^2}\biggr) \\
\intertext{Combining the results of both sections gives}
  &= {1\over (\pi s)^2} {\exp(2 \pi i s) - \exp(-2 \pi i s) \over (2 i)^2} \\
  &= {\sin^2(\pi s) \over (\pi s)^2} \\
\end{align*}
  $$\boxed{ \mathscr{F}\Lambda = \sinc^2(s) }$$


  \subsection{The Gaussian, $\Phi$}

  The Gaussian in its simplest form is
  $$\Phi(t) = e^{-\pi t^2}$$
  , which when integerated gives 
  $$\int\limits_{-\infty}^{\infty} \,d t e^{-\pi t^2}= 1$$

  The trick for evaluating its Fourier transform
  $$F(s) = \int\limits_{-\infty}^{\infty} e^{-2 \pi i s t} e^{-\pi t^2} \,d t $$
  is to notice
\begin{align*}
  F^{\,\prime}(s) &= {\, d \over \,d s} \Biggl(\,\, \int\limits_{-\infty}^{\infty} e^{-i2 \pi s t}\, e^{-\pi t^2} \,d t \Biggr) = \int\limits_{-\infty}^{\infty} {\, d \over \,d s} \biggl( e^{-i2 \pi s t} \biggr) e^{-\pi t^2} \,d t \\
\intertext{, the derivative with respect to $s$, is}
  &= \int\limits_{\infty}^{\infty} (-i2\pi t) e^{-i2 \pi s t} e^{-\pi t^2} \,d t \\
\intertext{, which when written as}
  &= (i) \int\limits_{-\infty}^{\infty} \underbrace{e^{-i2 \pi s t}}_{u} \underbrace{\bigl((-2 \pi t)\,e^{-\pi t^2} \bigr) \,d t}_{\,d v} \\
\intertext{suggests integration by parts, with the substitutions
  $$ u = e^{-i2\pi s t} \quad,
  \quad \,d v = (-2\pi t)\,e^{-\pi t^2} \,d t $$
  $$ \quad v = e^{-\pi t^2} \qquad,
  \quad \,d u = (-i2\pi s)\,e^{-i2\pi s t} \,d t $$
}
\intertext{Integration by parts $\Bigl(\, \int uv' = \int (uv)' - \int vu' \,\Bigr)$ gives}
  &= (i) \Biggl\{ \underbrace{\biggl[e^{-\pi t^2} e^{-i2\pi s t} \biggr]\Biggr|_{t=-\infty}^{t=\infty}}_{=0} - \int\limits_{-\infty}^{\infty} e^{-\pi t^2} (-i2 \pi s) \,e^{-i2 \pi s t} \,dt \Biggr\} \\
  &= (i) \Biggl\{ \biggl(\underbrace{e^{-\pi (\infty)^2}}_{=0} e^{-i2\pi s (\infty)} - \underbrace{e^{-\pi (-\infty)^2}}_{=0} e^{-i2\pi s (-\infty)} \biggr) + (i2 \pi s) \int\limits_{-\infty}^{\infty} e^{-\pi t^2} \,e^{-i2 \pi s t} \,dt \Biggr\} \\
  &= (i^2 2 \pi s) \int\limits_{-\infty}^{\infty} e^{-i2 \pi s t} \,e^{-\pi t^2} \,d t \\
  &= (-2 \pi s) F(s) \\
\end{align*}

  The solution that is ``screaming out'' to solve this differential equation is, $F(s) = e^{-\pi s^2}$, given
  $$ {\,d \over \,d s } \Bigl( e^{- \pi s^2} \Bigr) = (-2\pi s) e^{- \pi s^2} $$
  , which in general is subject to some initial condition, but in this case
  $$ F(0) = \int\limits_{-\infty}^{\infty} e^{-i2\pi (0) t} e^{-\pi t^2} \,d t = \int\limits_{-\infty}^{\infty} e^{-(\pi) t^2} \,d t = \sqrt{{\pi \over (\pi)}} = 1 $$
  and the Fourier transform of a Gaussian {\em is a Gaussian}
  $$ \boxed{ e^{-\pi t^2} \xlongleftrightarrow[\text{$\mathscr{F}^{^{-1}}$}]{\text{$\mathscr{F}$}} e^{-\pi s^2} } $$
  % *** FIX: arrows, two arrows right arrow above left arrow.
  as is the inverse Fourier transform.


  \subsection{The Sinc Function, $\sinc()$}

  This Fourier transform of the $\sinc()$ function can be computed without integration using duality
  $$\mathscr{F} \sinc = \mathscr{F} \mathscr{F} \boldsymbol{\Pi} = \boldsymbol{\Pi}^- = \boldsymbol{\Pi}$$
  because $\boldsymbol{\Pi}$ is an even function.

  Similarly, for $\sinc^2()$
  $$\mathscr{F} \sinc^2 = \mathscr{F} \mathscr{F} \boldsymbol{\Lambda} = \boldsymbol{\Lambda}^{reversed} = \boldsymbol{\Lambda}$$
  because $\boldsymbol{\Lambda}$ is an even function.



  \section{The Modern Fourier Transform}


  \subsection{Issues with the Old Framework}

  A more robust definition of the Fourier transform is required to handle even the most common signals.

  There are two issues
\begin{enumerate}
  \item {\bf Convergence of the integral that defines the F.T.} 
  	\begin{itemize}
    	\item The expresssion for the Fourier transform of even the simplest possible signal $f(t) = 1$
		$$\mathscr{F} f = \int\limits_{-\infty}^\infty e^{-2\pi i s t} \cdot 1 \,d t$$
		is meaningless.
		\item Similarly, the F.T. for $\sin (2\pi t)$, and $\cos (2\pi t)$, is not defined (does not converge).
  	\end{itemize}
  \item {\bf Convergence of the integral that defines the inverse F.T.} 
	\begin{itemize}
		\item The F.T. of the boxcar/rectangle function is
  		$$\mathscr{F} \boldsymbol{\Pi} = \sinc(s) = {\sin(\pi s)\over \pi s}$$
  		Duality was used to show the inverse F.T. of the $\sinc$ function is the boxcar function.
  		But to actually solve the integral 
  		$$\mathscr{F}^{-1} \sinc = \int\limits_{-\infty}^\infty e^{2\pi i s t} {\sin (\pi t)\over \pi t} \,d t$$
  		requires special techniques (issue at the end points, $s = \pm 1/2$).
	\end{itemize}
\end{enumerate}


  \subsection{``Where Do We Go From Here?'' (circa 1940)}

  %(Brad, Lect. 11)
  In response to this ambiguity there are two approaches, 1. ad hoc approaches, or 2. rework the foundations.
  Taking the latter approach involes...
\begin{itemize}
  \item Choose the phenomena (a class of functions) upon which to build the theory (that will explain all others).
    \begin{itemize}
	  \item Note: $\mathcal{S}$ is a restricted class that serves as the foundation for a broader class.
    \end{itemize}
  \item Establish the requirements for ``the best'' class of signals/functions ($\mathcal{S}$) for performing Fourier transforms
  \begin{enumerate}
    \item If $f$ is in $\mathcal{S}$ then its Fourier transform and inverse Fourier transform are defined and are also in $\mathcal{S}$
    $$f \in \mathcal{S} \;\Rightarrow\; \mathscr{F}f \in \mathcal{S} \text{ , and }\; \mathscr{F}^{-1}f \in \mathcal{S}$$
    \begin{itemize}
      \item Notice: The most fundamental signals, eg: $\sin$, $\cos$, $1$, are not in this class $\mathcal{S}$ because the F.T. is not defined.
	  \item Notice: $\boldsymbol{\Pi}$ is not in the class $\mathcal{S}$ because the F.T. $\mathscr{F}\boldsymbol{\Pi}$, the $\sinc$ function, is not in $\mathcal{S}$.
    \end{itemize}
    \item If $f$ is in $\mathcal{S}$ then Fourier inversion works
	$$f \in \mathcal{S} \Rightarrow \mathscr{F}^{-1}\mathscr{F}f = \mathscr{F}\mathscr{F}^{-1}f = f$$
  \item * {\bf Parseval's Identity} - this is not a requirement of the class $\mathcal{S}$ but comes up often
  $$\int\limits_{-\infty}^\infty |\mathscr{F}(s)|^2 \,d s = \int\limits_{-\infty}^\infty |f(t)|^2 \,d t$$
  Interpretation: total spectral power can be computed in frequency as well as in time.
  \\ Proof: based on definition of the inner product for functions $f$ and $g$ $\in \mathcal{S}$
  $$\int\limits_{-\infty}^\infty \mathscr{F}f(s) \,\overline{\mathscr{F}g(s)} \,d s
  = \int\limits_{-\infty}^\infty f(t) \,\overline{g(t)} \,d t$$ 

  Consider
  \begin{align*}
  g(t) &= \mathscr{F}^{-1}((\mathscr{F}g)(s)) \\
  &= \mathscr{F}^{-1} \Biggl(\, \int\limits_{-\infty}^\infty g(t) \,e^{-2\pi i s t} \,d t \Biggr) \\
  &= \int\limits_{-\infty}^\infty \Biggl(\, \int\limits_{-\infty}^\infty g(t) \, e^{-2\pi i s t} \,d t \Biggr) e^{2\pi ist} \,ds \\
  \overline{g(t)} &= \int\limits_{-\infty}^\infty \Biggl(\, \int\limits_{-\infty}^\infty \overline{g(t)} \, e^{2\pi i s t} \,d t \Biggr) e^{-2\pi ist} \,ds
  \end{align*}

  Substituting this expression for $\overline{g(t)}$ into the definition of the inner product
  \begin{align*}
  \int\limits_{-\infty}^\infty f(t) \,\overline{g(t)} \,d t 
  &= \int\limits_{-\infty}^\infty f(t) \Biggl(\, \int\limits_{-\infty}^\infty \Biggl(\, \int\limits_{-\infty}^\infty \overline{g(t)} \, e^{2\pi i s t} \,d t \Biggr) e^{-2\pi ist} \,ds \Biggr) \,d t \\
  &= \int\limits_{-\infty}^\infty \Biggl(\, \int\limits_{-\infty}^\infty f(t) \, e^{-2\pi ist} \,d t \Biggr) \Biggl(\, \int\limits_{-\infty}^\infty \overline{g(t)} \, e^{2\pi i s t} \,d t \Biggr) \,ds \\
  &= \int\limits_{-\infty}^\infty \mathscr{F}f(s) \, \overline{\mathscr{F}g(s)} \,d s \\
  \end{align*}
  \end{enumerate}
\end{itemize}

  The best class of functions for Fourier transforms, known as the ``distributions'' (distinct from probability distributions, aka generalized functions), are named in honor of Laurent Schwartz, $\mathcal{S}$.
  The insight for $\mathcal{S}$ was inspired by the derivative theorem
  $$\mathscr{F}(f^{(m)})(s) = (2\pi i s)^m \mathscr{F} f(s)$$
  The class $\mathcal{S}$ is described as the set of \underline{{\em rapidly decreasing functions}}, and is defined by the following properties
\begin{enumerate}
  \item $f(x)$ is infinitely differentiable.
	\begin{itemize}
	  \item The constant functions are not in the class $\mathcal{S}$.
      \item The rectangle function $\boldsymbol{\Pi} \notin \mathcal{S}$ because it's not continuous.
      \item The triangle function $\boldsymbol{\Lambda} \notin \mathcal{S}$ because it's not differentiable.
	\end{itemize}
  \item Any derivative of $f(x)$ tends to 0 faster than any power of $x$.
  For any $m$, $n$ $\ge 0$
  $$|x|^n \biggl| {\,d^m f(x) \over \,d x^m} \biggr| \longrightarrow 0 \quad\text{as}\quad x \rightarrow \pm \infty$$
  \begin{itemize}
	\item The smooth functions (infinitely differentiable) that are {\em equal to zero} outside a finite interval (``functions of compact support'')) $\mathcal{C}$ are in the class $\mathcal{S}$.
	\item The Gaussian is in the class $\mathcal{S}$.
	\item The trig functions are not in the class $\mathcal{S}$.
  \end{itemize}
\end{enumerate}

  For the Schwartrz $\mathcal{S}$ functions the following properties hold
\begin{enumerate}
	\item If $f$ is in the Schwartz class, then so is its Fourier transform
		$$f \in \mathcal{S} \Longrightarrow \mathscr{F}f(s) \in \mathcal{S}$$
	\item If $f$ is in the Schwartz class, then Fourier inversion works
		$$f \in \mathcal{S} \Longrightarrow \mathscr{F}^{-1}\mathscr{F}f = f$$
\end{enumerate}

  The Schwartz functions are ultimately used to define the {\bf tempered dinstributions}, $\mathcal{T}$, a broad class of ``generalized functions''.
  The tempered distributions expands the Schwartz $\mathcal{S}$ class to include sines and cosines, delta $\delta$-functions (nonintegrable functions), ....
  Furthermore, the properties
  $$f \in \mathcal{S} \Longrightarrow \mathscr{F}f(s) \in \mathcal{S}$$
  $$f \in \mathcal{S} \Longrightarrow \mathscr{F}^{-1}\mathscr{F}f = f$$
  continue to hold for the tempered distributions, $\mathcal{T}$.

  Having restricted the class of functions to $\mathcal{S}$, two requirements must be demonstrated.
\begin{itemize}
  \item Nothing must be lost by restricting the class of functions to $\mathcal{S}$.
  \item Greater generality must be gained.
\end{itemize}
  

  \subsection{The Dirac $\delta$-function} %(Brad, Lect 12: utter trash lecture!)

  The Dirac delta function, $\delta (x)$ is the quintessential distribution.
  The statements typically used to define the Dirac-$\delta$ lack rigor%(... bullshit!)
\begin{enumerate}
\item $$\begin{cases}
	  \delta(x) = 0 , x \neq 0 \\
	  \delta(0) = \infty
        \end{cases}$$
\item $$\int\limits_{-\infty}^\infty \delta(x) \,d x = 1$$
\item $$\int\limits_{-\infty}^\infty \delta(x) \varphi(x) \,d x = \varphi(0)$$ 
\end{enumerate}

  The $\delta$ is supposed to represent a function that is concentrated at a point, and is commonly defined via a limiting process on a function.
  For example, consider the one-parameter family of rectangle functions $\boldsymbol{\Pi}_\varepsilon$

\begin{figure}[H]
\centering
\begin{tikzpicture}
% Horizontal and vertical axes
\draw[<->] (-3,0) node[anchor=north] {$$}
	  -- (0,0)
	  -- (3,0) node[anchor=north] {$x$};
\draw[->] (0,0) -- (0,3);

% Parent function for family of rectangle functions of area 1
\draw[very thick] (-1,0) node[anchor=north] {${-\varepsilon/2}$} 
		-- (-1,2)
		-- (-0.5, 2);
\draw[very thick] (0, 2) node[anchor=east] {${1\over\varepsilon}$}
		-- (1, 2) node[anchor=west ] {$\boldsymbol{\Pi}_\varepsilon(x)$}
		-- (1, 0) node[anchor=north] {${\varepsilon/2}$};
\end{tikzpicture}
\caption{The parent function for the family of rectangle functions of area 1, with width ($\varepsilon$) and height ($1/\varepsilon$).}
\end{figure}

  Note that the limit 
  $$\lim_{\varepsilon \rightarrow 0} \Pi_\varepsilon(x)$$
  on its own makes no sense.

  However, operationally, the following does make sense
\begin{align*}
  \lim_{\varepsilon \rightarrow 0} \int\limits_{-\infty}^\infty \Pi_\varepsilon(x) \varphi(x) \,d x &= \lim_{\varepsilon \rightarrow 0} {1\over\varepsilon} \int\limits_{-\varepsilon/2}^{\varepsilon/2} \varphi(x) \,d x \\
  \intertext{Assuming $\varphi(x)$ is smooth and continuous, it can be written as a Taylor expansion} \\
  &=\lim_{\varepsilon \rightarrow 0} {1\over\varepsilon} \int\limits_{-\varepsilon/2}^{\varepsilon/2} \Bigl( \varphi(0) + \varphi^\prime(0) x + \varphi''(0) {x^2\over 2} + \cdots \Bigr) \,d x \\ 
  &=\lim_{\varepsilon \rightarrow 0} {1\over\varepsilon} \int\limits_{-\varepsilon/2}^{\varepsilon/2} \varphi(0) \,d x +
  \lim_{\varepsilon \rightarrow 0} {1\over\varepsilon} \int\limits_{-\varepsilon/2}^{\varepsilon/2} \varphi^\prime(0) x \,d x + 
  \lim_{\varepsilon \rightarrow 0} {1\over\varepsilon} \int\limits_{-\varepsilon/2}^{\varepsilon/2} \varphi''(0) {x^2\over 2}  \,d x + \cdots \\
  \intertext{Beyod the first term is a series of higher order terms in $x$} \\
  &= \varphi(0) + \lim_{\varepsilon \rightarrow 0} \:\mathcal{O}(\varepsilon) \\
  &= \varphi(0)
\end{align*}

  Notice, the $\lim$ of $\boldsymbol{\Pi}_\varepsilon$ as $\varepsilon \rightarrow 0$ is rigorously defined when \underline{\em paired with a function and integrated}.
  This amounts to a fundamental shift in view point, which emphasizes outcomes over the process, ie: extracting the value of a function at a particular point v. concentration via a limiting process.  %(...whatever)

  The Dirac $\delta$-fuction is endowed with two important properties that are essential for Fourier transforms, the shifting and scaling properties:
\begin{enumerate}
  \item The \underline{shifting property of the $\delta$-function} is
  $$\boxed{\varphi * \delta = \int\limits_{-\infty}^\infty \varphi(x) \delta(x-b) \,dx = \varphi(b)}$$
  , which amounts to convolution with the $\delta$-function (Might have to demonstrate this further...).
  %...Thrown in at the end of lecture 14 with no context but introduced out of imminent necessity.
  %...issue is treated on p.194 of Osgood, in section on distributions 'Scaling and the Stretch Theorem'
  \item Somewhat counter intuitively there is the \underline{scaling property for the $\delta$-function} defined as
  $$\boxed{\delta(a x) = {1\over |a|} \delta(x)}$$
\begin{align*}
  \intertext{For $a > 0$, use the $u$-substitution
     $\begin{cases}
		u = a x \\
		\,d u = a \,d x
     \end{cases}$}
  \int\limits_{-\infty}^\infty \delta(a x) \varphi(x) \,d x
  &= {1\over a} \int\limits_{-\infty}^\infty \delta(u) \varphi\biggl({u\over a}\biggr) \,d u \\
  &= {1\over a} \, \varphi(0) \\
  \intertext{For $a < 0$ the $u$-substitution effectively switches the limits of integration because $a$ is negative}
  &= {1\over a} \int\limits_\infty^{-\infty} \delta(u) \varphi\biggl({u\over a}\biggr) \,d u \\
  \intertext{To recover the original limits of integration introduces a negative sign}
  &= - {1\over a} \int\limits_{-\infty}^\infty \delta(u) \varphi\biggl({u\over a}\biggr) \,d u \\
  \intertext{But $a$ is negative, so}
  &= {1\over |a|} \int\limits_{-\infty}^\infty \delta(u) \varphi\biggl({u\over a}\biggr) \,d u
\end{align*}
  , which implies $\delta(ax) = (1/|a|)\delta(x)$.
  While a scaling factor has been introduced into the argument of the pairing function it is ultimately of no consequence as it will be evaluated at 0 by the delta function.
  This is distinct from the scaling theorem for the Fourier transform since the argument variable (on the right hand side of the equation) is not also scaled.
\end{enumerate}


  \section{Distributions}

  The thermometer analogy...

  The definition of distributions/generalized functions
\begin{itemize}
	\item First specify a class of ``test functions'' (signals), $\varphi$ (the best suited for a given problem).
		\begin{itemize}
  			\item For the F.T. this is the Schwartz functions.
		\end{itemize}

  	\item Associated with a class of test functions is a class of distributions $\mathcal{T}$ (a distribution is ``{\em paired}\,'' with a test function).
		\begin{itemize}
			\item[--] A pairing is defined such that a distribution operates on a test function to produce a number, such that $\mathcal{T}(\varphi) \in \mathbb{C}$.
			\item[--] Pairing notation: $\langle \mathcal{T}, \varphi \rangle$ (not an inner product), or (less commonly) $\mathcal{T}(\varphi)$.
			\item[--] A physical interpretation might be that $\varphi$ corresponds to the distribution for a system or of one of its properties, and $\mathcal{T}$ is a measurement.
  		\end{itemize}

	\item $\mathcal{T}$ must be linear.

  	\item $\mathcal{T}$ must be continuous (needed for taking limits)
		\begin{itemize}
			\item[--] The convergence of a sequence of test functions $\varphi_n$ converges to a function $\varphi$
			$$\lim_{n\rightarrow\infty} \{\varphi_n\} \rightarrow \varphi \Rightarrow \mathcal{T}(\varphi_n) \rightarrow \mathcal{T}(\varphi)$$
			\item[--] {\bf *(This seems like more of a requirement on test functions than on distributions).}
			\item[--] This is the most problematic aspect of the framework because this is the difficult thing to prove.
			\item[--] Although each individual function might be infinitely differentiable (as in all the derivatives decay) the more properties conferred upon the component functions the more difficult it becomes to control convergence (foundations for the theory of distributions/generalized functions delve into this).
		\end{itemize}
\end{itemize}

  In sum, a distribution is a continuous linear functional that operates on a set of test function to produce a number.

  \subsection{The Dirac $\delta$-Function as a Distribution}

  To recover the Dirac-$\delta$ in the context of distributions, consider it operationally as function evaluation (at the origin) and turn that into a definition.
  \underline{\em For the Dirac-$\delta$ the pairing is defined by sheer assertion as}
  $$\langle \delta , \varphi \rangle = \varphi(0)$$
  Check linearity, for the distributive property
  $$\langle \delta , \varphi_1 + \varphi_2 \rangle
  = (\varphi_1 + \varphi_2)(0)
  = \varphi_1(0) + \varphi_2(0) = \langle \delta, \varphi_1\rangle + \langle \delta, \varphi_2\rangle$$
  And similarly, for scalar multiplication
  $$\langle T, \alpha\varphi \rangle = \alpha \langle T, \varphi \rangle$$

  To check continuity, show %***(very shaky demo): 
  $$\varphi_n \rightarrow \varphi \Rightarrow \langle \delta, \varphi_n\rangle \rightarrow \langle \delta, \varphi\rangle$$
  By definition
  $$\langle\delta, \varphi_n\rangle = \varphi_n(0)$$
  If $\varphi_n \rightarrow \varphi \Rightarrow \varphi_n(0) \rightarrow \varphi(0)$, then
  $$\langle\delta, \varphi_n\rangle \rightarrow \langle \delta, \varphi\rangle$$

  The Dirac-$\delta$ is re-couched as the simplest possible distribution.
  The pairing is defined by sheer assertion.
  Similarly, the shifted $\delta$-function, $\delta_a$ as a distribution is defined by the pairing $\langle \delta_a , \varphi \rangle = \varphi(a)$.
  \vspace{1cm}
  
  \subsection{Distributions Induced by Functions}
  %We have gained $\delta$ but what have we lost?
  %...Is it possible to recover, e.g. $\boldsymbol{\Pi}$, $\boldsymbol{\Lambda}$, $\cos$, $\sin$?
  %How to consider "ordinary functions" in this context, ie: how to treat $f(x)=1$ as a distribution, and define a pairing of 1 and $\varphi$?
  For distributions ``induced'' by functions the pairing is defined by integration, which is both linear and *continuous (the latter is difficult to prove, requires limiting theorems for integrals)
  $$\langle f, \varphi \rangle = \int\limits_{-\infty}^\infty f(x) \varphi(x) \,d x < \infty$$
  , provided the integral converges.
\begin{itemize}
  \item $f(x) = 1$ as a distribution
  $$\langle 1, \varphi \rangle = \int\limits_{-\infty}^\infty 1 \cdot \varphi(x) \,d x$$

  \item $\boldsymbol{\Pi}(x)$ as a distribution
  $$\langle \Pi , \varphi \rangle = \int\limits_{-\infty}^\infty \Pi(x) \varphi(x) \,d x$$

  \item $f(x) = \exp (-2\pi iax)$ (which includes sines and cosines)
  $$\langle 1, \varphi \rangle = \int\limits_{-\infty}^\infty e^{-i2\pi ax} \varphi(x) \,d x < \infty$$
\end{itemize}

  Notice the flexibility in the definition.
  There is a trade-off between $\varphi(x)$ and $f(x)$.
  The more restricted $\varphi(x)$ is, the more freedom $f(x)$ can have.
  %Vernacular: a function "determines" a distribution by specifying how it operates on a test function (via integration... which is a linear operation and (the operation is) continuous)
  (The space of distributions is the "dual space" of the space of test functions).


  \subsection{The Fourier Transform of a Distribution}

  (...from "classical" F.T.s to "generalized" F.T.s)
  The test functions are the Schwartz functions $\mathcal{S}$, and the class of distributions is called the class of {\em tempered} distributions.
  If $T$ is a tempered distribution, then its F.T. is a tempered distribution.
  The pairing is defined by integration as
  \begin{align*}
  \langle \mathscr{F}T, \varphi \rangle &= \int\limits_{-\infty}^\infty \biggl(\int\limits_{-\infty}^\infty e^{-2\pi ixy} T(y) \,d y\biggr) \varphi(x) \,d x \\
  &= \int\limits_{-\infty}^\infty \int\limits_{-\infty}^\infty e^{-2\pi ixy} T(y) \varphi(x) \,d y\,d x \\
  &= \int\limits_{-\infty}^\infty \biggl(\int\limits_{-\infty}^\infty e^{-2\pi ixy} \varphi(x) \,d x\biggr) T(y) \,d y \\
  &= \int\limits_{-\infty}^\infty \mathscr{F}\varphi(y) T(y) \,d y \\
  &= \langle T, \mathscr{F}\varphi \rangle \\
  \end{align*}

  Turn this into a definition.
  Define $\mathscr{F}T$ by
  $$\boxed{\langle \mathscr{F}T, \varphi \rangle = \langle T, \mathscr{F}\varphi \rangle}$$
  Likewise, the inverse F.T. of a distribution is
  $$\boxed{\langle \mathscr{F}^{-1}T, \varphi \rangle = \langle T, \mathscr{F}^{-1}\varphi \rangle}$$

  Importantly, Fourier inversion works for tempered distributions	
  $$\mathscr{F}^{-1}\mathscr{F}T = \mathscr{F}\mathscr{F}^{-1}T = T$$
  \begin{align*}
  \langle \mathscr{F}^{-1}\mathscr{F}T, \varphi \rangle
  &= \langle \mathscr{F}T, \mathscr{F}^{-1}\varphi \rangle\\
  &= \langle T, \mathscr{F}\mathscr{F}^{-1}\varphi \rangle\\
  &= \langle T, \varphi \rangle\\
  \end{align*}
  because Fourier inversion works for Schwartz functions.
  %Hence, $\mathscr{F}^{-1}\mathscr{F}T = T$

  %Show this is actually useful, that is you can compute Fourier transforms using this framework
  \vspace{0.5cm}
  The \underline{Fourier transform of the Dirac $\delta$-function} is
\begin{align*}
  \langle \mathscr{F}\delta, \varphi \rangle &= \langle \delta, \mathscr{F}\varphi \rangle \\
  &= \mathscr{F}\varphi(0) \\
  \intertext{ Now right the classical definition of the F.T. and evaluate it at zero} \\
  &= \int\limits_{-\infty}^\infty e^{-2\pi i 0 x} \varphi(x) \,d x \\
  &= \int\limits_{-\infty}^\infty 1\cdot \varphi(x) \,d x \\
  &= \langle 1, \varphi \rangle
  \Rightarrow \mathscr{F}\delta = 1
\end{align*}
  Notice, for the distribution that is infinitely concentrated its F.T. is uniformly spread out.

  \vspace{0.5cm}
  The \underline{Fourier transform of the shifted delta function, $\delta_a$}, is
\begin{align*}
  \langle \mathscr{F}\delta_a, \varphi \rangle
  &= \langle \delta_a, \mathscr{F}\varphi \rangle \\
  &= \mathscr{F}\varphi(a) \\
  &= \int\limits_{-\infty}^\infty e^{-2\pi i a x} \varphi(x) \,d x \\
  &= \langle e^{-2\pi i a x}, \varphi \rangle
  \Longrightarrow \mathscr{F}\delta_a = e^{-2\pi iax}
\end{align*}

  \vspace{0.5cm}
  The \underline{Fourier transform of the complex exponential} is
\begin{align*}
  \langle \mathscr{F}e^{2\pi iax}, \varphi \rangle 
  &= \langle e^{2\pi iax}, \mathscr{F}\varphi \rangle \\
  &= \int\limits_{-\infty}^\infty e^{2\pi iax} \mathscr{F}\varphi(x) \,d x  \\
  \intertext{This is the inverse F.T. of the classical F.T. evaluated at $a$} \\
  &= \varphi(a) \\
  &= \langle \delta_a, \varphi \rangle
  \Longrightarrow \mathscr{F}e^{2\pi iax} = \delta_a
\end{align*}
  Note that for $a=1 \longrightarrow \mathscr{F}(1) = \delta$.

  \vspace{0.5cm}
  The \underline{Fourier transform of cosine} is even and real
  $$\mathscr{F}\bigl(\cos(2\pi a x)\bigr) = \mathscr{F} \Biggl({1\over 2} \bigl( e^{2\pi iax} + e^{-2\pi iax} \bigr)\Biggr) = {1\over 2} (\delta_a + \delta_{-a})$$

  \vspace{0.5cm}
  The \underline{Fourier transform of sine} is odd and imaginary
  $$\mathscr{F}\bigl(\sin(2\pi a x)\bigr) = \mathscr{F} \Biggl( {1\over 2i} \bigl( e^{2\pi iax} - e^{-2\pi iax} \bigr) \Biggr) = {1\over 2i} (\delta_a - \delta_{-a})$$


  \subsection{The Derivative of a Distribution}
  %Lect 14: 

  The derivative of a distribution
  $$\langle T^\prime, \varphi\rangle$$
  is defined via the pairing.
  If $T^\prime$ is given by a function then the pairing is defined by integration
\begin{align*}
  \langle T^\prime, \varphi \rangle 
  &= \int\limits_{-\infty}^\infty T^\prime (x) \varphi(x)\,d x \\
  \intertext{Integration by parts, and using properties of Schwartz functions, gives} \\
  &= \underbrace{T(x)\varphi(x)\bigg|_{-\infty}^\infty}_{\text{$=0$, $\varphi(\pm\infty)=0$}} - \int\limits_{-\infty}^\infty T(x)\varphi^\prime(x) \,d x \\
  &= - \int\limits_{-\infty}^\infty T(x)\varphi^\prime(x) \,d x \\
  &= - \langle T, \varphi^\prime\rangle \\
\end{align*}
  Note the requirement on Schwartz functions, $\varphi \in \mathcal{S} \Rightarrow \varphi^\prime \in \mathcal{S}$.
 
  Consider the derivative of a function that has no business having a derivative, which is just a jump discontinuity
  $$ u(x) =
     \begin{cases}
	     1 & \text{, $x > 0$} \\
	     0 & \text{, $0 \leq 0$}
     \end{cases}
  $$
  , the Heaviside (unit step) function.
  The Heaviside function induces a distribution if the integral exists
  $$\langle u, \varphi\rangle = \int\limits_{-\infty}^\infty u(x) \varphi(x) \,d x$$
  , ``and therefore has a derivative because all distributions have derivatives''(Osgood).

  So, the \underline{\bf derivative of the Heaviside function}, $u^\prime$, exists as a distribution.
\begin{align*}
  \langle u, \varphi\rangle
  = -\langle u, \varphi^\prime\rangle
  &= -\!\int\limits_{-\infty}^\infty u(x) \varphi^\prime(x) \,d x \\
  &= -\!\int\limits_0^\infty \varphi^\prime(x) \,d x \\
  &= -\Bigl[\varphi(x)\Bigr]_0^\infty = - (\varphi(\infty) - \varphi(0)) \\
  &= \varphi(0) = \langle\delta, \varphi\rangle
	\implies u^\prime = \delta \\
\end{align*}

  To take the Heaviside one step further in complexity, consider
  $$ \sgn(x) =
     \begin{cases}
	     1  & \text{, $x > 0$} \\
	     0  & \text{, $0 = 0$} \\
	     -1 & \text{, $0 < 0$}
     \end{cases}
  $$
  , the signum function.

  The \underline{\bf derivative of the signum (aka sign) function} is
\begin{align*}
  \langle\sgn^\prime, \varphi\rangle
  = -\langle\sgn, \varphi^\prime\rangle
  &= -\Biggl( \!\int\limits_{-\infty}^0 (-1) \varphi^\prime(x) \,d x
  + \!\int\limits_{-\infty}^0 (+1) \varphi^\prime(x) \,d x \Biggr) \\
  &= -\biggl( \bigl(-\varphi(0) -(-\varphi(-\infty))\bigr) + \bigl(\varphi(\infty) - \varphi(0)\bigr) \biggr) \\
  &= \varphi(0) + \varphi(0) = 2\varphi(0) \\
  &= \langle 2\delta, \varphi\rangle
	\implies \sgn^\prime(x) = 2\delta
\end{align*}

  \underline{Applications to F.T.s}

  \underline{Derivative Theorem Pt. I}: 
  $$\boxed{\mathscr{F}(T^\prime) = (2\pi is)\mathscr{F}T}$$
\begin{align*}
  \langle\mathscr{F}T^\prime, \varphi\rangle &= \int\limits_{-\infty}^\infty \mathscr{F}T^\prime \varphi(t) \,d t \\
  &= \int\limits_{-\infty}^\infty\underbrace{\Biggl(\int\limits_{-\infty}^\infty T^\prime e^{-2\pi ist} \,d t \Biggr)}_{\text{By Parts}}\varphi(t)\,d t \\
  &= \int\limits_{-\infty}^\infty\Biggl( T e^{-2\pi ist} \bigg|_{-\infty}^\infty
  - (-2\pi is) \int\limits_{-\infty}^\infty T e^{-2\pi ist} \,d t\Biggr)\varphi(t)\,d t \\
  &= \int\limits_{-\infty}^\infty\Biggl( \underbrace{T e^{-2\pi ist} \varphi(t)\biggr|_{-\infty}^\infty}_{\text{$=0$}}
  + (2\pi is) \int\limits_{-\infty}^\infty T e^{-2\pi ist} \varphi(t)\,d t\Biggr)\,d t \\
  &= (2\pi is) \int\limits_{-\infty}^\infty \mathscr{F}T\varphi(t)\,d t
  = (2\pi is) \langle\mathscr{F}T, \varphi\rangle \\
\end{align*}

  \vspace{0.5cm}
  \underline{Derivative Theorem Pt. II}:
  $$\boxed{(\mathscr{F}T)^\prime = \mathscr{F}(-2\pi is)T}$$
\begin{align*}
  \langle(\mathscr{F}T)^\prime, \varphi\rangle &= -\langle\mathscr{F}T, \varphi^\prime\rangle \\
  &= -\!\int\limits_{-\infty}^\infty \Biggl[ \int\limits_{-\infty}^\infty T e^{-2\pi ist}\,d t \Biggr] \varphi^\prime(t) \,d t \\
  &= -\!\int\limits_{-\infty}^\infty \Biggl( \int\limits_{-\infty}^\infty \varphi^\prime(t) e^{-2\pi ist}\,d t \Biggr) T \,d t \\
  &= -\!\int\limits_{-\infty}^\infty \Biggl( \Bigl[ e^{-2\pi ist} \varphi(t) \Bigr]_{-\infty}^\infty -
  \int\limits_{-\infty}^\infty (-2\pi is) \varphi(t) e^{-2\pi ist}\,d t \Biggr) T \,d t \\
  &= -\!\int\limits_{-\infty}^\infty (2\pi is) \int\limits_{-\infty}^\infty \varphi(t) e^{-2\pi ist} \,d t T \,d t \\
  &= -(2\pi is) \!\int\limits_{-\infty}^\infty \int\limits_{-\infty}^\infty T e^{-2\pi ist} \,d t \varphi(t) \,d t \\
  &= -2\pi is \langle\mathscr{F}T, \varphi\rangle \\
\end{align*}

  Use the derivative theorem pt. 1 to find the \underline{\bf Fourier transform of the $\sgn$ function}
\begin{align*}
  \intertext{Based on the formula for $\sgn^\prime = 2 \delta$} 
  \mathscr{F}(\sgn^\prime) = \mathscr{F}(2 \delta) = 2 \\
  \intertext{Using the derivative theorem}
  \mathscr{F}(\sgn^\prime) = (2\pi is) \mathscr{F}(\sgn) \\
  \intertext{Combining these two expressions}
  \mathscr{F}\sgn(s) = {1\over \pi is}
\end{align*}
  This derivation requires further justification
  (the function ${1\over s}$ has a singularity, to show this is a distribution requires a special definition for the pairing, the principal value distribution, refer to text).

  \vspace{0.5cm}
  The Fourier transform of the signum function provides a simple way to obtain the \ul{\bf Fourier transform of the Heaviside function}. 
\begin{align*}
  \intertext{Writing the Heaviside function in terms of the signum function, allowing the value at the origin to be $1/2$, as}
  u(x) = {1\over 2} (1 + \sgn(x)) \\
  \intertext{then its Fourier transform is given by} 
  \mathscr{F}(u(x)) &= {1\over 2} \mathscr{F}(1 + \sgn) \\
  &= {1\over 2} \biggl(\delta + {1\over\pi is}\biggr)
\end{align*}


  \subsection{The Shift Theorem for Distributions}

  $$\boxed{\langle \tau_{_b} \,T, \varphi \rangle = \langle T, \tau_{_{-b}} \,\varphi \rangle}$$

  Introduce the shift operator, $\tau_b$
  $$(\tau_{_b} \,\varphi)(x) = \varphi(x-b)$$

  Assume the tempered distribution, $T$, comes from a function $f(x-b)$ and the pairing is defined by integration
  $$\langle \tau_b f, \varphi\rangle = \langle f(x-b), \varphi\rangle = $$
  $$\int\limits_{-\infty}^\infty (\tau_b f)(x) \varphi(x) \,dx = \int\limits_{-\infty}^\infty f(x-b) \varphi(x) \,dx$$ 
  Use the $u$-substitution $u = x-b$
  $$= \int\limits_{-\infty}^\infty f(u) \varphi(u+b) \,du$$ 
  $$= \int\limits_{-\infty}^\infty f(x) (\tau_{-b} \varphi)(x) \,dx = \langle f, \tau_{-b} \varphi \rangle$$
  The switch from $x$ to $u$ introduces ambiguity that the ``variable-free'' angle-bracket notation avoids.

  The shift theorem for the Fourier transform of a tempered distribution, $T$, is
  $$\boxed{\mathscr{F}(\tau_b T) = e^{-2\pi ibx} \mathscr{F}T}$$
  $$\langle \mathscr{F}(\tau_b T), \varphi \rangle = \langle \tau_b T, \mathscr{F} \varphi \rangle$$
  $$= \langle T, \tau_{-b} \mathscr{F} \varphi \rangle$$
  $$\tau_{-b}(\mathscr{F}\varphi)(x) = \mathscr{F} \varphi(s+b)$$
  $$= \int\limits_{-\infty}^\infty e^{-2\pi i(s+b)x} \varphi(x) \,dx$$
  $$= \int\limits_{-\infty}^\infty e^{-2\pi isx} e^{-2\pi ibx} \varphi(x) \,dx$$
  $$= \mathscr{F}(e^{-2\pi ibx} \varphi)(s)$$
  Plugging in
  $$\langle T, \tau_{-b} \mathscr{F} \varphi \rangle$$
  $$= \langle T, \mathscr{F}( e^{-2\pi ibx} \varphi) \rangle$$
  $$= \langle \mathscr{F} T, e^{-2\pi ibx} \varphi \rangle$$
  $$= \langle e^{-2\pi ibx} \mathscr{F} T, \varphi \rangle$$

  It is instructive to examine the effect on the Dirac $\delta$-function...
  The shifted $\delta$-function is
  $$\langle \tau_a \delta, \varphi \rangle = \langle \delta, \tau_{-a} \varphi \rangle$$
  $$= (\tau_{-a} \varphi)(0)$$
  $$= \varphi(0 + a) = \langle \delta_a, \varphi \rangle$$
  So,
  $$\tau_a \delta = \delta_a$$
  The Fourier transform of the shifted $\delta$-function is
  $$\mathscr{F}(\tau_b \delta) = e^{-2\pi ibs} \mathscr{F} \delta = e^{-2\pi ibs}$$
  ...as expected.


  \subsection{The Scaling Theorem for Distributions}

  $$\boxed{\langle \varsigma_a f, \varphi\rangle
  = \biggl\langle f, {\varsigma_{_{1/a}} \over |a|} \,\varphi \biggr\rangle}$$

  Assume the tempered distribution comes from a function $f(ax)$ and the pairing is defined by integration
\begin{align*}
  \langle f(ax), \varphi\rangle &= \int\limits_{-\infty}^\infty f(ax) \varphi(x) \,dx \\
  \intertext{For $a > 0$, the $u$-substitution $u = ax$ gives}
  &= {1 \over a} \int\limits_{-\infty}^\infty f(u) \varphi\biggl({u\over a}\biggr) \,du \\
  \intertext{For $a < 0$, the $u$-substitution $u = ax$ gives}
  &= {1 \over a} \int\limits_\infty^{-\infty} f(u) \varphi\biggl({u\over a}\biggr) \,du \\
  &= {-1 \over a} \int\limits_{-\infty}^\infty f(u) \varphi\biggl({u\over a}\biggr) \,du \\
  \intertext{But $a$ is negative, so}
  &= {1 \over |a|} \int\limits_{-\infty}^\infty f(u) \varphi\biggl({u\over a}\biggr) \,du \\
  \intertext{To avoid ambiguity in notation caused by confering the factor of ${1\over a}$ to the argument of $\varphi(x)$ introduce the {\bf scaling operator}, $\varsigma$, which scales {\em the argument} of the function
  $$\varsigma_a \varphi(x) = \varphi(ax)$$}
  \intertext{Combining the results}
  \langle \varsigma_a f, \varphi\rangle &= \int\limits_{-\infty}^\infty (\varsigma_a f(x)) \varphi(x) \,dx \\
  &= {1\over |a|} \int\limits_{-\infty}^\infty f(x) (\varsigma_{_{1/a}} \varphi(x)) \,dx \\
  &= \biggl\langle f, {\varsigma_{_{1/a}} \over |a|} \, \varphi \biggr\rangle \\
\end{align*}

  \subsection{The Scaling Theorem for the Fourier Transform}

  $$\boxed{\mathscr{F}(\varsigma_a T) = {1\over |a|} \varsigma_{_{1/a}}(\mathscr{F} T)}$$
  (but what does this actually mean? How to implement this in practice?)
  Using the definition of the Fourier transform
\begin{align*}
  \langle \mathscr{F} (\varsigma_a T), \varphi\rangle &= \langle \varsigma_a T, \mathscr{F} \varphi \rangle \\
  \intertext{Applying the scaling theorem}
  &= \biggl\langle T, {\varsigma_{_{1/a}} \over |a|} \mathscr{F} \varphi \biggr\rangle
  \intertext{Applying the scaling theorem again (``Any property or operation on test functions may be extended to distributions''(p.12-13, \cite{Strichartz}))}
  &= \langle T, \mathscr{F} (\varsigma_a \varphi) \rangle \\
  \intertext{Using the definition of the Fourier transform again}
  &= \langle \mathscr{F} T, \varsigma_a \varphi \rangle \\
  \intertext{Applying the scaling theorem yet again}
  &= \biggl\langle {\varsigma_{1/a}\over |a|} \mathscr{F} T, \varphi \biggr\rangle
\end{align*}
  
  Honestly WFT, I hate this proof...
  
  NOW:... The scaling property of the delta function defined via the scaling operator
  What happens when the scaling operator is applied to the shifted delta function?...is this essentially what is happening in the diffraction example.
  
  
  \subsection{Multiplication and Convolution of Distributions}
  %Lect 14 (min 25:30) Multiplication and Convolution in the context of distributions.
  %(refer to text for details)
  Not all operations on functions carry over to distributions.
  % Multiplication:
  It is not generally true that multiplication of distributions makes sense.
  If $\mathcal{S}$, $\mathcal{T}$ are distributions, then $\mathcal{S}\mathcal{T}$ is not necessarily defined.
  Rather, a function times a distribution, $f \mathcal{T}$, using integration to specify the pairing
\begin{align*}
  \langle f \mathcal{T}, \varphi\rangle &= \int\limits_{-\infty}^\infty f(x) \mathcal{T}(x) \varphi(x) \,d x \\
  &= \int\limits_{-\infty}^\infty \mathcal{T}(x) f(x) \varphi(x) \,d x \\
  &= \langle \mathcal{T}, f \varphi\rangle
\end{align*}
  is more likley to be defined.
  In general, define $f \mathcal{T}$ as 
  $$\langle f \mathcal{T}, \varphi\rangle = \langle \mathcal{T}, f \varphi\rangle$$
  provided $f \varphi$ has the properties that define the class, is a test function.
  (this is actually necessary to derive the derivative theorem pt. 1)
  
  A special case of multiplying a function times a distribution is, $f \delta$
\begin{align*}
  \langle f \delta, \varphi\rangle &= \langle \delta, f \varphi\rangle \\
  &= f(0) \varphi(0) \\
  &= \langle f(0) \delta, \varphi\rangle \\
  &\Rightarrow \boxed{f \delta = f(0) \delta} \\
  \intertext{More generally,}
  \Aboxed{f \delta_a &= f(a) \delta_a}
\end{align*}
  This is the \underline{\em sampling property of the delta $\delta$-function}.

  % Convolution:
  Similarly, if $\mathcal{S}$ and $\mathcal{T}$ are distributions the convolution, $\mathcal{S} * \mathcal{T}$, is not always defined.
  There are some additional restrictions on $\mathcal{S}$ and $\mathcal{T}$ in order to define the pairing for the convolution of distributions.
  There are many cases where it is defined but what is most often considered is a function convolved with a distribution, $f * \mathcal{T}$.
  Importantly, the convolution theorem holds, that is $\mathscr{F}(f * \mathcal{T}) = (\mathscr{F} f) (\mathscr{F} \mathcal{T})$.

  A special case, which is particularly important is when $\mathcal{T} = \delta$
  $$\boxed{f * \delta = f}$$
  (\underline{\em the $\delta$-function serves as the identity element for convolution}).
  More generally, the \ul{\em shifting property of the $\delta$-function} is based on convolultion of a function with the shifted $\delta_a$-function
  $$(f * \delta_a)(x) = f(x - a)$$
  which produces a shifted version of the function.

  A case when the convolution of two distributions does makes sense is
  $$\delta_a * \delta_b = \delta_{a + b}$$
  the convolution of two $\delta$-functions. %(refer to text for derivation)
  Note that $(f * \delta_a) * \delta_b = f(x-a) * \delta_b = f(x-a-b)$.
  Furthermore, 
\begin{align*}
  f * \delta_{a+b} &= f(x-(a+b)) \\
  &= f(x-a-b))
\end{align*}
  , so it is consistent.


  \subsection{Application of the Fourier Transform to Diffraction} 
  %Lect 15

  source: monochromatic light source at large distance from the aperture plane such that waves impinging on the aperture plane can be regarded as plane waves.
  aperture plane: apertures on a flat surface generate waves of uniform strength ($E_0$), frequency, and (initially) phase - Huygens' principle (each aperture generates a new source).
  Image plane: diffraction pattern observed, based on field stregth $E$.
  Near-field (Fresnel) v. far-field (Fraunhofer) diffraction: based on the distance between the image and the source relative to the wavelength.
  (Another thing to consider is the aperture size relative to the wavelength.)
  Light is represented by electo(magnetic) oscillations $E e^{2\pi i\nu t}$, where $E$ is the magnitude of the electric field.

  {\bf Fraunhofer Diffraction}
  In Fraunhofer diffraction the light source is at a distance from the aperture plane large relative to the wave length, such that impinging wave fronts can be regarded as plane waves.
  Assume constant strength of the electric field, and uniform phase, at the aperture plane.
  Reduce the problem to a one dimensional problem by looking at the aperture plane sideways.
  Regarding a vertical distance, $\,d x$, as a small distance along the aperture, the real part of

  $$E_0 e^{2\pi i\nu t} \,d x$$

  represents the electric part of the electromagnetic field generated along the aperture.

  Phase change associated with separate paths to a point on the image plane is
  $$2\pi r\over \lambda$$
  , where $r$ is the distance from the aperture plane to the image plane.
  The magnitude of the electric field at a distance $r$ is
  $$\,d E = E_0 e^{2\pi i \nu t} e^{-2\pi i r/\lambda} \,d x$$
  Integrating over ($x$) the aperture plane
  $$E = \int\limits_{aperture} E_0 e^{2\pi i\nu t} e^{-2\pi ir/\lambda} \,d x$$
  One observes and experiments measure the magnitude of the electric field $|E|$, the total field strength at the image plane, so the time varying part becomes 1 and can be ignored
\begin{align*}
  = \int\limits_{aperture} e^{-2\pi ir/\lambda} \,d x
\end{align*}
  However, this expression is too complicated to be useful ...
  The Fraunhofer approximation to the diffraction experiment where the distance between the aperture plane and the image plane is large relative to the aperture size.
  Here, the difference in path length between points along the aperture is is approximately $\approx x\sin\theta$.
  Where, $d$ is the perpendicular from the longer path to the aperture of the shorter path.
  And, $\theta$ is the angle between the perpendicular ($d$) and the aperture plane %(figure at, Lect15: 31:15)

\begin{center}
\begin{tikzpicture}

% Aperture plane
\draw[line width=0.1cm] (-4,-4) -- (-4,-2);
\draw[line width=0.1cm] (-4,2) -- (-4,4);
\draw[dashed] (-4, 2) -- (-4, 0);
\draw[
	gray,
    -triangle 90,
    %line width=4mm,
    line width=2mm,
    %postaction={draw, line width=1cm, shorten >=1cm, -}
    postaction={draw, line width=0.5cm, shorten >=0.5cm, -}
] (-4.5,1.7)
	-- (-4.5,-2);
\node[anchor=east] (note) at (-5, 0) {``Integrate Along $x$''};

% Image plane
\draw[line width=0.1cm] (4,-4) -- (4,4);

% Paths taken by E-waves from aperture to image plane
\draw[->] (-4, 0) -- (4, 3);
\draw[->] (-4, 2) node[anchor=east] {$(x=0)$}
		-- (4, 3);
\node [anchor=south] (note) at (-0.5, 2.45) {$r_0$};
\node [anchor=south] (note) at (0, 1.5) {$r$};
\draw[-, red] (-4, 0)
		-- (-3.34, .25);

% Distance (d) between paths
\draw[dashed] (-4, 2)
		-- (-3.34, .25);
\node [anchor=west] (note) at (-3.55,1.1) {$d$};

% Context
% Distance between aperture and image planes
\draw[dashed] (-4, 0) 
		-- (4, 0);
% Angle between perpendicular and paths
\node [anchor=south] (note) at (-2.5,0) {$\theta$};
\draw [thick] (-2,0) arc[start angle=0, end angle=21,radius=2];
\draw[<->, dashed] (-4, 0) 
		-- (4, 0);

\end{tikzpicture}
\end{center}

\begin{align*}
  E &= \int\limits_{aperture} e^{-2\pi i(r_0 + x\sin\theta)/\lambda} \,d x \\
  &= e^{-2\pi ir_0/\lambda} \int\limits_{aperture} e^{-2\pi i{x\sin\theta/\lambda}} \,d x \\
  \intertext{Again, the complex exponential factor can be ignored.}
  \intertext{Introduce $p=\sin\theta/\lambda$}
  &= \int\limits_{apertures} e^{-2\pi ixp} \,d x
\end{align*}

  Introduce the ``aperture function''
  $$ A(x) =
     \begin{cases}
	     1 & \text{$x \in$ aperture} \\
	     0 & \text{otherwise}
     \end{cases}
  $$
  , and integrating over the entire aperture plane
  $$E = \int\limits_{-\infty}^\infty e^{-2\pi ixp} A(x) \,d x$$
  , gives the Fourier transform of the aperture function, $\mathscr{F}A(p)$.
  For far-field diffraction under the Fraunhofer approximation, the diffraction pattern (strength of the field, intensity of light) at the image plane is the magintude of the Fourier transform of the aperture function.

  \vspace{0.5cm}
  {\bf Single Slit Diffraction}:
  The aperture function for a slit of width $a$ is $$\Pi_a(x)$$
  The Fourier transform is
  $$a\sinc({ap})$$
  where $p=\sin\theta/\lambda$.
  The absolute value (magnitude/intensity) is observed at the image plane.

  \vspace{0.5cm}
  {\bf Diffraction from a Point Source}:
  The aperture function for a point source is $$A(x) = \delta$$
  The Fourier transform is
  $$\mathscr{F}\delta = 1$$
  The image plane is uniformly illuminated.

  \vspace{0.5cm}
  {\bf Young's Double Slit Experiment}:
  The aperture function is
  $$A(x) = \Pi_a(x - b/2) + \Pi_a(x + b/2)$$
  where the slits of width $a$ are separated by a distance $b$.
  The Fourier transform (using the modulation theorem) is
  $$a\sinc(ap)2\cos(\pi bp)$$
  where $p=\sin\theta/\lambda$.

  \vspace{0.5cm}
  {\bf X-ray Crystalography: a sampling primer}%1-dimensional crystal}
  %Lect 16: X-ray crystalography  
\iffalse
  Wilhelm Rontgen discovered X-rays 
  1. is it a wave?
  - wavelength on order of $10^{-8}$cm, which were too small to measure, ie: using diffraction grating.
  2. crystals
  - how to test conjecture that atomic structure (lattice) determines macroscopic structure?
  Max van Laue 1st studies of atomic structure of crystals via X-ray diffraction.
  spacing between atoms is comparable to X-ray wavelength.
  
  How to interpret the results of X-ray crystalography experiment.

  Diffraction pattern is influenced by the size of the aperture relative to the wavelength.
  If the spacing between atoms is on the order of the wavelength of the X-rays.
\fi

  Treat the 1-D crystal as a periodized version of the electron density of a single atom $\rho(x)$
  $$P_d(x) = \sum_{n\,=\,-\infty}^\infty \rho_d = \sum_{n\,=\,-\infty}^\infty \rho(x - nd)$$
  as an infinite array of identical atoms evenly spaced with spacing ($d$) along a line. 
  The shifted electron density can be written as the convolution of the electron density with a shifted $\delta$-function
  $$\rho_d = \rho(x - nd) = \rho(x)*\delta(x - nd)$$
  and write the periodization
\begin{align*}
  P_d(x) &= \sum_{n\,=\,-\infty}^\infty \rho(x - nd) \\
  &= \sum_{n\,=\,-\infty}^\infty \rho(x)*\delta(x - nd) \\
  &= \rho(x) * \Biggl(\sum_{n\,=\,-\infty}^\infty \delta(x - nd)\Biggr)
  \intertext{Introduce the Shah function/distribution (Bracewell 1999) of spacing, $d$}
  &= \rho(x)*\Sh_d(x)
\end{align*}

  The diffraction pattern is determined by the Fourier transform of $P_d$.
  By the convolution theorem
  $$\mathscr{F}(P_d) = \mathscr{F} (\rho(x) * \Sh_d) = (\mathscr{F}\rho) (\mathscr{F}\Sh_d)$$

  Now the problem reduces to, ``what is $\mathscr{F}\Sh$ ?''
  The Shaw function makes sense as a distribution
  $$\Sh(x) = \sum_{n\,=\,-\infty}^\infty \delta(x - n)$$
  in that it operates on a test function as
  $$\langle\Sh, \varphi\rangle = \int\limits_{-\infty}^\infty \Sh \varphi(x) \,dx 
	= \int\limits_{-\infty}^\infty \sum_{n\,=\,-\infty}^\infty \delta (x - n) \varphi(x) \,dx
	= \sum_{n\,=\,-\infty}^\infty \, \int\limits_{-\infty}^\infty \delta (x - n) \varphi(x) \,dx
	= \sum_{n\,=\,-\infty}^\infty\varphi(n)$$ 
  , and the sum converges (because $\varphi$ is rapidly decreasing), so the pairing makes sense.
  Because the $\Sh$-function makes sense as a distribution so does its Fourier transform
  $$\langle\mathscr{F}\Sh, \varphi\rangle
  = \langle\Sh, \mathscr{F}\varphi\rangle
  = \sum_{n\,=\,-\infty}^\infty \mathscr{F}\varphi(n)$$

  Note, classically this series does not converge
  $$\mathscr{F}\Sh = \sum_{n\,=\,-\infty}^\infty \mathscr{F}\bigl(\delta(x - n)\bigr)
  = \sum_{n\,=\,-\infty}^\infty e^{-i2\pi s n}$$
  although it makes sense as a distribution.
  %``However, this misses the point (the deepest fact known about the integers)!''

  The modern approach involves {\bf the Poisson summation formula} (one of the deep insights regarding the integers).
  If $\varphi$ is a rapidly decreasing function (there are other classes for which this applies as well), then
  $$\boxed{\sum_{n\,=\,-\infty}^\infty \varphi(n) = \sum_{n\,=\,-\infty}^\infty \mathscr{F}\varphi(n)}$$

  \underline{Proof}:
  \vspace{0.05cm}
\begin{align*}
  \intertext{The periodized version\cite{Strichartz} of $\varphi$ with period 1 is (why is it periodized? in what sense is it periodized? In the case of describing the crystal it was clear what periodization was for.  But why is it necessary to replicate just a function? I don't know if this answer is really enough, but the periodically replicated signal definitely has a Fourier transform!)}
  \phi(x) &= \sum_{n\,=\,-\infty}^\infty \varphi(x-n) \\
  \intertext{Expanding $\phi$ in a Fourier series}
  &= \sum_{n\,=\,-\infty}^\infty {\hat \phi}(n) e^{i2\pi sn} \\
  \intertext{where the $n^{th}$ Fourier coefficient is ${\hat \phi}(n) = \mathscr{F}\varphi(n)$}
  &= \sum_{n\,=\,-\infty}^\infty \mathscr{F}\varphi(n) e^{i2\pi sn} \\
  \intertext{Evaluating the two expressions for $\phi$ at $x$ equals zero
  $$\phi(0) = \sum_{n\,=\,-\infty}^\infty \varphi(-n) = \sum_{n\,=\,-\infty}^\infty \varphi(n)
  \qquad\text{and}\qquad
  \phi(0) = \sum_{n\,=\,-\infty}^\infty \mathscr{F}\varphi(n) $$}
  \intertext{allows them to be equated, and gives the Poisson summation formula}
  \Longrightarrow \sum_{n\,=\,-\infty}^\infty \varphi(n) &= \sum_{n\,=\,-\infty}^\infty \mathscr{F}\varphi(n)
\end{align*}

  Invoking the Poisson summation formula, the \ul{\bf Fourier transform of the Shah $\Sh$-function (as a distribution) is the Shah $\Sh$-function}	
  $$\langle\mathscr{F}\Sh, \varphi\rangle 
  = \langle\Sh, \mathscr{F}\varphi\rangle 
  = \sum_{n\,=\,-\infty}^\infty \mathscr{F}\varphi(n)
  = \sum_{n\,=\,-\infty}^\infty \varphi(n) = \langle\Sh, \varphi\rangle$$
  $$\boxed{\mathscr{F}\Sh = \Sh}$$
  
  The \underline{\bf Fourier transform of the Shah function with spacing ($d$)}
  $$\boxed{\mathscr{F}\Sh_d = {1\over d}\Sh_{1/d}}$$
  Scaling the shifting parameter of the $\delta$-function by $(d)$ 
\begin{align*}
  \Sh_d(x) &= \sum_{n\,=\,-\infty}^\infty \delta(x-nd) \\
  \intertext{Factoring the scaling factor}
  &= \sum_{n\,=\,-\infty}^\infty \delta\biggl(d\biggl({x\over d}-n\biggr)\biggr) \\
  \intertext{By the scaling property of the $\delta$-function ($\delta(ax) = (1/|a|)\delta(x)$)}
  &= {1\over d} \sum_{n\,=\,-\infty}^\infty \delta\biggl({x\over d}-n\biggr) \\
  &= {1\over d}\,\Sh\biggl({x\over d}\biggr) \\
  \intertext{Then, the Fourier transform is}
  \mathscr{F}(\Sh_d(x)) &= {1\over d}\,\mathscr{F}\Bigl(\Sh\biggl({x\over d}\biggr)\Bigr) \\
  \intertext{By the scaling theorem for Fourier transforms ($\mathscr{F}(\varkappa_a f) = (1/|a|)(\mathscr{F} (\varkappa_{1/a} f))$, where varkapp, $\varkappa$, scales {\em the argument} of the function it is applied to (***UPDATE notation from $\varsigma$ to $\varkappa$ in previous work on scaling theorems and distributions***))}
  &= {1\over d} \, \Bigl( (d) \varkappa_{d} ( \mathscr{F} \Sh ) \Bigr) \\
  &= \Sh(dx) = \sum_{n\,=\,-\infty}^\infty \delta(dx-n) \\
  &= \sum_{n\,=\,-\infty}^\infty \delta\biggl( d \biggl( x - {n\over d} \biggr)\biggr) \\
  &= {1\over d} \sum_{n\,=\,-\infty}^\infty \delta\biggl(x-{n\over d}\biggr) = {1\over d}\Sh_{1/d}(x)
\end{align*}
  Notice the reciprocal relationship between the original domain and the Fourier transform domain.
  The spacing {\em and the amplitude} are scaled by $(1/d)$.

  The crystal is represented by the periodized version of the electron density distribution for the atom with spacing ($d$)
  $$P_d(x) = \rho(x) * \Sh_d(x)$$
  The observation of the diffraction experiment produces the Fourier transform
\begin{align*}
  \mathscr{F}(\rho * \Sh_d) &=  (\mathscr{F}\rho) (\mathscr{F}\Sh_d) \\
  &= (\mathscr{F}\rho) \, \biggl({1\over d} \, \Sh_{1/d}\biggr) \\
  &= {1\over d} \, \bigl(\mathscr{F}\rho(x)\bigr) \, \sum_{n\,=\,-\infty}^\infty \delta\biggl(x-{n\over d}\biggr) \\
  &= {1\over d}\sum_{n\,=\,-\infty}^\infty \mathscr{F}\rho(x) \, \delta\biggl(x-{n\over d}\biggr) \\
  &= {1\over d}\,\sum_{n\,=\,-\infty}^\infty \mathscr{F}\rho\biggl({n\over d}\biggr) \delta\biggl(x-{n\over d}\biggr)
  %\intertext{Can't this last statement be simplified to...}
  %&= {1\over p}\,\sum_{k\,=\,-\infty}^\infty \mathscr{F}\rho\biggl({k\over p}\biggr)
  %\intertext{???}
\end{align*}
  The spacing between the spots on the diffraction pattern is proportional to {\em the inverse} of the spacing between the atoms.
  In general, the diffraction pattern is given by the Fourier transform of the aperture function.
  The representation for the crystal latice for X-ray diffraction should be based on the electron density, which is comprised mostly of empty space in comparison to the nuclei, which demarcate the aperture boundary.
  (While seemingly banal, this is an extremely important point based on the structure of the atom, without which leads to ambiguity when interpreting the diffraction pattern.)


  \section{Sampling and Interpolation}
  %Lect. 17: Sampling and Interpolation: to sample at a rate $p$ is to convolve with the Shah function with spacing $p$, $\Sh_p$ and to cut-off with the rect-function with width $p$, $\Pi_p$.

  *** note: ``interpolation'' in the context of the Fourier transform amounts to reconstructing the signal from the set of samples - the data points - but specifically from the frequency content extracted from the sampling process. ***

  The aperture can be understood as a sampling using a density to describe the measuring device.
  The density shifted by a distance $d$ can be written as the convolution with a shifted $\delta$-function
  $$\rho(x - d) = \rho(x)*\delta(x - d)$$
  The periodized version aka periodization is (***UPDATE?: does periodized mean ''replicated''?)
  %= \,\sum_{k\,=\,-\infty}^\infty
\begin{align*}
  P_d(x) &= \,\sum_{n\,=\,-\infty}^\infty \rho(x - nd) \\
  &= \,\sum_{n\,=\,-\infty}^\infty \rho(x) * \delta(x - nd) \\
  &= \rho(x) * \,\sum_{n\,=\,-\infty}^\infty \delta(x - nd) \\
  &= \rho(x) * \Sh_d(x)
\end{align*}

  The Shah function/distribution $\Sh$ (aka Dirac comb, pulse-train) is
  $$\Sh_d(x) = \sum_{n\,=\,-\infty}^\infty \delta(x-nd)$$

\begin{center}
\begin{tikzpicture}
% horizontal labels
\draw[<->] (-3.5,0)
	  -- (-2,0) node[anchor=north] {$-2d$}
	  -- (-1,0) node[anchor=north] {$-d$}
	  -- (0,0) node[anchor=north] {$0$}
	  -- (1,0) node[anchor=north] {$d$}
	  -- (2,0) node[anchor=north] {$2d$}
	  -- (3.5,0);

% Dirac functions
\draw[->, very thick] (-2,0) -- (-2,1.5);
\draw[->, very thick] (-1,0) -- (-1,1.5);
\draw[->, very thick] (0,0)  --  (0,1.5);
\draw[->, very thick] (1,0)  --  (1,1.5);
\draw[->, very thick] (2,0)  --  (2,1.5);

% annotation of continuation
\draw (-2.75,1) node{$\cdots$}
      (2.75,1)  node{...}; 
\end{tikzpicture}
\end{center}

  Three important properties of the $\Sh$-function are critical for solving the interpolation problem%(Lect 16 for derivations???)
\begin{enumerate}
  \item Sampling
        $$f(x) \, \Sh_d(x) = \sum_{n\,=\,-\infty}^\infty f(nd) \, \delta(x-nd)$$
  \item Periodizion
        $$f(x)*\Sh_d(x) = \sum_{n\,=\,-\infty}^\infty f(x - nd)$$
  \item Fourier transform
	$$\mathscr{F}(\Sh_d) = {1\over d}\,\Sh_{1/d} = \mathscr{F}^{-1} \,\Sh_d$$
\end{enumerate}

  \underline{\bf The Interpolation Problem}:  {\em Sampling a process evolving in time at equal intervals, can one interpolate all values of a signal from a discrete set of measurements?}
  ie: can one write the function 
  $f(t) \,\forall\; t$
  based on a discrete set of samples, $f(t_n)$ at $t_n$?
  An important issue to consider is, the more rapidly a signal oscillates the more uncertainty there is in interpolating the values between sample points.


  \subsection{Bandlimited Signals}

  **********************
  ** Basis for a Medium Article: Are there two definitions for the 'bandwidth' in use? **
  ************************
  {\bf ***** The definition of the bandwidth here is...not standard? wrong? domain specific? ... And not what is used in the treatment of the DFT...?} ref. (Osgood, The FT and Applicaitons, CH 5.6, Sampling and Interpolation for Bandlimited Signals, p.225).
  {\bf However, it is important to point out that in Osgood's definition, the reciprocal of the bandwidth is the sample spacing.  Which is an important mental check...}

  {\bf The wikipedia definition of bandwidth (signal processing) is "the difference between the upper and lower frequencies in a continuous band of frequencies", which sounds like it at least agrees with Osgood's definition.}

  \underline{\bf For bandlimited signals the interpolation problem can be solved exactly}.
  A function $f(t)$ is ``bandlimited'' if
  $$(\mathscr{F}f)(s) \equiv 0 \qquad \textrm{for}\, |s| \ge \Omega/2$$
  where the smallest such ($\Omega$) is the ``{\em \gls{bandwidth}}''.
  
\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
	\begin{axis} [
			enlarge x limits=false, enlarge y limits=false,
  		width=10cm, height=5cm,
			axis y line=middle, axis x line=bottom,
  		xmin=-2.5, xmax=2.5,
  		ymin=0, ymax=2,
			xtick={-2,-1,0,1,2},
			xticklabels={$-\Omega$,$-\Omega/2$,$0$,$\Omega/2$,$\Omega$},
			yticklabel style = {font=\small},
			xticklabel style = {font=\small}
 		]

  	\addplot [domain=-3:3, smooth, thick] {-2*x^4 - x^3 + x^2 + x + 1};

  	\draw (1.25,1.6) node{$\sim\!(\mathscr{F}f)(s)$};

	\end{axis}
\end{tikzpicture}
\end{center}
\caption{
  	Conceptual representation of the Fourier transform of $f$ (this image is not to be taken literally, because the Fourier transform is in general complex).  For $\mathbb{R}$eal signals, the Fourier transform in the frequency domain is symmetric.
}
\end{figure}

  To derive the expression for the signal reconstruction start with the periodization of the Fourier transform by convolution with $\Sh_\Omega$.

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
  \begin{axis} [
			enlarge x limits=false, enlarge y limits=false,
 	  		width=10cm, height=5cm,
			axis y line=middle, axis x line=bottom,
			xmin=-3, xmax=3,
			ymin=0, ymax=2,
			xtick={-2,-1,0,1,2},
			xticklabels={$-\Omega$,$-\Omega/2$,$0$,$\Omega/2$,$\Omega$},
			yticklabel style = {font=\small},
			xticklabel style = {font=\small}
		]

  	\addplot [domain=-1:1, smooth, thick] {-2*x^4 - x^3 + x^2 + x + 1};
  	\addplot [domain=1:3, smooth, thick] {-2*(x-2)^4 - (x-2)^3 + (x-2)^2 + (x-2) + 1};
  	\addplot [domain=-3:-1, smooth, thick] {-2*(x+2)^4 - (x+2)^3 + (x+2)^2 + (x+2) + 1};

	% Label
  	\draw (1.5,1.75) node{$(\mathscr{F}f) * \Sh_\Omega$};
  \end{axis}
\end{tikzpicture}
\end{center}
\caption{The periodization of the Fourier transform.}
\end{figure}

  There is no overlap between copies because the spacing between $\delta$'s of the Shah function $\Sh_\Omega$ is $\Omega$, and $\mathscr{F}\!f$ is 0 outside $[-\Omega/2, \Omega/2]$.

  To recover the Fourier transform ($\mathscr{F}\!f$) multiply the periodization (in the frequency domain) by a boxcar of width $\Omega$ (the bandwidth), $\Pi_\Omega$
\begin{equation}
  \boxed{ \mathscr{F}\!f = \Pi_\Omega \,(\mathscr{F}\!f * \Sh_\Omega) }
\end{equation}

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
  \begin{axis} [
			enlarge x limits=false, enlarge y limits=false,
 	  		width=10cm, height=5cm,
			axis y line=middle, axis x line=bottom,
			xmin=-3, xmax=3,
			ymin=0, ymax=2,
			xtick={-2,-1,0,1,2},
			xticklabels={$-\Omega$,$-\Omega/2$,$0$,$\Omega/2$,$\Omega$},
			yticklabel style = {font=\small},
			xticklabel style = {font=\small}
		]

  	\addplot [domain=-1:1, smooth, thick] {-2*x^4 - x^3 + x^2 + x + 1};
  	\addplot [domain=1:3, smooth, thick] {-2*(x-2)^4 - (x-2)^3 + (x-2)^2 + (x-2) + 1};
  	\addplot [domain=-3:-1, smooth, thick] {-2*(x+2)^4 - (x+2)^3 + (x+2)^2 + (x+2) + 1};

		% Indicate rect-function of width p^prime
		\draw [dashed, thick, color=black] (-1,0) -- (-1,1)
								-- (-1,1) -- (1,1)
								--  (1,1) -- (1,0);

		% Label
  	\draw (1.2,1.1) node{\small$\Pi_{\Omega}$};
  	\draw (1.5,1.75) node{$(\mathscr{F}f) * \Sh_\Omega$};
  \end{axis}
\end{tikzpicture}
\end{center}
\caption{The periodization of the Fourier transform $(\mathscr{F}\!f)*\Sh_\Omega$ (solid curves), and the boxcar of width $\Omega$, $\Sh_Omega$ (dashed lines).}
\end{figure}

  To recover the original signal take the inverse Fourier transform
\begin{flalign}
  % latex ref used: https://latex-tutorial.com/align-equations/
  \Aboxed{
f = \mathscr{F}^{^{-1}}(\mathscr{F}\!f) &= \mathscr{F}^{^{-1}} (\Pi_\Omega \,(\mathscr{F}\!f * \Sh_\Omega))
} \\
  &= (\mathscr{F}^{^{-1}} (\Pi_\Omega )) * (\mathscr{F}^{^{-1}}(\mathscr{F}\!f * \Sh_\Omega)) \nonumber
\end{flalign}

\begin{itemize}
  \item The first part is
	\begin{align*}
		\mathscr{F}^{^{-1}} (\Pi_\Omega ) &= \mathscr{F}\Bigl( \, \Pi_\Omega^{(-)} \, \Bigr) \quad\text{ (by duality)} \\
		&= \mathscr{F}(\, \Pi_\Omega \,) \qquad\text{  (because $\Pi$ is even)} \\
		&= \Omega \,\sinc(\,\Omega \, t\,) \quad\text{(the Fourier transform of the boxcar function)}
	\end{align*}

  \item The second part is
\begin{align*}
  \mathscr{F}^{^{-1}}((\mathscr{F}\!f) * \Sh_\Omega) &= (\mathscr{F}^{^{-1}}\mathscr{F}\!f)(\mathscr{F}^{^{-1}}\Sh_\Omega) \\
\intertext{By the scaling property of the Fourier transform, $\mathscr{F}(\varkappa_{(a)} f) = (\varkappa_{(1/a)}/|a|) (\mathscr{F} f)$ (where $\varkappa_{(a)}$ scales {\em the argument} of the function by a factor of $(a)$)}
  &= f(t) \, {1\over \Omega} \,\Sh_{1/\Omega}(t) \\
  \intertext{Using the sampling property of the $\Sh$-function (in the time domain)}
  &={1\over \Omega}\, f(t) \sum_{n\,=\,-\infty}^\infty \delta\biggl(t- {n\over \Omega}\biggr) \\
  &={1\over \Omega} \sum_{n\,=\,-\infty}^\infty f\biggl({n\over \Omega}\biggr) \,\delta\biggl( t - {n\over \Omega}\biggr)
\end{align*}
\end{itemize}

  The signal can now be written as
\begin{align*}
  f(t) &= \Bigl( \Omega \,\sinc(\,\Omega t\,) \Bigr) * \Biggl( {1\over \Omega} \, \sum_{n\,=\,-\infty}^\infty f\biggl({n\over \Omega}\biggr) \,\delta\biggl( t - {n\over \Omega}\biggr) \Biggr) \\
  \intertext{The function $f$ is unaffected by distributing the convolution inside the summationas, only the $\sinc()$ function and the $\delta$-function contain ($t$)}
  &= \sum_{n\,=\,-\infty}^\infty f\biggl({n\over \Omega}\biggr) \sinc(\Omega \, t) * \delta\biggl( t - {n\over \Omega}\biggr) \\
  \intertext{Use the shifting property of the $\delta$-function to apply the convolution (see section on the Dirac $\delta$-function)}
  &= \sum_{n\,=\,-\infty}^\infty f\biggl({n\over \Omega}\biggr) \sinc\Biggl(\Omega \, \biggl( t - {n\over \Omega}\biggr)\Biggr)
\end{align*}

  This formula, which specifies the function at all values of $t$ based on the sampled values, is known as the {\bf Sampling (Nyquist-Shannon-Whittaker) Theorem/Formula}
\begin{equation}
  \boxed{f(t) = \sum_{n\,=\,-\infty}^\infty f\biggl({n\over \Omega}\biggr) \sinc\Biggl(\Omega \, \biggl(t - {n\over \Omega}\biggr)\Biggr)}
\end{equation}
  Writing out the $\sinc$ function
  $$f(t) = \sum_{n\,=\,-\infty}^\infty f(t_n) \, {\sin \bigl(\,\Omega\,\pi \, (t - t_n)\,\bigr) \over \Omega\,\pi \, (t - t_n)}$$

  There is a fair amount of jargon associated with the variables pertaining to this formula.
  The values, $f(n/\Omega)$, are the sample points.
  The bandwidth, ($\Omega$), is a frequency referred to in (Hertz) samples per second.
  The $\Omega$ used here is the same as that used in Briggs and Henson\cite{BriggsHenson}, and is the same as the variable $p$ used by Osgood\cite{Osgood}.
  To recall the magnitude of the bandwidth, refer to the conceptual depiction of the Fourier transform.
  The bandwidth is the ``distance'' (in units of frequency) between $\omega_{max} = \Omega/2$ and $\omega_{min} = -\Omega/2$; between $\nu_{max}$ and $\nu_{min}$ or $2 \nu_{max}$ in the Osgood notation\cite{Osgood}, $2 B$ on the wikipedia page\cite{wikipedia}.
  The variable $\omega_{max} = \Omega/2$, $\nu_{max}$ (Osgood\cite{Osgood}), $B$ (wikipedia\cite{wikipedia}), is the highest frequency that can be resolved for a given $\Omega$, and is called the Nyquist frequency.

  Here there is a discrepancy between the definition of the Nyquist frequency stated by Osgood\cite{Osgood} and Briggs and Hesnen\cite{BriggsHenson}.
  For Briggs and Hensen the ``Nyquist frequency is the highest frequency that can be resolved using a given sample spacing $\triangle x$'' (p.97, Ch.3, subsection on Aliasing, \cite{BriggsHenson})
  $$\Omega / 2$$
  For Osgood (p. 366, Ch 6. Sampling and Interpolation) the Nyquist frequency is 
  $$2 \nu_{max}$$
  Both statements cannot be true but Briggs and Hensen definition is preferable.

  The definition of the bandwidth is so important because $1/\Omega$ determines the grid spacing between sample points, $\triangle t$ for time and $\triangle x$ for space, denoted as $T$ on the wikipedia page\cite{wikipedia}.
  Counterintuitively, the largest grid spacing for which the sampling formula is true is called the Nyquist (or critical) sampling rate (p.96)\cite{BriggsHenson} - although technically this quantity is not a rate.
  This leads to ambiguous statements like the Nyquist sampling rate is the ``lowest'' sampling rate that can reconstruct the signal.
  This statement makes sense from the stand point of a frequency, but the Nyquist rate is not a frequency, and the meaning of ``low'' in relation to grid spacing is not unambiguous.

  The sampling theorem can be written without explicit reference to the frequency domain.
  The substitution, $n/\Omega = t_n$, gives the formula as written by Osgood (Ch.5 Sampling and Interpolation)\cite{Osgood} (in terms of $\Omega$ instead of $p$)
  $$f(t) = \sum_{n\,=\,-\infty}^\infty f(t_n) \sinc\bigl(\Omega (t - t_n)\bigr)$$
  The bandwidth, $\Omega$, is related to the sample spacing, $\triangle t$, as $\triangle t = 1/\Omega$, which gives the wikipedia formula ($\triangle t$ here is equivalent to $T$ in the wikipedia article)
  $$f(t) = \sum_{n\,=\,-\infty}^\infty f(t_n) \sinc\biggl({t - t_n \over \triangle t}\biggr)$$
  Writing out the $\sinc$ function gives the Briggs and Henson formula\cite{BriggsHenson} (in time, $t$, rather than, $x$, in space)
  $$f(t) = \triangle t \sum_{n\,=\,-\infty}^\infty f(t_n) \, {\sin (\pi (t - t_n)/\triangle t) \over \pi (t - t_n)}$$

  The sampling formula is the source of significant insights into the model, as well as a source of intuition working with the discrete Fourier transform (DFT).
  \ul{\em Most importantly the signal cannot be both time-limited and band-limited (note the mathematical model and reality disagree, in practice signals {\em are} limited in time and in frequency)}.
  Recall, the sampling theorem is true for band-limited signals.
  If the Fourier transform is zero outside the bandwidth (at frequencies beyond the absolute value of the Nyquist frequency)
  $$(\mathscr{F}f)(s) \equiv 0 \quad\text{ for }\quad |s| \ge \Omega/2$$
  then $f(t)$ cannot be limited in time
  $$f(t) \not\equiv 0 \quad\text{beyond some $t$ (large)}$$
  (does not have compact support) goes on forever.
  To see this, apply the rationale used to derive the sampling formula from the Fourier transform without periodization (Osgood, The Fourier Transform and its Applications, Lect. 18)
\begin{align*}
  \mathscr{F}f &= \Pi_\Omega (\mathscr{F}f) \\
  \mathscr{F}^{^{-1}}\mathscr{F}f &= \mathscr{F}^{^{-1}}(\Pi_\Omega (\mathscr{F}f)) \\
  f(t) &= \Omega \sinc(\Omega t) * f(t_{sampled})
\end{align*}
  The $\sinc$ function is never identically 0 beyond a certain point so the right-hand side goes on forever.  %(more rigorous argument in notes)
  The $\sinc$ will not do this - act as the identity element for convolution - for all signals but it will for band-limited signals.

  Likewise, if the signal is limited in time, 
  $$f(t) \equiv 0 \quad\text{ for }\quad |t| \ge q/2 \quad\text{(outside some range)}$$ 
  then the Fourier transform cannot be limited in frequency
  $$(\mathscr{F}f)(s) \not\equiv 0 \quad\text{beyond some $s$ (large)}$$
  (does not have compact support).
  (Refer to Osgood text for better proofs).

  {\bf\ul(Some considerations to check off the list in what follows:)}
  Sampling theorem is still true for a greater bandwidth/ more sampling (sample points spaced closer together).
  But what happens if the sample rate is too low?
  Interpolation relies on an infinite number of sample points.
  In practice, however, only a finite number of sample points is possible, which introduces error such that the sampling theorem becomes an approximation rather than an equality.


  \subsection{Aliasing and Interpolation}

  A common problem is the signal is band-limited, but the chosen bandwidth ($\Omega^\prime$) is too small.
  
\begin{center}
\begin{tikzpicture}
	\begin{axis} [
			enlarge x limits=false, enlarge y limits=false,
  		width=8cm, height=5cm,
			axis y line=middle, axis x line=bottom,
  		xmin=-1.5, xmax=1.5,
  		ymin=0, ymax=2,
			xtick={-0.95,0,.95},
			xticklabels={$-\Omega^{\,\prime}/2$,$0$,$\Omega^{\,\prime}/2$},
			yticklabel style = {font=\small, xshift=-0.15cm},
			xticklabel style = {font=\small}
 		]

  	\addplot [domain=-3:3, smooth, thick] {-2*x^4 - x^3 + x^2 + x + 1};

  	\draw (0.475,1.7) node{\small$\sim\!(\mathscr{F}\! f)(s)$};

		% Indicate chosen bandwidth as too small
		\draw [dashed] (-0.95,0) -- (-0.95,2);
		\draw [dashed]  (0.95,0) --  (0.95,2);

	\end{axis}
\end{tikzpicture}
\end{center}

  Performing the periodization $(\mathscr{F}\!f * \Sh_{\Omega^\prime})$ shifting by $\Omega^\prime$ the shifted copies overlap, and overlapping contributions add up (shown in red).

\begin{center}
\begin{tikzpicture}
  \begin{axis} [
			enlarge x limits=false, enlarge y limits=false,
 	  	width=10cm, height=5cm,
			axis y line=middle, axis x line=bottom,
			xmin=-3, xmax=3,
			ymin=0, ymax=2,
			xtick={-1.9,-0.95,0,0.95,1.9},
			xticklabels={$-\Omega^\prime$,$-\Omega^\prime/2$,$0$,$\Omega^\prime/2$,$\Omega^\prime$},
			yticklabel style = {font=\small},
			xticklabel style = {font=\small}
		]

		% Original signal cut-off at overlaps
  	\addplot [domain=-0.9:0.9, smooth, thick] {-2*x^4 - x^3 + x^2 + x + 1};

		% Right-Shifted copy signal cut-off at overlap
  	\addplot [domain=1.0:2.8, smooth, thick] {-2*(x-1.9)^4 - (x-1.9)^3 + (x-1.9)^2 + (x-1.9) + 1};

		% Sum of the overlap between original and right-shifted signals 
  	\addplot [domain=0.9:1.0, smooth, thick, color=red] {-2*x^4 - x^3 + x^2 + x + 1 -2*(x-1.9)^4 - (x-1.9)^3 + (x-1.9)^2 + (x-1.9) + 1};

		% Right-Shifted copy signal cut-off overlap with the original
  	\addplot [domain=-2.8:-1.0, smooth, thick] {-2*(x+1.9)^4 - (x+1.9)^3 + (x+1.9)^2 + (x+1.9) + 1};

		% Sum of the overlap between original and right-shifted signals 
  	\addplot [domain=-1.0:-0.9, smooth, thick, color=red] {-2*x^4 - x^3 + x^2 + x + 1 -2*(x+1.9)^4 - (x+1.9)^3 + (x+1.9)^2 + (x+1.9) + 1};

		% Label
  	\draw (1.25,1.75) node{\small$(\mathscr{F}\! f * \Sh_{\Omega^\prime})$};

  \end{axis}
\end{tikzpicture}
\end{center}

  Cutting off at $\Omega^\prime$ using the rect-function (shown by dashed lines in blue) does not recover the original Fourier transform, because $\mathscr{F}\!f(s) \ne \Pi_{\Omega^\prime} (\mathscr{F}\!f * \Sh_{\Omega^\prime})$

\begin{center}
\begin{tikzpicture}
  \begin{axis} [
			enlarge x limits=false, enlarge y limits=false,
 	  	width=10cm, height=5cm,
			axis y line=middle, axis x line=bottom,
			xmin=-3, xmax=3,
			ymin=0, ymax=2,
			xtick={-1.9,-0.95,0,0.95,1.9},
			xticklabels={$-\Omega^\prime$,$-\Omega^\prime/2$,$0$,$\Omega^\prime/2$,$\Omega^\prime$},
			yticklabel style = {font=\small},
			xticklabel style = {font=\small}
		]

		% Original signal cut-off at overlaps
  	\addplot [domain=-0.9:0.9, smooth, thick] {-2*x^4 - x^3 + x^2 + x + 1};

		% Right-Shifted copy signal cut-off at overlap
  	\addplot [domain=1.0:2.8, smooth, thick] {-2*(x-1.9)^4 - (x-1.9)^3 + (x-1.9)^2 + (x-1.9) + 1};

		% Sum of the overlap between original and right-shifted signals 
  	\addplot [domain=0.9:1.0, smooth, thick, color=red] {-2*x^4 - x^3 + x^2 + x + 1 -2*(x-1.9)^4 - (x-1.9)^3 + (x-1.9)^2 + (x-1.9) + 1};

		% Right-Shifted copy signal cut-off overlap with the original
  	\addplot [domain=-2.8:-1.0, smooth, thick] {-2*(x+1.9)^4 - (x+1.9)^3 + (x+1.9)^2 + (x+1.9) + 1};

		% Sum of the overlap between original and right-shifted signals 
  	\addplot [domain=-1.0:-0.9, smooth, thick, color=red] {-2*x^4 - x^3 + x^2 + x + 1 -2*(x+1.9)^4 - (x+1.9)^3 + (x+1.9)^2 + (x+1.9) + 1};

		% Indicate rect-function of width p^prime
		\draw [dashed, thick, color=blue] (-0.95,0) -- (-0.95,1)
								-- (-0.95,1) -- (0.95,1)
								--  (0.95,1) -- (0.95,0);

		% Label
  	\draw (1.2,1.1) node{\small$\Pi_{\Omega^\prime}$};

  \end{axis}
\end{tikzpicture}
\end{center}


  Taking the inverse Fourier transform $\mathscr{F}^{-1}(\Pi_{\Omega^\prime} (\mathscr{F}f * \Sh_{\Omega^\prime}))$ gives
  $$g(t) = \sum_{k\,=\,-\infty}^\infty f\biggl({k\over \Omega^\prime}\biggr) \sinc\biggl( \Omega^\prime \biggl(t - {k\over \Omega^\prime} \biggr)\biggr) \ne f(t)$$
  , which does not equal $f(t)$.
  Nevertheless, $g(t)$ and $f(t)$ agree {\em at the sample points} ${m\over \Omega^\prime}$ where $m \in \mathbb{Z}$.
  $$g\biggl({m\over \Omega^\prime}\biggr) = \sum_{k\,=\,-\infty}^\infty f\biggl({k\over \Omega^\prime}\biggr) \sinc\biggl( \Omega^\prime \biggl({m\over \Omega^\prime} - {k\over \Omega^\prime} \biggr)\biggr)$$
  Note:
  $$\sinc(m - k) = 
     \begin{cases}
	     1 & \text{$m = k$} \\
	     0 & \text{$m \ne k$}
     \end{cases} \quad\text{, where $k, m \in \mathbb{Z}$}$$
  In this sense $g(t)$ is an {\em alias} for $f(t)$.


  \underline{Example}: $f(t) = \cos\bigl({9\pi\over 2} t\bigr)$

  The Fourier transform is 
  $${1\over 2} (\delta(s + 9/4) + \delta(s - 9/4))$$

\begin{center}
\begin{tikzpicture}
% horizontal labels
\draw[<->] (-3.5,0)
	  -- (-9/4,0) node[anchor=north] {$-9/4$}
	  -- (0,0) node[anchor=north] {$0$}
	  -- (9/4,0) node[anchor=north] {$9/4$}
	  -- (3.5,0);

% Delta functions
\draw[->, very thick] (-9/4,0) -- (-9/4,1.5);
\draw[->, very thick] (9/4,0)  --  (9/4,1.5);

\end{tikzpicture}
\end{center}

  Consider sampling at a rate of $\Omega=1$.
  (\ul{\em To sample at a rate of $\Omega=1$ means to convolve with the Shah function $\Sh_1$ and to form the cut-off with the indicator function $\Pi_1$}).
  $${1\over 2} (\delta(s + 9/4) + \delta(s - 9/4)) * \sum_{k\,=\,-\infty}^\infty \delta(x - k)$$

\begin{figure}
\begin{center}
\begin{tikzpicture}
% horizontal labels
\draw[<->] (-3.5,0)
	  -- (-9/4,0) node[anchor=north] {${-9\over4}$}
	  -- (-7/4,0) node[anchor=north] {${-7\over4}$}
	  -- (-5/4,0) node[anchor=north] {${-5\over4}$}
	  -- (-3/4,0) node[anchor=north] {${-3\over4}$}
	  -- (-1/4,0) node[anchor=north] {${-1\over4}$}
	  %-- (0,0) node[anchor=north] {$0$}
	  -- (3/4,0) node[anchor=north] {${3\over4}$}
	  -- (1/4,0) node[anchor=north] {${1\over4}$}
	  -- (5/4,0) node[anchor=north] {${5\over4}$}
	  -- (7/4,0) node[anchor=north] {${7\over4}$}
	  -- (9/4,0) node[anchor=north] {${9\over4}$}
	  -- (3.5,0);

% Delta functions
\draw[->, very thick] (-9/4,0) -- (-9/4,1.5);
\draw[->, very thick] (-7/4,0) -- (-7/4,1.5);
\draw[->, very thick] (-5/4,0) -- (-5/4,1.5);
\draw[->, very thick] (-3/4,0) -- (-3/4,1.5);
\draw[->, very thick] (-1/4,0) -- (-1/4,1.5);
\draw[->, very thick] (1/4,0) -- (1/4,1.5);
\draw[->, very thick] (3/4,0) -- (3/4,1.5);
\draw[->, very thick] (5/4,0) -- (5/4,1.5);
\draw[->, very thick] (7/4,0) -- (7/4,1.5);
\draw[->, very thick] (9/4,0)  -- (9/4,1.5);

% Indicate rect-function of width p^prime
\draw [dashed, thick, color=blue] (-0.5,0) -- (-0.5,1)
								-- (-0.5,1) -- (0.5,1)
								--  (0.5,1) -- (0.5,0);

% annotation of continuation
\draw (-2.75,1) node{$\cdots$}
      (2.75,1)  node{...}; 

\end{tikzpicture}
\end{center}
\caption{Periodization of the Fourier transform of the signal with the Shaw function $\Sh_1$ and cut-off with $\Pi_1$ (blue).}
\end{figure}

  Recall, $\delta_a * \delta_b = \delta_{a+b}$.
  Only two $\delta$'s in the periodization satisfy $-1/2 < s  < 1/2$
  $$\Pi_1 (\mathscr{F}\!f * \Sh_1) = {1\over 2} (\delta(s+1/4) + \delta(s-1/4))$$

  Taking the inverse Fourier transform gives $g(t) = \cos({\pi\over 2} t) \ne f(t) = \cos({9\pi\over 2} t)$, but the two equations agree at the sample points for $t=\pm1, \pm2, \pm3, \ldots$.
  (...check)

  \section{The DFT}
  %Lect 19
  %Side: Range of human ear $20 - 20\text{k Hz}$, so the appropriate sampling rate is $40\text{k Hz}$.

\iffalse
  REMEMBER HOW DIFFICULT THIS WAS...
  ************ The big problem with Osgood notation is that he has two different definitions for the bandwith when working with the the Fourier transform versus the DFT.
  For the continuous Fourier transform the bandwidth is $p$, or for me $\Omega$.
  But for the discrete Fourier transform the bandwith is $B$, where $2B = \Omega$.
  Clearly $p$ and $B$ are not equivalent, rather $B = p/2$.
  So this section needs to be redone.
  When looking back, is it really worth it?
  Also, must change indices notation.
  Use $k$ for indexing frequency.
  Use $n$ for indexing time/space.
  I SEE NOW WHY Osgood wants to use the index range of $[0, N-1] \in Z$...
  It's because he wants to combind the sumations over space and frequency without having conflicts in indexes (second step in derivation below). 
  This is not such a simple point.
  The number of sample points/ indices in the time/space domain is $N$.
  ...How many points are in the frequency domain, $N$, $2N$?
  Is there are correspondence between points and or indices in the two domains?
  ************
\fi

  The DFT has two different definitions depending on where you decide to put the factor of ${1/N}$.
  In the spirit of No Pedagogy, this amounts to doing everything twice, which sux but is maybe a price worth paying to satisfy everyone, and to be able to check everything.


  \subsection{Introductory statements (should be a summary of important practical facts/details)}

  AHEAD/SIDE: need to be able to display the ``spectrogram'' for signal analysis.
  What is multitaper? (Spectral Analysis for Physical Applications Percival and Walden)

  Introductory statements:
  can only take a limited number of samples...
  real signals are limited in frequency and time...

  ``A common first step in data or signal analysis is to subtract the mean''(p.9\cite{BriggsHenson}.

  WANT TO NAIL DOWN THE REASONING FOR THIS FACT!
  ``if a signal is sampled once every $\triangle t$ units, then a wave with a period less than $2\triangle t$ cannot be resolved accurately''(p.9\cite{BriggsHenson}).
  $$\omega_{max}=1/(2\triangle t) \quad \text{units}^{-1}$$

  There are multiple routes for deriving the DFT.
  Each reveals practical details of the calculation.
  But first, some preliminaries: conditions on $f$ (the function in theory, the signal in practice), the bandwidth, cylic complex exponentials, the spatio-temporal and frequency grids.

\begin{itemize}
	\item only integer modes of a fundamental aka 'one' mode are accessible.
	\item there is a factor of $1/N$ that distinguishes the DFT from the Fourier transform, and which contributes to alternative definitions for the DFT.
	\item indices for $N$ even on the symmetric grid range from $-N/2+1$ to $N/2$ (but in truth any consecutive range of $N$ indices will work)
	\item end points - Briggs and Henson p.93 sec.3.4 ``Averaging at Endpoints''\cite{BriggsHenson}
	\item the DFT is an approximation for the Fourier coefficients
	\item The relationship between the continuous Fourier transform and the DFT is (p.23\cite{BriggsHenson}) $$\boxed{F(s_k) \approx L F[k]}$$
	\item missing points: take average of adjacent points
\end{itemize}


  \subsection{prelim}

  Need to specify conditions on $f$ somewhere, ie:
  $$\int f (x) \,dx < \infty$$

  Need to state that the underlying assertion of the DFT involves a misapplication of the (classical) sampling theorem, that a signal cannot be both bandlimited and timelimited - {\ul both terms are one-word, no hyphen, no space}.
  % Wikipedia article on bandlimiting - https://en.wikipedia.org/wiki/Bandlimiting

  \subsection{Review: The Bandwidth}

  Different communities use a different definition for the bandwidth (is it because they're using continuous or discrete Fourier transform?).
  The definition I use is based on the following illustration\cite{BriggsHenson}
\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
	\begin{axis} [
		enlarge x limits=false,
		enlarge y limits=false,
  		width=10cm,
		height=5cm,
		axis x line=bottom,
		x axis line style={<->, gray},
		y axis line style={->, white},
  		xmin=-2.5, xmax=2.5,
		xtick={-1,0,1},
		xticklabels={$-B$,$0$,$B$},
		axis y line=middle,
  		ymin=0, ymax=2,
		ytick={0},
		yticklabels={},
		yticklabel style = {font=\small},
		xticklabel style = {font=\small},
		x label style={at={(axis description cs:1,0)},anchor=north west},
		xlabel=$\omega$
 	]

  	\addplot [domain=-3:3, smooth, thick] {-2*x^4 - x^3 + x^2 + x + 1};

  	\draw (1.25,1.6) node{$\sim\!(\mathscr{F}f)(\omega)$};

	%\draw [decorate, decoration={brace,amplitude=1pt}, xshift=0pt, yshift=0pt] (0,0) -- (0,1) node [black, midway, yshift=-0.5cm] {\footnotesize $b$};
	\draw [red, decorate, decoration={brace,amplitude=10pt}, xshift=0pt, yshift=0pt] (-1,0) -- (1,0) node [red, midway, yshift=0.55cm] {\scriptsize bandwidth};

	\end{axis}
\end{tikzpicture}
\end{center}
%\caption{The Fourier transform in the frequency domain (should be symmetric, for real signals).  Note: this image is not to be taken literally, because the Fourier transform is in general complex.}
\end{figure}
  The benefit of this definition is it directly relates the bandwidth to the sample spacing, $\triangle t = 1/\Omega$.

  Sometimes when people refer to the bandwidth they are referring to the depiction below, (although I think this is less standard).
	Maybe $2 B$ is introduced to avoid issues for an even or odd number of samples ($N$) when indices are introduced for the frequencies.

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
	\begin{axis} [
		enlarge x limits=false,
		enlarge y limits=false,
  		width=10cm,
		height=5cm,
		axis x line=bottom,
		x axis line style={<->, gray},
		y axis line style={->, gray},
  		xmin=-2.5, xmax=2.5,
		xtick={-1,0,1},
		xticklabels={$-B$,$0$,$B$},
		axis y line=middle,
  		ymin=0, ymax=2,
		ytick={0},
		yticklabels={},
		yticklabel style = {font=\small},
		xticklabel style = {font=\small},
		x label style={at={(axis description cs:1,0)},anchor=north west},
		xlabel=$\omega$
 	]

  	\addplot [domain=-3:3, smooth, thick] {-2*x^4 - x^3 + x^2 + x + 1};

  	\draw (1.25,1.6) node{$\sim\!(\mathscr{F}f)(\omega)$};

	%\draw [decorate, decoration={brace,amplitude=1pt}, xshift=0pt, yshift=0pt] (0,0) -- (0,1) node [black, midway, yshift=-0.5cm] {\footnotesize $b$};
	\draw [red, decorate, decoration={brace,amplitude=10pt}, xshift=0pt, yshift=0pt] (0,0) -- (1,0) node [red, midway, yshift=0.55cm] {\scriptsize bandwidth};

	\end{axis}
\end{tikzpicture}
\end{center}
%\caption{The Fourier transform in the frequency domain (should be symmetric, for real signals).  Note: this image is not to be taken literally, because the Fourier transform is in general complex.}
\end{figure}
  Essentially, these two definitions for the bandwidth differ by a factor of two, $\Omega = 2B$.
  According to the definition for the bandwidth ($\Omega$) adopted here, this alternative definition for the bandwidth ($B$) is equivalent to the Nyquist frequency given by $\Omega/2$.


  The definition and concept of the ``bandwidth'' needs to be understood before diving into the DFT.
  Its definition gets to the point behind the primary application of the DFT, to reconstruct the signal (from its Fourier series representation).
  It, furthermore, identifies where limitations in the DFT arise.
  Lastly, understanding the bandwidth is intimately tied to the intrinsic differences between the DFT and the Fourier transform (... cyclic v. non-cyclic complex exponentials).
  

  One way to understand the concept of the bandwidth is by what it means for a signal to be band-limited.
  Briggs and Henson are careful to define a {\ul periodic function} as one which 

\begin{itemize}
  \item only need be piece-wise smooth on the interval $0,L$, ($-L/2,L/2$, however you want to set things up).
\end{itemize}

  Then, the signal is band-limited if

\begin{itemize}
  \item The Fourier coefficients are zero for frequencies $$\omega \ge \Omega/2$$.
  \item If $f$ is sampled at least twice per period $$\triangle x \le {1\over \Omega}$$.
\end{itemize}

  And, the DFT can reproduce the signal exactly.



  *** AHEAD: criteria for signals that are ``essentially bandlimited'' (p.97 Ch.3 \cite{BriggsHenson}) 
  

  \subsection{The Cyclic Complex Exponential}
  %(SHOULD THIS BE THE FIRST THING COVERED???...it's fundamental to the differences between the continuous v. discrete Fourier transform, and leads to numerous properties that are essential for working with the DFT.)
  %PUT THE Orthogonality shit here and the def of the cyclic kronecker delta...
  %Observe the summation index


  Many differences between the continuous and discrete Fourier transforms stem from the cyclic complex exponential
  $$e^{-i2\pi s x} \qquad\text{v.}\qquad e^{-i2\pi k n/N}$$
  Briggs and Henson adopt the notation\cite{BriggsHenson} for the cyclic complex exponential 
  $$ \omega_N^{k n} = e^{i2\pi k n / N} $$

  COME BACK: also do $\delta[n-m]$ AND SHOW IMAGES TO SHOW THEY ARE THE SAME show symmetry to emphasize that you must make a choice when defining the Kronecker delta for a particular setting, ie: when defining the $\Sh$ function!!!
  The Kronecker delta 
  $$\delta[n] = 
	\begin{cases}
		1 & \text{for $n=0$} \\
		0 & \text{else} \\
	\end{cases}
  $$
\begin{center}
\begin{tikzpicture}
% horizontal labels
\draw[<->]
   	(-4,0) node[] {}
 --	 (0,0) node[] {$|$}
 --  (0,0) node[anchor=north, yshift=-1.5ex] {$0$}
 --	 (2,0) node[] {}
 --  (4,0) node[anchor=west, xshift=1.5ex] {$[n]$};
\draw (1,1) node{$\delta[n]$};
% Discrete Shaw
\draw[-, very thick] (0,0) -- (0,1);
\filldraw[color=black] (0,1) circle (.1);
\end{tikzpicture}
\end{center}
  plays the role of the Dirac delta in the discrete context.
  Sums over the Kronecker delta behave like integrals over the Dirac delta.
  While the Dirac delta is infinitely high and infinitely narrow at a point along the axis, the Kronecker delta needs only to be equal to one to achieve the desired behavior.
  In that sense it is fair to represent the Kronecker delta as equal to 1 at a point along the axis.
  However, in practice the Kronecker delta serves more like a sampling function and sort of conceptually can be thought of as a distribution - that way you are not tempted to evaluate the sum when defining the Shah, $\Sh$, function as the sum of shifted Kronecker deltas - from this perspective it is still appealling to represent the Kronecker delta as a spike with an arrow.
  (COME BACK: I must accept that this is a pedagogical stance...and I will have to come back and revise this/ make a decisive choice on how the Kronecker delta should be represented and conceptualized...but right now i cannot say for sure what works best...)

\begin{center}
\begin{tikzpicture}
% horizontal labels
\draw[<->]
   	(-4,0) node[] {}
 --	 (0,0) node[] {$|$}
 --  (0,0) node[anchor=north, yshift=-1.5ex] {$0$}
 --	 (2,0) node[] {}
 --  (4,0) node[anchor=west, xshift=1.5ex] {$[n]$};
\draw (1,1) node{$\delta[n]$};
% Discrete Shaw
\draw[->, very thick] (0,0) -- (0,1);
\end{tikzpicture}
\end{center}

  The shifted Kronecker delta 
  $$\delta[m-n] = 
	\begin{cases}
		1 & \text{for $n=m$} \\
		0 & \text{else}
	\end{cases}
  $$
\begin{center}
\begin{tikzpicture}
% horizontal labels
\draw[<->]
   	(-4,0) node[] {}
 --	 (0,0) node[] {$|$}
 --  (0,0) node[anchor=north, yshift=-1.5ex] {$0$}
 --	 (2,0) node[] {$|$}
 --	 (2,0) node[anchor=north, yshift=-1.5ex] {$m$}
 --  (4,0) node[anchor=west, xshift=1.5ex] {$[n]$};
\draw (3.5,1) node{$\delta[m-n]$};
% Discrete Shaw
\draw[->, very thick] (2,0) -- (2,1);
\end{tikzpicture}
\end{center}

  The Kronecker delta is symmetric, $\delta[m-n] = \delta[n-m]$, in either case the function spikes where $n=m$
\begin{center}
\begin{tikzpicture}
% horizontal labels
\draw[<->]
   	(-4,0) node[] {}
 --	 (0,0) node[] {$|$}
 --  (0,0) node[anchor=north, yshift=-1.5ex] {$0$}
 --	 (2,0) node[] {$|$}
 --	 (2,0) node[anchor=north, yshift=-1.5ex] {$m$}
 --  (4,0) node[anchor=west, xshift=1.5ex] {$[n]$};
\draw (3.5,1) node{$\delta[n-m]$};
% Discrete Shaw
\draw[->, very thick] (2,0) -- (2,1);
\end{tikzpicture}
\end{center}
  so that one must make a choice as to how to define the shifted Kronecker delta when implementing it, AHEAD this comes up when defining the discrete Shah function.

  The Kronecker delta defined using the cyclic complex exponentials is where the DFT derives its cyclic property (along with the usual orthogonality property).
  Understanding this cyclic/modular\cite{BriggsHenson} Kronecker delta, ($\delta_N$), is germane to manipulations involving the DFT.

  For the modular Kronecker delta, Briggs and Henson use the notation (p.29\cite{BriggsHenson})
  $$\hat{\delta}_N(j-k)$$
  I use the notation
  $$\delta_N [m-n]$$
  The hat over delta is ostensibly to suggest the discrete context.
  I use square brackets to indicate discreteness when enclosing the arguments of a function.
  *** (NOW YOU CAN UPDATE NOTATION - no more hat!) ***
  

  \ul{\bf Periodicity}

  The cyclic complex exponential is $N$-periodic.
  For $z \in \mathbb{Z}$.
\begin{align*}
  \omega_N^{n+zN} &= e^{i2\pi(n+zN)/N} = e^{i2\pi n/N } e^{i2\pi zN/N } \\
  &= e^{i2\pi n/N } (\cos(2\pi z)+i\sin(2\pi z) = e^{i2\pi n/N} (1) \\
  &=\omega_N^{n}
\end{align*}

  where $N \in \mathbb{Z}>1$ (ie: the number of samples) is the period. 


  \ul{\bf Modular Geometric Series}
\begin{align*}
\intertext{The geometric series formed by summing the terms of the geometric progression/sequence ($|a|<1$)...Recall:}
  S_0 &= \sum_{n=0}^{N-1} a^n = 1 + a + a^2 + \cdots + a^{N-1} \\
  S_1 &= a S_0 = \sum_{n=1}^N a^n = a + a^2 + \cdots + a^N \\
  S_0 - S_1 &= S_0 (1 - a) = 1 - a^N
\end{align*}
  $$ \boxed{\sum_{n=0}^{N-1} a^n = {1 - a^N \over 1 - a}} $$

  More generally, truncating the sum at the beginning
\begin{align*}
  S &= \sum_{n=M}^{N-1}a^n = a^M + a^{M+1} + \cdots + a^{N-1} \\
  S^\prime = aS &= \sum_{n=M}^{N-1}a^{n+1} = a^{M+1} + a^{M+2} + \cdots + a^{N} \\
  S - S^\prime = S - aS &= S(1-a) = a^M-a^N
\end{align*}
  $$ \boxed{\sum_{n=M}^{N-1} a^n = {a^M - a^N \over 1-a}} $$

  For $a = \omega_N$, ($a$) is $N$-periodic
  $$a[p\pm N] = w_N^{p\pm N} = w_N^p w_N^{\pm N} = w_N^p (1) = a[p]$$
  Any list of $N$ consecutive integers, ie: $p$ to $p+N-1$, will result in the same geometric series
\begin{align*}
  S_0 &= \sum_{n=p}^{p+N-1} a^n = a^p + a^{p+1} + \cdots + a^{p+N-1} \\
  S_1 = aS_0 &= \sum_{n=p}^{p+N-1} a^{n+1} = a^{p+1} + a^{p+2} + \cdots + a^{p+N} \\
  S_0 - S_1 &= S_0 (1 - a) = a^p - a^{p+N} \\
  S_0 &= {a^p - a^{p+N} \over 1 - a}
\end{align*}
  $$ \boxed{ \sum_{n=p}^{p+N-1} a^n = {a^p - a^{p+N}\over 1 - a} \quad\text{(not general, specific to {\em cyclic} ($a$))} } $$


  \ul{\bf Orthogonality}
  % Problems 11 and 12 in The DFT Book\cite{BriggsHenson}
  The cyclic/modular Kronecker delta $\hat{\delta}_N(m-n)$ (where the subscript $N$ specifies the periodicity) 
  ...the sum of the product of cyclic complex exponentials $w_N^{k n}$ and $w_N^{-\ell n}$
  Now, define the cyclic Kronecker delta $\delta_N(m-n)$ 
and derive the orthogonality of cylcic complex exponentials
\begin{align*}
  \sum_{k=0}^{N-1} e^{i2\pi k m/N} e^{-i2\pi k n/N}
  &= \sum_{k=0}^{N-1} e^{i2\pi k(m-n)/N} = N \hat\delta_N(m-n) \\
  \Aboxed{ 
	\sum_{k=p}^{p+N-1} \omega_N^{k(m-n)} 
	= {\omega_N^p - \omega_N^{p+N}\over 1 - \omega_N} 
	\equiv 
	N\,\hat\delta_N(m-n) 
	} % NOTE: cases cannot go within Aboxed-environment
	= \begin{cases}
	  N & \text{if $m=n$} \\
	  0 & \text{if $m\ne n$ because $\omega_N^p=\omega_N^{p+N}$ }
    \end{cases}
\intertext{, which can be used to derive the DFT, what else?...}
\end{align*}

  I will adopt a slightly different notation.
  Rather than use the hat over the delta function to denote discreteness, I will use the square brackets instead of parentheses.
  The substript is useful for denoting the cyclic/modular behavior so I keep it.
  $$\boxed{ 
	\sum_{k=p}^{p+N-1} \omega_N^{k(m-n)} 
	= {\omega_N^p - \omega_N^{p+N}\over 1 - \omega_N} 
	\equiv 
	N\,\delta_N[m-n] 
	= \begin{cases}
	  N & \text{if $m=n$} \\
	  0 & \text{if $m\ne n$ because $\omega_N^p=\omega_N^{p+N}$ }
    \end{cases}
	}
  $$

  \ul{\bf The Dirichlet Kernel}

  ... establishing the Dirichlet kernel (Percival + Walden prob.1.2.c)
\begin{align*}
  \sum_{t=0}^{N-1}e^{i2\pi s t} &= \sum_{t=0}^{N-1} (e^{i2\pi s})^t
  = {1 - e^{i2\pi s N} \over 1-e^{i2\pi s}}\\
  &= { e^{i\pi s N} ( e^{-i\pi s N} - e^{i\pi s N} ) \over e^{i\pi s}(e^{-i\pi s}-e^{i\pi s}) }\\
  &= { e^{i\pi s N} e^{-i\pi s} ( e^{-i\pi s N} - e^{i\pi s N} ) (-1/i2) \over (e^{-i\pi s}-e^{i\pi s}) (-1/i2)}\\
  &= e^{i\pi s (N-1)} {\sin(N \pi s)\over\sin(\pi s)}\\
\end{align*}
  
  The ratio of $\sin()$'s is contininuous at the limit $s \rightarrow z \in \mathbb{Z}$, where $s$ takes on integer values.
  Applying L'Hospital's rule on the form ${0\over0}$
\begin{align*}
  \lim_{s \rightarrow z\in\mathbb{Z}} {\sin(N\pi s) \over \sin(\pi s)} &= {{\,d \over \,d s} \sin(N\pi s) \over {\,d \over \,d s} \sin(\pi s)}\\
  &= {\cos(N\pi (z\in\mathbb{Z}))(N\pi) \over \cos(\pi (z\in\mathbb{Z})) (\pi)}\\
  &=  N {(-1)^{Nz} \over (-1)^z} = N {(-1)^{(N-1)z}}\\
\end{align*}
  gives a modified version of the {\bf Dirichlet kernel} (partly due to the range, from $0$ to $N$, ie: an asymmetric grid),
  $$ D_N(s) =
		\begin{cases}
	   		{\sin(N\pi s)\over \sin(\pi s)} & \text{for $s \notin \mathbb{Z}$} \\
	    	N (-1)^{(N-1) s} & \text{for $s \in \mathbb{Z}$}
     	\end{cases}
  $$

 Percival and Walden (prob.1.2.c, p.17 \cite{PercivalWalden2020}) play a weird ``song and dance'' with a factor of $N$ that allows them to write the Dirichlet kernel as
  $$ D_N(f) =
		\begin{cases}
	   		{\sin(N\pi f)\over N\sin(\pi f)} & \text{for $f \notin \mathbb{Z}$} \\
	    	(-1)^{(N-1) f} & \text{for $f \in \mathbb{Z}$}
     	\end{cases}
  $$
  , but which requires them to multiply by a factor of $N$ in the calculation of the geometric progression - defines the DFT - as follows
  $$\sum_{t=0}^{N-1} e^{i2\pi f t} = N e^{i(N-1)\pi f} D_N(f)$$
  I'll stick with my notation / derivation, it's simpler.

  Brad Osgood derivation using a geometric progression on the symmetric grid from $-N$ to $N$ looks more like the standard definition of the Dirichlet kernel (ie: p.189, Rudin, "Principles of Mathematical Analysis, Ed.3") but not exactly
\begin{align*}
  \sum_{k=-N}^{N}e^{i2\pi k t} &= {e^{-i2\pi N t} - e^{i2\pi (N+1) t} \over 1 - e^{i2\pi t} } \\
  &= { e^{i\pi t} \biggl( \exp\Bigl(-i2\pi N t - i \bigl({2 \over 2}\bigr) \pi t \Bigr) - \exp\Bigl(i2\pi (N+1) t - i \bigl({2 \over 2}\bigr) \pi t \Bigr) \biggr) \over e^{i\pi t} \bigl( e^{-i \pi t} - e^{i\pi t} \bigr) } \\
  &= { \exp\Bigl(-i2\pi \bigl(N + {1\over2}\bigr) t \Bigr) - \exp\Bigl(i2\pi \bigl(N + 1 - {1\over2}\bigr) t\Bigr) \over \bigl( e^{-i \pi t} - e^{i\pi t} \bigr) } \\
  &= { \exp\Bigl(-i2\pi \bigl(N + {1\over2}\bigr) t \Bigr) - \exp\Bigl(i2\pi \bigl(N + {1\over2}\bigr) t\Bigr) \over \bigl( e^{-i \pi t} - e^{i\pi t} \bigr) } \\
  &= { (-1) \biggl(\, \exp\Bigl(i2\pi \bigl(N + {1\over2}\bigr) t \Bigr) - \exp\Bigl(-i2\pi \bigl(N + {1\over2}\bigr) t\Bigr) \,\biggr) \Big/ (i2) \over (-1) \bigl( e^{i \pi t} - e^{-i\pi t} \bigr) / (i2) }\\
  &= { \sin\Bigl(2\pi \bigl(N + {1\over 2}\bigr) t\Bigr) \over \sin(\pi t) } \\
\end{align*}


  \subsection{The Reciprocity Relations}

  The two variables that you can control when using the DFT are $N$ and $L$...

  %INTRO (some statements to set things up, ie: one can only obtain a finite number of samples in practice):
  %NOTE: Need to rework subsequent notation but these are the ``reciprocity relations'' clearly stated.
  Taking the view that the ``reciprocity relations are the keystone of the DFT that holds its entire structure in place.''(p.20, \cite{BriggsHenson}, they are stated without hesitation and an effort is made to not overlook fine details.
  These fundamental relationships are derived from the temporal (or spatial) and frequency grids.
  Briggs and Henson take spatial (temporal) domain $[-A/2, A/2]$ with grid spacing $\triangle x = L/N$ and grid points at $x_n = n\triangle x$; and the frequency domain also consists of $N$ grid points with equal spacing $\triangle \omega$, denoted $\omega_k = k \triangle \omega$, where the indices range from $k=-N/2+1:N/2$.
  They shirk derivation by ``assuming that both the spatial and frequency grids consist of $N$ points''(p.21, \cite{BriggsHenson}).
  They simply take for granted the congruence between indices, and let $n = -N/2+1 : N/2$
  Here, the opportunity is taken to clarify any assumptions that may introduce ambiguity in details or in practical considerations of the respective grids.

  In practice, it is only possible to obtain a finite number of samples ($N$).
  From the persepctive of sampling in time it is natural to think of the first sample as occuring at time zero, $t=0$, and continuing for some length of time $L$.
  For temporal data it is also natural to index $N$ samples from $0$ to $N-1$.
  This can introduce some ambiguity when constructing the frequency grid with the same set of indices.
  The way to understand both, the motivation for indexing samples from $n = [0:N-1]$, and how to adjust the frequency grid to use the same set of indices for $k = [0:N-1]$, is by the $N$-periodicity of the DFT.
  Over the finite duration ($L$), which can be subdivided into $N$ equally spaced intervals, the grid spacing (aka ``sampling rate''(p.7 \cite{BriggsHenson})) is given by $\triangle t = L/N$.
  %(Ahead) Given a finite sampling it is natural to artificially periodize the signal when trying to obtain its Fourier transform.
  %In digital signal processing samples are obtained from discrete processes, which take place over a finite duration.

\begin{figure}[H]
\centering
\begin{tikzpicture}
% Depiction of the sampling axis
\draw[->, thick, dashed]
	(-5,1) node[anchor=north, yshift=-1ex, xshift=2ex] {$t_0=0$} 
 -- (-2,1) node[anchor=north, yshift=-1ex] {$t_1$}
 -- (1,1)  node[anchor=north, yshift=-1ex] {$t_2$}
 -- (4,1)  node[anchor=north, yshift=-1ex, xshift=2.5ex] {$t_3 = L$}
 -- (5,1)  node[] {};
\node [anchor=west, align=left] (note) at (6,1) {{\small(discrete time, $t_n = n \,\triangle t$)}};

% Denote steps where sampling takes place
\filldraw[black] (-5,1) circle (.1);
\filldraw[black] (-2,1) circle (.1);
\filldraw[black] (1,1)  circle (.1);
\filldraw[color=black, fill=white] (4,1) circle (.1);

% Denote time/space interval
\draw[|-, thick] (-5,1.5) -- (-4,1.5) node[anchor=west] {${\triangle t_1}$}; 
\draw[-, thick] (-3,1.5) -- (-2,1.5); 
\draw[|-, thick] (-2,1.5) -- (-1,1.5) node[anchor=west] {${\triangle t_2}$}; 
\draw[-, thick] (0,1.5) -- (1,1.5); 
\draw[|-, thick] (1,1.5) -- (2,1.5) node[anchor=west] {${\triangle t_3}$}; 
\draw[-|, thick] (3,1.5) -- (4,1.5); 
\node [anchor=west, align=left] (note) at (6,1.5) {{\small(sampling increments)}};

% Annotations indices
\node [xshift=2ex] (note) at (-5,2.5) {$n=0$};
\node [xshift=2ex] (note) at (-2,2.5) {$\ldots$};
\node [xshift=4ex] (note) at (1,2.5) {$n=N\!-\!1$};
\node [xshift=2ex] (note) at (4,2.5) {$n=N$};
\node [anchor=west, align=left] (note) at (6,2.5) {{\small(sample index, $n = [0 : N]$, ie: $N\!=\!3$)}};

% Annotations grid
\node [anchor=south, xshift=2ex] (note) at (-5,3) {sn $=1$};
\node [anchor=south, xshift=2ex] (note) at (-2,3) {sn $=2$};
\node [anchor=south, xshift=2ex] (note) at (1,3) {sn $=3$};
\node [anchor=south, xshift=3ex] (note) at (4,3) {sn $=N\!+\!1$};
\node [anchor=south west, align=left] (note) at (6,3) {{\small(sample number (sn))}};

\end{tikzpicture}
\caption{
  Depiction of a full sampling cycle, from $0$ to $L$, for $N\!=\!3$ samples, and the onset of the next cycle.
  \ul{\it Notice the full extent of the sampling period is not accounted for until the cycle repeats}.
  The $N\!+\!1^{\text{th}}$-sample corresponding to the $N^{\text{th}}$ sampling index ($n\!=\!N$), the first sample of the next sampling sequence (hollow circle), bridges the gap between the sampling extent at the last sample in a cycle and the complete sampling length.
  The subtle point I am trying to point out is that, while the total number of sample points and the number of sampling intervals is the same, ($N$), it is not enough to look at the elapsed time at the last sample in the cycle to determine the length of the sampling duration, rather, $L = N \triangle t$.
}
\end{figure}

  It is natural to think of frequencies as being essentially zero or higher.
  The DFT only permits modes with a complete number of cycles over the sampling length to be used to reconstruct the signal.
  (This is in stark contrast to the continuous case, where the possible modes are not limited by any boundary and frequencies can take on any value).
  All available modes are multiples of the fundamental mode, which consists of one period over the full sampling length, depicted below.

\begin{figure}[H]
\centering
\begin{tikzpicture}
% Depiction of the sampling axis
\draw[->, thick]
	(-5,0) node[anchor=north, yshift=-1ex] {$0$} 
	-- (-5,0) node[] {$|$} 
	-- (-2,0) node[] {}
	-- (1,0) node[] {}
	-- (4,0) node[] {$|$}
	-- (4,0) node[anchor=north, yshift=-1ex] {$L$}
	-- (5,0) node[] {};
\node [anchor=west, align=left] (note) at (6,0) {(sampling axis)};

% note the sin and cos commands draw only a quarter sine/cos curve and the y coordinate of two points should be different.
\draw[ultra thick, red] (-5,0) sin (-2.75,1);
\draw[ultra thick, red] (-2.75,1) cos (-0.5,0);
\draw[ultra thick, red] (-0.5,0) sin (1.75,-1);
\draw[ultra thick, red] (1.75,-1) cos (4,0);

\filldraw[black] (-5,0)  circle (.1);
\filldraw[color=black, fill=white] (4,0) circle (.1);

\end{tikzpicture}
\caption{
  The fundamental mode, aka ``one-mode'' (red), which determines the grid spacing in the frequency domain $\triangle s = 1/L$.
  The dots indicate where this frequency is sampled during the present (black) and ensuing (hollow) sampling cycle.
}
\end{figure}

  Therefore, the grid spacing in the frequency domain is given by the fundamental frequency as, $\triangle s = 1/L$.
  BUT where does $N$ come from in determining the number of frequency-samples, and the number of frequency intervals that comprise the full extent of the frequency domain? 
  ??? must somehow get to $\Omega = N \!\triangle s$ ???
  How to motivate indices starting from 0???
  There is clearly some desire to have matching indices for points along the respective grids, but does this occur naturally???
  Is there a correspondence between indices used in the respective grids???
  (...)
  Given the bandwidth is equal to $\Omega = N\!\triangle s = N/L$, the grid spacing in the time domain is related to the bandwidth by $\triangle t = 1/\Omega$.

\begin{figure}[H]
\centering
\begin{tikzpicture}
% Depiction of the temporal (spatial) domain
\draw[-, thick]
	(-5,3) node[anchor=north, yshift=-1ex] {$0$}
	(-5,3) node[anchor=east, xshift=-3ex] {(temporal grid)}
	-- (-5,3) node[] {$|$}
	-- (-2,3) node[] {$|$}
	-- (1,3) node[] {$|$}
	-- (4,3) node[] {$|$}
	-- (4,3) node[anchor=north, yshift=-1.5ex, xshift=4ex] {$L = N\, \triangle t $};

% Denote frequency interval
\draw[|-, thick] (-5,3.75) -- (-4.55,3.75) node[anchor=south west] {${\triangle t = L/N}$}; 
\draw[-|, thick] (-2.4,3.75) node[anchor=north east, yshift=1ex] {${ =\, 1/\Omega}$}
	-- (-2,3.75); 

% Annotate 
\node [align=left] (note) at (4,4.5) {$t_n = n\, \triangle t = n/\Omega \qquad\qquad n = 0,1,2,\ldots,N\!-\!1$};

% Depiction of the frequency domain
\draw[-, thick]
	(-5,0) node[anchor=north, yshift=-1ex] {$0$}
	(-5,0) node[anchor=east, xshift=-3ex] {(frequency grid)}
	-- (-5,0) node[] {$|$}
	-- (-2,0) node[] {$|$}
	-- (1,0) node[] {$|$}
	-- (4,0) node[] {$|$}
	-- (4,0) node[anchor=north, yshift=-1.5ex, xshift=8ex] {$\Omega = N\, \triangle s = N/L$};

% Denote frequency interval
\draw[|-, thick] (-5,0.5) -- (-4.55,0.5) node[anchor=west] {${\triangle s = 1/L}$}; 
\draw[-|, thick] (-2.4,0.5) -- (-2,0.5); 

% Annotate 
\node [align=left] (note) at (4,1) {$s_k = k\, \triangle s = k/L \qquad\qquad k = 0,1,2,\ldots,N\!-\!1$};

\end{tikzpicture}
\caption{
  %An intuitive depiction of the temporal (/spatial) and frequency domains.
  %There is a fundamental correspondence between the temporal domain and the frequency domain via the number of samples, $N$.
  %The number of sample points is the same as the number of frequency intervals. 
}
\end{figure}

  (
  Does the frequency $s_0=0$ correspond to the average value (the DC component, check out Denis Freeman MIT OCW Signals and Systems Lect.14)?
  How is $s_0$ handled by subtracting the average value at the outset?
  Think a little bit more about what is going on with a variable/quantity like this $s_{N-1}={N-1\over L}$.
  )
  Based on the temporal (spatial) and frequency grids the \underline{\bf reciprocity relations} are

\begin{subequations}
\begin{empheq}[box=\widefbox]{align}
  \Omega \, L &= N \\
  \triangle s \, \triangle t &= {1 \over N}
\end{empheq}
\end{subequations}

  Both time and frequency grids intuitively start from zero, as negative values of these quantities are merely mathematical constructs without actual physical meaning.
  %The important take away from this is that $N$ number of samples leads to the same $N$ number of frequency intervals that make up the full extent of the frequency domain.
  It is trivial to shift the temporal axis to include negative values, ie: for the sake of symmetry.
  Importantly, and slightly less trivial is how the intuitive conceptual representation for the frequency domain must adjust to accommodate the complex exponential representation of the normal modes.
  Recall, corresponding positive and negative indices are needed to reconstruct the $|k|^{\text{th}}$ mode.
  To preserve the bandwidth associated with the old grid when transitioning to the new grid involves a shift along the frequency axis, followed by accompanying indices, depicted below.

\begin{figure}[H]
\centering
\begin{tikzpicture}
% Titles
\node[anchor=south] (note) at (0, 6) {``Shift to symmetric grid''};
\node[anchor=south east] (note) at (-2, 5) {($N=4$)};
\node[anchor=south west] (note) at (3, 5) {($N=5$)};

% Big arrow
\draw[
	gray,
    -triangle 90,
    %line width=4mm,
    line width=2mm,
    %postaction={draw, line width=1cm, shorten >=1cm, -}
    postaction={draw, line width=0.5cm, shorten >=0.5cm, -}
] (0.1,3) -- (0.1,1.6);

% Top left axis 
\draw[-, thick]
	(-5,4) node[] {$|$}
 -- (-5,4) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (-5,4) node[anchor=south, yshift=1.5ex] {$0$}
 -- (-4,4) node[] {$|$}
 -- (-4,4) node[anchor=south, yshift=1.5ex] {$1$}
 -- (-3,4) node[] {$|$}
 -- (-3,4) node[anchor=south, yshift=1.5ex] {$2$}
 -- (-2,4) node[] {$|$}
 -- (-2,4) node[anchor=south, yshift=1.5ex] {$3$}
 -- (-1,4) node[] {$|$}
 -- (-1,4) node[anchor=south, yshift=1.5ex] {$4$}
 -- (-1,4) node[anchor=north, yshift=-1.5ex] {$\Omega$};
\filldraw[color=black] (-5,4) circle (.1);
\filldraw[color=black] (-4,4) circle (.1);
\filldraw[color=black] (-3,4) circle (.1);
\filldraw[color=black] (-2,4) circle (.1);
% Bottom left axis
\draw[-, thick]
	(-5,0) node[] {$|$}
 -- (-5,0) node[anchor=north, yshift=-1.5ex, xshift=-0.75ex] {$-\dfrac{\Omega}{2}$}
 -- (-4,0) node[] {$|$}
 -- (-4,0) node[anchor=south, yshift=1.5ex] {$-\dfrac{N}{2}+1$}
 -- (-3,0) node[] {$|$}
 -- (-3,0) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (-2,0) node[] {$|$}
 -- (-1,0) node[] {$|$}
 -- (-1,0) node[anchor=south, yshift=1.5ex] {$\dfrac{N}{2}$}
 -- (-1,0) node[anchor=north, yshift=-1.5ex] {$\dfrac{\Omega}{2}$};
\filldraw[color=black] (-4,0) circle (.1);
\filldraw[color=black] (-3,0) circle (.1);
\filldraw[color=black] (-2,0) circle (.1);
\filldraw[color=black] (-1,0) circle (.1);

% Top right axis
\draw[-, thick]
	(1,4) node[] {$|$}
 -- (1,4) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (1,4) node[anchor=south, yshift=1.5ex] {$0$}
 -- (2,4) node[] {$|$}
 -- (2,4) node[anchor=south, yshift=1.5ex] {$1$}
 -- (3,4) node[] {$|$}
 -- (3,4) node[anchor=south, yshift=1.5ex] {$2$}
 -- (4,4) node[] {$|$}
 -- (4,4) node[anchor=south, yshift=1.5ex] {$3$}
 -- (5,4) node[] {$|$}
 -- (5,4) node[anchor=south, yshift=1.5ex] {$4$}
 -- (6,4) node[] {$|$}
 -- (6,4) node[anchor=south, yshift=1.5ex] {$5$}
 -- (6,4) node[anchor=south west, yshift=0.75ex, xshift=3ex] {(index, k)}
 -- (6,4) node[anchor=north west, yshift=-1.25ex, xshift=3ex] {(frequency)}
 -- (6,4) node[anchor=north, yshift=-1.5ex] {$\Omega$};
\filldraw[color=black] (1,4) circle (.1);
\filldraw[color=black] (2,4) circle (.1);
\filldraw[color=black] (3,4) circle (.1);
\filldraw[color=black] (4,4) circle (.1);
\filldraw[color=black] (5,4) circle (.1);
% Bottom right axis
\draw[-, thick]
	(1,0) node[] {$|$}
 -- (2,0) node[] {$|$}
 -- (2,0) node[anchor=south, yshift=1.5ex] {$\dfrac{1-N}{2}$}
 -- (3,0) node[] {$|$}
 -- (4,0) node[] {$|$}
 -- (4,0) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (5,0) node[] {$|$}
 -- (6,0) node[] {$|$}
 -- (6,0) node[anchor=south, yshift=1.5ex] {$\dfrac{N-1}{2}$}
 -- (7,0) node[] {$|$}
 -- (7,0) node[anchor=south west, yshift=0.75ex, xshift=3ex] {(index, k)}
 -- (7,0) node[anchor=north west, yshift=-1.25ex, xshift=3ex] {(frequency)};
\filldraw[color=black] (2,0) circle (.1);
\filldraw[color=black] (3,0) circle (.1);
\filldraw[color=black] (4,0) circle (.1);
\filldraw[color=black] (5,0) circle (.1);
\filldraw[color=black] (6,0) circle (.1);
\draw [dashed, anchor=north] (1.5,0.25) -- (1.5,-0.25) node[anchor=north, xshift=-0.8ex] {$-\dfrac{\Omega}{2}$};
\draw [dashed, anchor=north] (6.5,0.25) -- (6.5,-0.25) node[anchor=north] {$\dfrac{\Omega}{2}$};

\end{tikzpicture}
\caption{Depiction (for $N=4$ (left), and $N=5$ (right) sample points) of the shift to the symmetric frequency grid imposed by the complex exponential representation of the normal modes.  (These indices match those presented by Briggs and Henson (p.28 \cite{BriggsHenson}))}
\end{figure}

  In practice, it is common to use indices with the same set of values for both the temporal and frequency grids.
  This is primarily for convenience, there is no inherent correspondence between locations on the time/position grid and locations on the frequency grid with the same index value.
  Rather, the relationship is linear; just as all data points along the discrete axis $\{t_n\}$ are needed to determine a single component $(s_k)$ in the frequency spectrum $\{s_k\}$, the entire spectrum is needed to reconstruct each data point, $f(t_n)$, of the signal.
  To restore the frequency domain so it matches the temporal domain uses the $N$-periodicity.
  The aliasing of negative indices to higher frequency positive indices is depicted below.
  Note, this shows that indices beyond $N/2$ up until $N$ are actually the counter part to indices from $0$ to $N/2$ (some vagueness due to $N$ even v. odd).

\vspace{1cm}
\begin{figure}[H]
\centering
\begin{tikzpicture}
% Frequency axis
\draw[<->, thick] (-4,0)
	  -- (-3,0) node[anchor=north, yshift=-1.5ex] {$-\Omega/2$} 
	  -- (-3,0) node[] {$|$}
	  -- (-2,0) node[anchor=south, yshift=2ex, xshift=6.5ex] {$k=-N/2+1$} 
	  -- (-2,0) node[] {$|$}
	  -- (-1,0) node[] {$|$}
	  -- (0,0) node[] {$|$}
	  -- (0,0) node[anchor=north, yshift=-1.5ex] {$0$}
	  -- (1,0) node[] {$|$}
	  -- (2,0) node[] {$|$}
	  -- (3,0) node[] {$|$}
	  -- (3,0) node[anchor=north, yshift=-1.5ex] {$\Omega/2$}
	  -- (4,0) node[] {$|$}
	  -- (4,0) node[anchor=south, yshift=2ex, xshift=5.5ex] {$k=N/2+1$}
	  -- (5,0) node[] {};

% Annotations 
\filldraw[color=black] (-2,0) circle (.1);
\filldraw[color=black, fill=white] (4,0) circle (.1);

% Annotations arrow from old index to new/shifted index
\draw[->, thick] (-2,1.3) to[out=45, in=135] (3.9,1.3);

% Annotations indices
\node [anchor=west, align=left] (note) at (7,0.7) {(index, $k$)};
\node [anchor=west, align=left] (note) at (7,0) {(frequency)};

\end{tikzpicture}
\caption{Reconstructing the asymmetric frequency domain from the symmetric frequency grid via aliasing based on the $N$-periodicity of the DFT.}
\end{figure}

  Observe, for $N$ samples over a spatial/temportal extent $L$ the DFT can at most resolve a maximum frequency of
  $$s_{max} = {\Omega\over2}$$
  , which corresponds to the index $k=N/2$ whether the frequency grid is constructed as symmetric or assymmetric.
  This shows that to resolve a mode with $k$ number of wavelengths/periods over $L$ at least $2 k$ samples over $L$ are required.

\begin{figure}[H]
\centering
\begin{tikzpicture}
% Depiction of the sampling axis
\draw[->, thick]
	(-5,0) node[anchor=north, yshift=-1ex] {$0$} 
	-- (-5,0) node[] {$|$} 
	-- (-2,0) node[] {}
	-- (1,0) node[] {}
	-- (4,0) node[] {$|$}
	-- (4,0) node[anchor=north, yshift=-1ex] {$L$}
	-- (5,0) node[] {};
\node [anchor=west, align=left] (note) at (6,0) {(sampling axis)};

% note the sin and cos commands draw only a quarter sine/cos curve and the y coordinate of two points should be different.
\draw[ultra thick, black] (-5,0) sin (-2.75,1);
\draw[ultra thick, black] (-2.75,1) cos (-0.5,0);
\draw[ultra thick, black] (-0.5,0) sin (1.75,-1);
\draw[ultra thick, black] (1.75,-1) cos (4,0);

\filldraw[black] (-5,0)  circle (.1);
\filldraw[black] (-0.5,0)  circle (.1);
\filldraw[color=black, fill=white] (4,0) circle (.1);

\end{tikzpicture}
\caption{
  Illustration, using $k=1$, of the condition that two samples are required to resolve a given mode.
}
\end{figure}


  To kick a dead horse, consider the case of $N=1$.
  For the DFT defined by $L\Omega=N$ the maximum frequency attainable is $s_{max}=\Omega/2$.
  The corresponding index is $k_{max}=N/2$; given $N=1$, then $k_{max}=1/2$.
  But $k$ is only allowed to take on integer values.

  MOVE THIS!!!
  You an chose the sampling rate $\triangle t$ and the number of samples $N$, then the resolution in frequency $\triangle s$ is fixed (and {\em vice versa}).
  (zero-padding: technique for dealing with this issue)

  Note: $L$ and $\Omega$ are determined by available memory and the sampling aparatus, respectively...

  \subsection{Derivation of the DFT via Misapplication of the Sampling Theorem}

  Assume the signal is both bandlimited and time-limited (although based on the sampling theorem both cannot be true simultaneously)
  It is perhaps worth noting at this point that the Fourier transform is derived by taking the limits of integration from a single period to infinity but in practice there are limits on the boundary of integration for a specific application.
  To go from the continuous and analog to the discrete and digital requires

\begin{enumerate}
  \item A discrete approximation to the signal $f(t)$.
  \item A discrete approximation to the Fourier transform $(\mathscr{F}\!f)(s)$.
  \item A way to go recover the discrete signal from the DFT that mimics the continuous case.
\end{enumerate}

  To define the sampled/discrete form use the Shaw function.
  The sampled form of $f(t)$ for $N$ samples is

\begin{align*}
  f(t)_{sampled} &= f(t)\sum_{n\,=\,0}^{N-1} \delta(t - t_n) \\
  &= \sum_n f(t) \, \delta(t - t_n)
  = \sum_n f(t_n) \, \delta(t - t_n) \\
  &= \sum_n f_n \, \delta(t - t_n)
\end{align*}

  This is a reasonable approximation to $f(t)$ in that (based on the sampling theorem) interpolation at the recorded values should reconstruct the signal exactly.
  Note the choice to keep the Shaw function in the expression, it is essential to the description of the sampled function, as in the function in its sampled form.
  To get rid of the Shaw function it needs to be integrated out.

  Taking the Fourier transform of the sampled form of $f(t)$
\begin{align*}
  (\mathscr{F}\!f_{sampled})(s) &= \int e^{-i2\pi s t} \, \sum_n f_n \,\delta(t - t_n) \,d t \\
  \intertext{Given the Fourier transform is a linear operator}
  &= \sum_n f_n \int e^{-i2\pi s t} \,\delta(t - t_n) \,d t \\
  %The sampled form of $f(t)$ is still a function of a continuous variable so its F.T. is
  &= \sum_n f(t_n) \, e^{-i2\pi st_n}
\end{align*}
  , which is still a function of a continuous variable, $s$.

  To get an approximate discrete version of $\mathscr{F}\!f_{sampled}$ take $N$ samples spaced $1/L$ apart over an interval of length $\Omega$ in the frequency domain,
  $s_0=0, s_1={1\over L}, s_2={2\over L},\ldots, s_{N-1}={N-1\over L}$.
  The sampled form of the Fourier transform is the sampled form of the Fourier transform (in frequency) of the sampled form of $f(t)$ (in time) 

\begin{align*}
  (\mathscr{F}\!f_{sampled})(s)_{sampled}
  &= (\mathscr{F}\!f_{sampled}) \Biggl(\sum_{k=0}^{N-1} \delta(s-s_k) \Biggr) \\
  &= \Biggl( \sum_{n=0}^{N-1} f_n \, e^{-2\pi ist_n} \Biggr) \Biggl(\sum_{k=0}^{N-1} \delta(s-s_k) \Biggr) \\
  &= \sum_k \sum_n f_n \, e^{-2\pi i s t_n} \,\delta(s-s_k) \\
  &= \sum_k \sum_n f_n \, e^{-2\pi i s_k t_n} \,\delta(s-s_k)
\end{align*}

  The explicit reference to time and frequency maintains the connection to the continuous picture.
  The last step to define the DFT is to eliminate any reference to the continuous variables, $t$ and $s$.
  The sample points in the time domain are $$t_n = {n\over \Omega}$$
  The sample points in the frequency domain are $$s_k = {k\over L}$$
  The product of $s_k$ and $t_n$ is
  $$s_k t_n = {k n \over \Omega L} = {k n \over N} $$
  because $\Omega\, L = N$, leaving only discrete indices, $k$ and $n$.

\iffalse
  At this point it is fine to equate the function at some time to the function with the same grid index, $f(t_n)=f[n]$, and similarly, $F(s_k)=F[k]$ (Osgood Lecture 19, this feels a little forced because the straight forward variable substitution as avoided, but the constant factors are really only secondary to the index).
  $$\boxed{F[k] = \sum_{n=0}^{N-1} f[n] e^{-i2\pi k n/N} \quad\text{where}\quad k \in [0, N-1]}$$
  This is the form given by Osgood\cite{Osgood}.
  Now all references to the continuous variables are gone, and what remains is a transformation from a discrete signal to another discrete signal.
  But, this definition is not the same as the DFT in Briggs and Henson (p.24,52) which has a factor of $1/N$, as does the DFT definition by Alan V. Oppenheim (MIT OCW, Lect. 10 - Discrete-Time Fourier Series, 14:53 min) although simply stated without proof.
  Similarly, Briggs and Henson's definition for the IDFT lacks any factor of $N$ (p.28)\cite{BriggsHenson}.
\fi

  Take the inverse Fourier transform to recover the discrete signal

\begin{align*}
  \mathscr{F}^{^{-1}}((\mathscr{F}\!f_{sampled})(s)_{sampled})
  &= \int e^{i2\pi s t}  \sum_{k=0}^{N-1} \sum_{n=0}^{N-1} f_n e^{-i2\pi s t_n} \, \delta(s-s_k) \,ds \\
  &= \sum_k \sum_n f_n e^{-i2\pi s_k t_n} \int e^{i2\pi s t} \, \delta(s-s_k) \,ds \\
  &= \sum_k \sum_n e^{-i2\pi s_k t_n} \, e^{i2\pi s_k t}
  = \sum_k \sum_n f_n \, e^{i2\pi s_k (t - t_n)} \\
  &= \sum_k \sum_n f_n \, \exp\Biggl(i2\pi {k\over L} \biggl({mL\over N} - {nL\over N}\biggr)\Biggr) \\
  &= \sum_k \sum_n f_n \, e^{i2\pi k(m-n)/N}
  = \sum_n f_n \underbrace{\sum_k e^{i2\pi k(m-n)/N}}_{\text{$N \hat\delta_N(m-n)$}} \\
  &= N \sum_n f_n \, \hat\delta_N(m-n) = f_m \quad\text{($m$ equals the index $n$ at the data points)}
\end{align*}

  The question remains, how should the factor of $N$ be distributed in defining the DFT and IDFT?
  Both options are in use.
  The form used by Briggs \& Henson, Bracewell, Weaver, etc.\cite{BriggsHenson,Bracewell,Weaver} is
\begin{subequations}
\begin{empheq}[box=\widefbox]{align}
  \mathcal{F} f = (\mathcal{F}(f(\{x\}_n))(s_k) = F[k] = {1\over N} \, \sum_{n=0}^{N-1} f_n \, e^{-i2\pi k n / N} \qquad k = 0,1,2,\ldots,N\!-\!1 \\
  \mathcal{F}^{^{-1}}\!F = (\mathcal{F}^{^{-1}}(F(\{s\}_k))(x_n) = f[n] = \sum_{k=0}^{N-1} F_k \, e^{i2\pi k n / N} \qquad n = 0,1,2,\ldots,N\!-\!1
\end{empheq}
\end{subequations}
  , where square brackets are used to connote indices for discrete transforms, curly braces are used for sequence notation (as distinguished from the continuous variable), and the symbol $\mathcal{F}$ denotes the DFT. 

  The alternative form, used by Osgood\cite{Osgood}, Brigham\cite{Brigham}, Oppenheim \& Schafer\cite{OppenheimSchafer}, Percival \& Wanlden\cite{PercivalWalden2020}, etc. is
  % Note: have to use mathfrak instead of mathcal because mathcal doesn't work for lowercase letters
\begin{subequations}
\begin{empheq}[box=\widefbox]{align}
  F[k] = \sum_{n=0}^{N-1} f_n \, e^{-i2\pi k n / N} \qquad k = 0,1,2,\ldots,N\!-\!1 \\
  f[n] = {1\over N} \, \sum_{k=0}^{N-1} F_k \, e^{i2\pi k n / N} \qquad n = 0,1,2,\ldots,N\!-\!1
\end{empheq}
\end{subequations}
  \ul{The $1/N$ factor is another feature of the DFT that distinguishes it from the continuous Fourier transform, and is an important practical detail}.
  I do not have a mathematical explanation for why the $1/N$ factor {\em should} be associated with the IDFT.
  But there is a significant computational motivation that serves as justification.
  In practice, the DFT will probably be calculated repeatedly in search of a good estimate, or continuously as data is streaming.
  The additional computational cost of including this $1/N$ factor up front in the calculation is enormous, while providing little additional information.
  In principle, the IDFT is only calculated once for the purpose of signal reconstruction.
  The additional computational cost imposed at this final stage is insignificant by comparison.
  {\bf So this is the one you should use!}

  A pragmatic rationale for associating the foctor of $1/N$ with the IDFT is that if {\em you forgot} to include it in your calculation of the DFT, it's totally fine you can reach for it when you need it.


  \subsection{Derivation of the DFT via Numerical Approximation (end points)}

  ...Important practical details for implementing the DFT can be seen by looking at other derivations of the DFT...
  Here we will explore:
\begin{itemize}
	\item how to handle data at the end-points?
	\item what does the DFT really calculate?
\end{itemize}

  The DFT can be derived as a numerical approximation to the Fourier transform.
  $$\hat f (s) = \int\limits_{-\infty}^\infty f(x) e^{-i2\pi s x}$$
  Assume the function/signal is zero outside the finite sampling domain.
  $$\hat f (s_k) = \int_L f(x) e^{-i2\pi s_k x}$$

  The area of a trapazoid based on the average of the two parallel sides is
  $$\Biggl( {g(x) + g(x+1) \over 2} \Biggr) \triangle x$$
  such that the trapazoid rule is
  $$ \int_L g(x) \,dx \approx
  {\triangle x\over 2} \Biggl( g(x_0) + (2) \sum_{n = 1}^{N - 1} g(x_n) + g(x_N) \Biggr)$$
  Assume (\ul{this is an important practical detail})
  $$\boxed{g(x_0) = g(x_N)}$$
  (this is reasonable when the periodic extension of the signal is often invoked but that is not the case here as the procedure clearly employs zero-padding); for an asymmetric interval $g(0) = g(L)$; for an symmetric interval $g(-L/2) = g(L/2)$.
  Then
  $$ = \triangle x \sum_n g(x_n)$$
  Then
  $$ {\hat f} (s_k) \approx {L\over N} \sum_n f(x_n) \,e^{-i2\pi s_k x_n}$$
  Using the reciprocity relation, $\triangle s^{-1} \triangle x^{-1} = 1/N$, it follows that the set of points $s_k x_n = kn/N$, which allows the substitution 
  $$ (\mathscr{F}\!f)(s)_k= F(s_k) = {\hat f} (s_k) \approx L \underbrace{{1\over N} \sum_n f(x_n) \,e^{-i2\pi k n/N}}_{\text{$F[k]$}}$$

  This expression mirrors the continuous case (see sections on Fourier coefficients and the derivation of the Fourier transform)
  $$F(s_k) = L c_k$$
  \ul{suggesting that the DFT is an approximation for the Fourier coefficients (an important practical detail)}.
  The relationship between the continuous Fourier transform and the DFT is then (p.23\cite{BriggsHenson})
  $$\boxed{F(s_k) \approx L F[k]}$$


  \subsection{Derivation of the DFT via Approximation to the Fourier Coefficients (series convergence and discontinuities)}

  Recall, the Fourier coefficients are obtained from
  $$c_k = {1\over L} \int_L f(x) e^{-i2\pi k x} \,d x$$ 
  ...
  The assumption
  $$f(x) = \sum_{k=-\infty}^\infty c_k e^{i2\pi k x}$$
  requires that the series converges.
  Convergence of Fourier series is deep ...
  Briggs + Henson are content with the definition

  def. {\bf piecewise continuous} on $[a, b]$
  There are finite number of points where $x_i$ is either not defined or discontinuous but the limits
  $$f(x_i^+) = \lim_{h\rightarrow 0^+} f(x_i + h)$$
  $$f(x_i^-) = \lim_{h\rightarrow 0^-} f(x_i + h)$$
  \ul{exist} and are \ul{finite}.

  {def. {\bf piecewise smooth} on $[a, b]$
  $f$ is piecewise continuous, and
  $$f^\prime(x_i^+) = \lim_{h\rightarrow 0^+} {f(x_i + h) - f(x_i)\over h}$$
  $$f^\prime(x_i^-) = \lim_{h\rightarrow 0^-} {f(x_i + h) - f(x_i)\over h}$$
  
  \ul{\em Theorem: Convergence of Fourier Series} (ref.158\cite{BriggsHenson})

  If $f$ is $L$-periodic and piecewise smooth then % discussion on the boundaries p.38 BriggsHenson
  $$f(x) = \sum_{k=-\infty}^\infty c_k e^{i2\pi k x}$$
  converges (pointwise) to
  $${f(x^+) + f(x^-)\over 2}$$
  for every $x$, which reveals an important practical detail, \ul{points of discontinuity take the average value of the points to the left and to the right}.

  If $f$ is undefined outside the sampling interval or is aperiodic over $L$, the Fourier series converges to the periodic extension.
  A subtle but important consequence of this periodic extension is the practical detail that the end points must be equal, ie: if $f(x_0=0) \ne f(x_N=L)$ then one value, eg: the latter, end point can be set such that (p.38 \cite{BriggsHenson})
  $$f(x_N) = {f(x_0^+) + f(x_N^-) \over 2}$$
  For $N$ even and ranging from 0 to $N-1$ there is no data point for $f(x_N=L)$ to terminate the sequence.
  The first point in the periodic extension is used as the $(N\!+\!1)$-data point needed to compute the $N$-point DFT.
  \ul{To initial data point, at index $n=0$, and the last data point, at index $n=N$, are both supplied manually as}
  $$f[N] = {f[0] + f[N-1] \over 2}$$
  to ensure continuity of the periodic extension.
  For $N$ even and $N$ ranges from $-N/2+1$ to $N/2$ there is no data point for $f(x_{-N/2}=-L/2)$, 
  The first data point, at index $n=-N/2$, is supplied manually as
  $$f[-N/2] = {f[-N/2+1] + f[N/2] \over 2}$$
  What to do in practice for $N$ odd?...

  The DFT can be derived by writing the integral for the Fourier coefficients as a series approximation
\begin{align*}
  c_k = {1\over L} \int_L f(x) e^{-i2\pi sx} \,dx &\approx {1\over L} \sum_{n=0}^{N-1} f_n e^{-i2\pi kn/N} \triangle x \\
  &= {1\over L}\sum_n f_n \omega_N^{-kn} \Bigl({L\over N}\Bigr) \\
  &= {1\over N}\sum_n f_n \omega_N^{-kn} = F[k] = (\mathfrak{F}(f(\{x\}_n))(s_k) \\
\intertext{, which shows that the DFT is an approxmation to the Fourier coefficients}
  \Aboxed{c_k &\approx F[k]}
\end{align*}


  \subsection{``The DFT from Trigonometric Approximation''\cite{BriggsHenson}}%Chapter 6 of The DFT An Owner' Manual}

  Idea: use trigonometric polynomial, a linear combination of sines and cosines, to fit data.
  Model data using trigonometric polynomial
  $$\psi(x) = \sum c_k e^{i2\pi k x / L}$$
  (This instance it is more convenient to work with an odd no. samples using the symmetric representation...)
  The (discrete) least square error
  $$E = \sum |f_n - \psi(x_n)|^2$$
  is non-negative real-valued function.

  (...)
  Minimize the error by writing the ``normal equations'' (prob.26)
  (...)
  Write the DFT based on the coefficients...(? I though the formula for the FT and the FT coefficients are different by a factor of $1\L$)!?!)...(another way prob.28)

  (...)
  Parseval's relation for the DFT
  $$\sum_n |f_n|^2 = N \sum_n |c_n|^2$$
  *** In practice this serves as a useful way to check your work (without having to reconstruct the signal).

  \subsection{The DFT Matrix}

  The DFT for a specific $k$ is the inner product of the discrete signal with the corresponding row of the DFT matrix.
  The elements of the DFT matrix are $\omega_N^{-kn} = e^{-i2\pi k n / N}$, where the $k$-index is associated with the row index, and the $n$-index is associated with the column index.
  For the $N$-point DFT indexed from $0$ to $N-1$  
  $$F[k] = {1\over N}\sum_{n=0}^{N-1} f_n \omega_N^{-kn}$$
  has a DFT matrix $W_{kn}$
$$W_{kn} = 
{1\over N}
\begin{bmatrix}
  \omega_N^{-(0)(0)} & \omega_N^{-(0)(1)} & \ldots & \omega_N^{-(0)(n)} & \ldots & \omega_N^{-(0)(N-1)} \\
  \omega_N^{-(1)(0)} & \omega_N^{-(1)(1)} & \ldots & \omega_N^{-(1)(n)} & \ldots & \omega_N^{-(1)(N-1)} \\
  \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
  \omega_N^{-(k)(0)} & \omega_N^{-(k)(1)} & \ldots & \omega_N^{-(k)(n)} & \ldots & \omega_N^{-(k)(N-1)} \\
  \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
  \omega_N^{-(N-1)(0)} & \omega_N^{-(N-1)(1)}& \ldots & \omega_N^{-(N-1)(n)}& \ldots & \omega_N^{-(N-1)(N-1)} \\
\end{bmatrix}
$$
  where $k$ is the row and $n$ specifies the column.


  \section{Properties of the DFT}

  \subsection{DFT Duality}

  The definitions of the DFT and the IDFT operators
  $$\mathcal{F} = {1\over N}\,\sum_n w_N^{-kn} \qquad\text{and}\qquad \mathcal{F}^{^{-1}} = \sum_k w_N^{kn}$$
  leed to the duality statements
\begin{subequations}
\begin{empheq}[box=\widefbox]{align}
  \mathcal{F}(f^{(-)}) = {1\over N}\, (\mathcal{F}^{^{-1}}\!f) \\
  \mathcal{F}^{^{-1}}(f^{(-)}) = (N)\, (\mathcal{F}\!f)
\end{empheq}
\end{subequations}

  The definitions of the DFT and the IDFT operators
  $$\mathcal{F} = \sum_n w_N^{-kn} \qquad\text{and}\qquad \mathcal{F}^{^{-1}} = {1\over N} \sum_k w_N^{kn}$$
\begin{subequations}
\begin{empheq}[box=\widefbox]{align}
  \mathcal{F}f^{(-)} = (N) \, (\mathcal{F}^{^{-1}}\!f) \\
  \mathcal{F}^{^{-1}}(f^{(-)}) = {1\over N} \, (\mathcal{F}f)
\end{empheq}
\end{subequations}

  Duality begins with the reverse signal, where the signal starts with the last index and continues counting down to the starting index, which correspond to the last and first sampling points, respectively.
  The $N$-element index $[n]$ proceeds from 0 up to $N-1$ as
  $$ [n] : 0, 1, 2, \ldots, N-1 $$
  The $[N-n]$-index counts down from $N$ to 1 as
  $$ [N-n] : N, N-1, N-2, \ldots, 1 $$
  Then the reverse signal can be written as
  $$ f[n]^{(-)} = f[N-n] $$

  The DFT of the reverse signal is
  $$ \mathcal{F}(f[n]^{(-)}) = {1\over N}\sum_n e^{-i2\pi kn/N} f_n^{(-)} = {1\over N}\sum_n e^{-i2\pi kn/N} f[N-n] = {1\over N}\sum_n e^{-i2\pi k n/N} f[-n] $$
  by the $N$-periodicity of the DFT.
  (Recall:
\[
\begin{matrix}
\mathcal{F}(f[0]) & = & \mathcal{F}(f[\pm N]) & = & \mathcal{F}(f[\pm 2 N]) & = & \mathcal{F}(f[\pm 3 N]) & \ldots \\
\mathcal{F}(f[1]) & = & \mathcal{F}(f[\pm N + 1]) & = & \mathcal{F}(f[\pm 2 N + 1]) & = & \mathcal{F}(f[\pm 3 N + 1]) & \ldots \\
\mathcal{F}(f[-1]) & = & \mathcal{F}(f[\pm N - 1]) & = & \mathcal{F}(f[\pm 2 N - 1]) & = & \mathcal{F}(f[\pm 3 N - 1]) & \ldots \\
\vdots & & & & \\
\mathcal{F}(f[\pm n]) & = & \mathcal{F}(f[\pm N \pm n]) & = & \mathcal{F}(f[\pm 2 N \pm n]) & = & \mathcal{F}(f[\pm 3 N \pm n]) & \ldots \\
\end{matrix}
\]
  from the cyclic behavior of the complex exponentials, $w_N^{k (n \pm N)} = w_N^{k n}$ ).

  Substituting ($n$) for ($-n$) is straight forward enough but let's be pedantic here, if there is any place to be long winded if explicit it is with the earlier simpler material.
  Let $m=-n$.
  The $-n$ index goes from
  $$[m] = [-n] : 0, -1, -2, -3, \ldots , -(N-1) $$
  $$= {1\over N} \sum_{n=0}^{N-1} e^{-i2\pi k (n)/N} f[-n] = {1\over N} \sum_{m=0}^{-(N-1)} e^{i2\pi k (m)/N} f[m]$$ 
  ... the cylcic property of $w_N$ means any consecutive $N$ integers will do...
  $$ {1\over N} \sum_{m=0}^{-(N-1)} e^{i2\pi k (m)/N} f[m] = {1\over N} \sum_{n=0}^{N-1} e^{i2\pi k (n)/N} f[n] $$

  in the argument to $f$ and in the complex exponentials gives
  $$= {1\over N} \sum_n e^{-i2\pi k (n)/N} f[(-n)] = {1\over N} \sum_n e^{-i2\pi k (-n) / N} f[(n)] = {1\over N} \sum_n e^{i2\pi k n / N} f[n] $$
  Here it helps to be flexible with our interpretation of the indices/variables, and say that the DFT is essentially the sum of cyclic complex exponentials with a negative exponent while the IDFT is the sum of cyclic complex exponentials with a possitive exponent.
  Then the DFT and the IDFT applied to the same function ignoring the meaning or physical significance of the summation indices are
  $$ \mathcal{F}(f[n]) = {1\over N} \sum_n e^{-i2\pi k n/N} f[n] \qquad\text{and}\qquad \mathcal{F}^{^{-1}}\!(f[k]) = \sum_k e^{i2\pi k n/N} f[k] $$
  From this perspective, the above equation shows the DFT applied to the reverse signal is equal to $1/N$ times the IDFT
  $$ \boxed{ \mathcal{F}(f^{(-)}) = {1\over N}\,\mathcal{F}^{^{-1}}(f) } $$
  , albeit not with the expected index.

  Similarly, the IDFT of the reverse signal
  $$ \mathcal{F}^{^{-1}}\!(f^{(-)}) = \sum_k f[k]^{(-)} \, w_N^{k n} $$
  By the $N$-periodicity of the IDFT
  $$= \sum_k f[N-k] \, w_N^{k n} = \sum_k f[-k] \, w_N^{k n} $$
  Substituting $k$ for $-k$
  $$= \sum_k f[k] \, w_N^{-k n} = {1\over N}\, \mathcal{F}(f[k]) $$
  where the $k$-index actually indexes over the function values via the spatial/temporal grid.
  $$ \boxed{ \mathcal{F}^{^{-1}}\!(f^{(-)}) = {1\over N} \, \mathcal{F}\!f } $$


  Now do this for the alternative versions of the DFT and IDFT...


  \subsection{The DFT \& Parseval's Theorem}

  Do discrete form of Parseval's theorem for both forms of the DFT equations...where to put the ${1\over N}$ factor?...

  COME BACK: (this is a practical and conceptual check)
  Taken from Numerical Methods p.608 s.12.1.2:
  If the DFT is defined as
  $$h_k = {1\over N} \sum_{n=0}^{N-1} H_n e^{-i2\pi kn/N}$$
  then the discrete form the Parseval's theorem is
  $$\sum_{k=0}^{N-1} |h_k|^2 = {} \sum_{n=0}^{N-1} |H_n|^2 $$



  \subsection{The DFT \& Discrete Convolution}

  Using the definitions for DFT and IDFT
  $$\mathcal{F}f = \sum_n f[n] \, w_N^{-kn} \qquad\text{and}\qquad \mathcal{F}^{^{-1}}F = {1\over N} \sum_k F[k] \, w_N^{kn} $$
  convolution in the spatial/time domain is defined as
\begin{equation}
  \boxed{(f*g)[m] \equiv \sum_n f[n]\,g[m-n]}
\end{equation}
  where $[m]$ is a dummy index that references the $[n]$ grid.
  And the convolution theorem consists of the expressions
\begin{subequations}
\begin{empheq}[box=\widefbox]{align}
  ( \mathcal{F}\!f ) \, ( \mathcal{F}\!g ) &= \mathcal{F} (f * g) \\
  (\mathcal{F}^{^{-1}}\!(f)) \, (\mathcal{F}^{^{-1}}\!(g)) &= {1\over N} \mathcal{F}^{^{-1}} (f*g) \\
  \mathcal{F}(f) * \mathcal{F}(g) &= N \, \mathcal{F} (f \, g) \\
  \bigl(\mathcal{F}^{^{-1}}\!f\bigr)*\bigl(\mathcal{F}^{^{-1}}\!g\bigr) &= \mathcal{F}^{^{-1}}(f\,g)
\end{empheq}
\end{subequations}

  For this topic it may be useful to explore the different versions of the DFT (where to put the factor of $1/N$) to try and understand why various authors adopt one convention over the other.

  In the continuous case, the definition of the convolution was prompted by the question, ``Given ($f$) and ($g$), is there an ($h$) such that''
\begin{align*}
  F G = \mathscr{F}(f) \, \mathscr{F}(g) &= \mathscr{F}(h) \\
  \intertext{For the DFT this becomes}
  F\,G = \mathcal{F}(f)\,\mathcal{F}(g) &\stackrel{?}{=} \mathcal{F}(h) \\
  \intertext{To find ($h$) take the IDFT}
  \mathcal{F}^{^{-1}} \Bigl(\mathcal{F}(f)\,\mathcal{F}(g) \Bigr) &= \mathcal{F}^{^{-1}} \mathcal{F}(h) = h
\end{align*}
  The functions $F$ and $G$ are both functions of the $k$-index, which refers to the frequency grid (in general, $F$ and $G$ are independent, and need not refer to the same frequency/ have the same index, ie: $F[k] G[\ell] $).
  $$\mathcal{F}\!f \,\mathcal{F}g = F[k] G[k] $$
  Separate indices are needed to destinguish each DFT's series summation over the spatial/temporal grid
  $$= \sum_n f_n\,e^{-i2\pi kn/N} \sum_{n'} g_{n'}\,e^{-i2\pi k n'/N} = \sum_n f_n\,w_N^{-kn} \sum_{n'} g_{n'}\,w_N^{-k n'} $$
  Writing out the multiplication of the series
\begin{align*}
  &= \bigl(f_0 w^{-k (0)} + f_1 w^{-k (1)} + \cdots + f_{N-1} w^{-k (N-1)} \bigr)
    \bigl( g_0 w^{-k (0)} + g_1 w^{-k (1)} + \cdots + g_{N-1} w^{-k (N-1)} \bigr) \\
  &=\quad f_0 w^{-k (0)} 
	\bigl( g_0 w^{-k (0)} + g_1 w^{-k (1)} + \cdots + g_{N-1} w^{-k (N-1)} \bigr) \\
	&\quad+ f_1 w^{-k (1)}
	\bigl( g_0 w^{-k (0)} + g_1 w^{-k (1)} + \cdots + g_{N-1} w^{-k (N-1)} \bigr) \\
	&\quad\:\:\vdots \\
	&\quad+ f_{N-1} w^{-k (N-1)}
	\bigl( g_0 w^{-k (0)} + g_1 w^{-k (1)} + \cdots + g_{N-1} w^{-k (N-1)} \bigr)
\end{align*}
  shows it can be thought of as
  $$= \sum_n \biggl( e^{-i2\pi kn/N} f_n \sum_{n'} e^{-i2\pi k n'/N} g_{n'} \biggr)
  \stackrel{\text{and}}{=} \sum_{n'} \biggl( e^{-i2\pi kn'/N} g_{n'} \sum_{n} e^{-i2\pi k n/N} f_{n} \biggr) $$
  Grouping the terms inside the parentheses
  $$= \sum_n \biggl( \Bigl( e^{-i2\pi kn/N} f_n \Bigr) \Bigl( \sum_{n'} e^{-i2\pi k n'/N} g_{n'} \Bigr) \biggr) $$
  suggests, by associativity, the rearrangement
  $$= \sum_n \sum_{n'} f_n\,g_{n'} e^{-i2\pi kn/N} e^{-i2\pi k n'/N} $$
  The double sum produces all $N \text{x} N$ combinations of the indices in constructing the series
\begin{align*}
  &=\quad
	f_0 g_0 w^{-k (0)} w^{-k (0)} + 
	f_0 g_1 w^{-k (0)} w^{-k (1)} +
	\cdots + 
	f_0 g_{N-1} w^{-k (0)} w^{-k (N-1)} \\
	&\quad+ 
	f_1 g_0 w^{-k (1)} w^{-k (0)} +
	f_1 g_1 w^{-k (1)} w^{-k (1)} + 
	\cdots + 
	f_1 g_{N-1} w^{-k (1)} w^{-k (N-1)} \\
	&\quad\:\:\vdots \\
	&\quad+ 
	f_{N-1} g_0 w^{-k (N-1)} w^{-k (0)} + 
	f_{N-1} g_1 w^{-k (N-1)} w^{-k (1)} + 
	\cdots + f_{N-1} g_{N-1} w^{-k (N-1)} w^{-k (N-1)}
\end{align*}

  To uncover ($h$) take the IDFT
\begin{align*}
  \mathcal{F}^{^{-1}} \Bigl( \mathcal{F}\!f \, \mathcal{F}g \Bigr) [m] &= \mathcal{F}^{^{-1}} \Bigl( F[k]\, G[k] \Bigr) [m] \\
  &= \mathcal{F}^{^{-1}} \Biggl( \biggl( \sum_n f_n w^{-kn}_N \biggr) \biggl( \sum_{n'} g_{n'} w^{-k n'}_N \biggr) \Biggr) [m] \\
  &= {1\over N} \sum_{k} w^{+k m}_N \Biggl( \biggl( \sum_n f_n w^{-kn}_N \biggr) \biggl( \sum_{n'} g_{n'} w^{-k n'}_N \biggr) \Biggr)
\end{align*}
\begin{align*}
  &= {1\over N}\bigl(w^{+(0) m} + w^{+(1) m} + \cdots + w^{+(N-1) m}\bigr) \\
	&\quad
	\Bigl(
	f_0 g_0 w^{-k (0)} w^{-k (0)} + 
	f_0 g_1 w^{-k (0)} w^{-k (1)} +
	\cdots + 
	f_0 g_{N-1} w^{-k (0)} w^{-k (N-1)} \\
	&\qquad+ 
	f_1 g_0 w^{-k (1)} w^{-k (0)} + 
	f_1 g_1 w^{-k (1)} w^{-k (1)} + 
	\cdots + 
	f_1 g_{N-1} w^{-k (1)} w^{-k (N-1)} \\
	&\qquad\:\:\vdots \\
	&\qquad+ 
	f_{N-1} g_0 w^{-k (N-1)} w^{-k (0)} + 
	f_{N-1} g_1 w^{-k (N-1)} w^{-k (1)} + 
	\cdots + 
	f_{N-1} g_{N-1} w^{-k (N-1)} w^{-k (N-1)}
	\Bigr) \\
	\\
  &= {w^{+(0) m} \over N} 
	\Bigl(
	f_0 g_0 w^{-k (0)} w^{-k (0)} + 
	\cdots + 
	f_{N-1} g_{N-1} w^{-k (N-1)} w^{-k (N-1)}
	\Bigr) \\
  &\quad+ {w^{+(1) m} \over N} 
	\Bigl(
	f_0 g_0 w^{-k (0)} w^{-k (0)} + 
	\cdots + 
	f_{N-1} g_{N-1} w^{-k (N-1)} w^{-k (N-1)}
	\Bigr) \\
  &\quad\:\:\vdots \\
  &\quad+ {w^{+(N-1) m} \over N} 
	\Bigl(
	f_0 g_0 w^{-k (0)} w^{-k (0)} + 
	\cdots + 
	f_{N-1} g_{N-1} w^{-k (N-1)} w^{-k (N-1)}
	\Bigr)
\end{align*}
\begin{align*}
  &= {1\over N} \sum_{k} w^{+k m}_N \biggl( \sum_n \sum_{n'} f_n \, g_{n'} w^{-kn}_N w^{-k n'}_N \biggr) \\
  &= {1\over N} \biggl( \sum_n \sum_{n'} f_n \, g_{n'} w^{-k(n+n')}_N \biggr) \sum_{k} w^{+k m}_N \\
  &= {1\over N} \sum_n \sum_{n'} f_n \, g_{n'} \sum_{k} w^{+k m}_N w^{-k(n+n')}_N \\
  &= {1\over N} \sum_n \sum_{n'} f_n \, g_{n'} \sum_{k} w^{k(m-n-n')}_N \\
  &= {1\over N} \sum_n \sum_{n'} f_n \, g_{n'} \bigl( N\,\hat{\delta}_N(m-n-n') \bigr) \\
  &= \sum_n \sum_{n'} f_n \, g_{n'} \, \hat{\delta}_N(m-n-n') \\
  \intertext{Performing the sum over $n'$}
  &= \sum_n f[n] \sum_{n'} \, g[n'] \, \hat{\delta}_N(m-n-n') \\
  &= \sum_n f[n] \, g[m-n]
\end{align*}
  This derivation leads to \ul{the definition of the discrete convolution}
\begin{equation}
  \boxed{(f*g)[m] \equiv \sum_n f[n]\,g[m-n]}
\end{equation}
  and shows that \ul{the product of DFTs is the DFT of the convolution}
\begin{equation}
  \boxed{ ( \mathcal{F}\!f ) \, ( \mathcal{F}\!g ) = \mathcal{F} (f * g) }
\end{equation}

  The IDFT version of this procedure is
  $$ \mathcal{F} \bigl( \mathcal{F}^{^{-1}}\!(f)[n]\,\mathcal{F}^{^{-1}}\!(g)[n] \bigr)[\ell] = \mathcal{F}^{^{-1}}\!(h) $$
  $$= \sum_n e^{-i2\pi \ell n /N} \Biggl( \biggl({1\over N} \sum_k f_k \, e^{+i2\pi k n / N} \biggr) \biggl({1\over N} \sum_{k'} g_{k'} \, e^{+i2\pi k' n / N} \biggr) \Biggr) $$
  $$= \sum_n w_N^{-\ell n} \biggl({1\over N^2} \sum_k \sum_{k'} f_k \, g_{k'} \, w_N^{(k+k') n} \biggr) $$
  $$= {1\over N^2} \sum_k \sum_{k'} f_k \, g_{k'} \, w_N^{(k+k') n} \sum_n w_N^{-\ell n} $$
  $$= {1\over N^2} \sum_k \sum_{k'} f_k \, g_{k'} \, \sum_n w_N^{(k+k'-\ell) n} $$
  $$= {1\over N^2} \sum_k \sum_{k'} f_k \, g_{k'} \, N \hat{\delta}_N(k+k'-\ell) $$
  Performing the summation over $k'$
  $$= {N\over N^2} \sum_k f[k] \sum_{k'} g[k'] \, \hat{\delta}_N(k+k'-\ell) $$
  selects the value, of $g$, when $k' = \ell - k$
  $$= {1\over N} \sum_k f[k] g[\ell-k] = {1\over N} (f*g) $$
\begin{equation}
  \boxed{ \mathcal{F}^{^{-1}}\!(f) \, \mathcal{F}^{^{-1}}\!(g) = {1\over N}\, \mathcal{F}^{^{-1}} (f*g) }
\end{equation}


  Using the definition of the convolution, the convolution of DFTs is
\begin{align*}
  (\mathcal{F}\!f)[k] * (\mathcal{F}\!g)[k] &= F[k] * G[\ell - k] \\
  &= \sum_k F[k] \, G[\ell - k] \\
  &= \sum_k \Biggl( \biggl( \sum_n f_n w_N^{-kn} \biggr) \biggl( \sum_{n'} g_{n'} w_N^{-(\ell-k) n'} \biggr) \Biggr) \\
  &= \sum_n f_n \sum_{n'} g_{n'} \sum_k w_N^{-kn} \, w_N^{-(\ell-k) n'} \\
  &= \sum_n f_n \sum_{n'} g_{n'} \sum_k w_N^{-kn} \, w_N^{-\ell n'} \, w_N^{+kn'} \\
  &= \sum_n f_n \sum_{n'} g_{n'} \,w_N^{-\ell n'} \sum_k w_N^{-kn} w_N^{+kn'} \\
  &= \sum_n f_n \sum_{n'} g_{n'} \,w_N^{-\ell n'} \sum_k w_N^{k(n'-n)} \\
  &= \sum_n f_n \sum_{n'} g_{n'} \,w_N^{-\ell n'} \Bigl( \,N \,\hat{\delta}_N (n'-n) \Bigr) \\
  &= (N) \sum_n f_n \sum_{n'} g_{n'} \,w_N^{-\ell n'} \,\hat{\delta}_N (n'-n) \\
\intertext{Performing the summation over $n'$}
  &= (N) \sum_n f[n] g[n] \,w_N^{-\ell n} \\
\intertext{, which is the DFT of the product of $f$ and $g$ evaluated at $\ell$}
  &= (N) \bigl(\mathcal{F}(fg)\bigr)[\ell]
\end{align*}

  The (discrete) convolution theorem also refers to \ul{the convolution of DFTs}, which under the given definitions for the DFT pair, is 
\begin{equation}
  \boxed{\mathcal{F}(f) * \mathcal{F}(g) = N \, \mathcal{F} (f \, g) }
\end{equation}

  
  Using the definition of the convolution, the convolution of IDFTs is
\begin{align*}  
  \bigl(\mathcal{F}^{^{-1}}\!f\bigr)*\bigl(\mathcal{F}^{^{-1}}\!g\bigr) &= \Bigl(\bigl(\mathcal{F}^{^{-1}}\!f\bigr)[n]\Bigr) * \Bigl(\bigl(\mathcal{F}^{^{-1}}\bigr)[n]\Bigr) \\
  &= \sum_n \bigl(\mathcal{F}^{^{-1}}\!f\bigr)[n] \, \bigl(\mathcal{F}^{^{-1}}\!g\bigr)[m-n] \\
  &= \sum_n \Biggl( \biggl({1\over N} \sum_k f_k \, w_N^{+ k n} \biggr) \, \biggl({1\over N} \sum_{k'} g_{k'} \, w_N^{+ k' (m - n)} \biggr) \Biggr) \\
  &= {1\over N^2} \sum_n \sum_k f_k \sum_{k'} g_{k'} \, w_N^{k n} \, w_N^{k' (m - n)} \\
  &= {1\over N^2} \sum_k f_k \sum_{k'} g_{k'} \sum_n w_N^{k n} \, w_N^{k' m} \, w_N^{-k' n} \\
  &= {1\over N^2} \sum_k f_k \sum_{k'} g_{k'} \, w_N^{k' m} \sum_n w_N^{(k-k') n} \\
  &= {1\over N^2} \sum_k f_k \sum_{k'} g_{k'} \, w_N^{k' m} \, N\,\hat{\delta}_N (k-k') \\
\intertext{Performing the summation over $k'$ the discrete delta, $\hat{\delta}_N$, selects the $k^{\text{th}}$ element}
  &= {N\over N^2} \sum_k f[k] g[k] \, w_N^{k m} \\
\intertext{, which is the IDFT evaluated at $\text{IDFT}[m]$}
  &= {1\over N} \sum_k f[k] g[k] \, w_N^{k m} = \bigl(\mathcal{F}^{^{-1}}(f \, g)\bigr)[m]
\end{align*}
  meaning \ul{the convolution of IDFTs is}
\begin{equation}
  \boxed{ \bigl(\mathcal{F}^{^{-1}}\!f\bigr)*\bigl(\mathcal{F}^{^{-1}}\!g\bigr) = \mathcal{F}^{^{-1}}(f\,g) }
\end{equation}



  \ul{Using the alternative definitions for the DFT and the IDFT}
  $$(\mathcal{F}\!f)[k] = {1\over N} \sum_n w_N^{-kn} \qquad\text{and}\qquad (\mathcal{F}^{^{-1}}\!f)[n] = \sum_k w_N^{kn}$$
  the procedure for deriving the discrete convolution becomes
\begin{align*}
  \mathcal{F}^{^{-1}} \Bigl( \mathcal{F}\!f \, \mathcal{F}g \Bigr) [m] &= \mathcal{F}^{^{-1}} \Bigl( F[k]\, G[k] \Bigr) [m] \\
  &= \sum_{k} w^{k m}_N \Biggl( \biggl( {1\over N} \sum_n f_n \,w^{-kn}_N \biggr) \biggl( {1\over N} \sum_{n'} g_{n'} \,w^{-k n'}_N \biggr) \Biggr) \\
  &= {1\over N^2} \biggl( \sum_n \sum_{n'} f_n\,g_{n'} \,w^{-k(n+n')}_N \biggr) \sum_{k} w^{k m}_N \\
  &= {1\over N^2} \sum_n \sum_{n'} f_n\,g_{n'} \sum_{k} w^{k (m-n-n')}_N \\
  &= {1\over N^2} \sum_n \sum_{n'} f_n\,g_{n'} \,N \hat{\delta}_N (m-n-n') \\
  &= {1\over N} \sum_n f[n] \, g[m-n]
\end{align*}
  and the definition of the convolution under this definition of the DFT ($\mathcal{F}\!f = {1\over N} \sum_n f_n \, w_N^{-kn}$) is
  $$\boxed{(f*g)[m] \equiv {1\over N} \sum_n f[n]\,g[m-n]} $$


  MOVE THESE NOTES TO END OF SECTION AND CLEAN UP...
  Briggs and Henson define the discrete convolution as (p.81 \cite{BriggsHenson})
  $$ f_n * g_n = \sum_j f_j g_{n-j} $$
  I find this notation incredibly confusing, $f$ and $g$ ostensibly share an index, $n$, yet it is not the index referenced in the summation?
  But their description of the sequences is interesting...
  To form the sequence $g_{n-j}$ first form the sequence $g_{-j}$ from the reverse sequence then shift this to the left by $n$ ... need to think more about this... since the convolution index feels to me like it should be a dummy index that eventually supplants the old index (even if in the end it refers to the same grid).
  

  *** IMPORTANT FORMALISM DETAILS BY YOU!!! *** At some point you need to lay down your formalism about discrete delta functions and the need to perform a sum in order to evaluate them, to ``collapse the delta function''... *** 

  MIT OCW 6.003 Signals and Systems, Fall 2011 Lect. 8 Convolution
  def. convolution
  $$y[n] = \sum_k x[k] h[n-k] = (x * h)[n]$$
  convolution is applied to (entire) signals not samples.
  Admittedly confusing conventional notation (min 14)
  $$y[n] = \sum_k x[k] h[n-k] = x[n] * h[n]$$
  ...really, neither $x$ or $h$ is a function of $n$ but rather $(x * h)[n]$ is a function of $n$.
  A better notation is
  $$(f[n] * g[n])[m] = \sum_n f[n] g[m-n]$$
  It is important to note the index $[m]$ has not yet been defined but it is essentially an alternative (dummy) index for the same grid.

  Freeman does a visualization that should be good reference for convolution but 
  WORKING ON IT
  A semi-illustration of the convolution algorithm

  NOTE: Ans. it is $h[k]$ reflected about the origin of the $[k]$ axis $[k=0]$ min 25:41 (but we do not know if $N$ is even or odd, although it is on a symmetric axis, in the example shown).

  The discrete convolution for $N$ even, ie: $N=8$, on a symmetric grid (axis centered at zero) of $f$ convolved with itself
  NOTE: change this from $N=8$ to $N=6$ so you can show the steps of convolution on a row...

  Just focus on the axes...put this with the convolution-algorithm it is associated with...do the other version for the alternative convolution-algorithm and place it accordingly...
\begin{figure}[H]
\centering
\begin{tikzpicture}
% Titles
\node[anchor=south] (note) at (0, 4) {Convolution Axes for the Asymmetric (Temporal/Spatial) Grid};
\node[anchor=north] (note) at (0, 4) {Total No. Samples $N=6$};
\node[anchor=east] (note) at (-1, -2) {$[m]$};

% links to corresponding indices during summation (do first so next layer on top)
\draw[dotted, very thick, color=purple, opacity=0.6] (0,0) -- (0,2); 
\draw[dotted, very thick, color=darkgray, opacity=0.6] (-1,0) -- (1,2); 
\draw[dotted, very thick, color=orange, opacity=0.6] (-2,0) -- (2,2); 
\draw[dotted, very thick, color=blue, opacity=0.6] (-3,0) -- (3,2); 
\draw[dotted, very thick, color=green, opacity=0.6] (-4,0) -- (4,2); 
\draw[dotted, very thick, color=brown, opacity=0.6] (-5,0) -- (5,2); 

% [-n] index
\node[anchor=west] (note) at (1, 0) {$[-n]$};
\draw[->, thick]
    (0,0) node[] {$|$}
 -- (0,0) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (0,0) node[anchor=south, yshift=1.5ex] {``1''}
 -- (-1,0) node[] {$|$}
 -- (-1,0) node[anchor=north, yshift=-1.5ex] {$-1$}
 -- (-1,0) node[anchor=south, yshift=1.5ex] {``2''}
 -- (-2,0) node[] {$|$}
 -- (-2,0) node[anchor=north, yshift=-1.5ex] {$-2$}
 -- (-2,0) node[anchor=south, yshift=1.5ex] {``3''}
 -- (-3,0) node[] {$|$}
 -- (-3,0) node[anchor=north, yshift=-1.5ex] {$-3$}
 -- (-3,0) node[anchor=south, yshift=1.5ex] {``4''}
 -- (-4,0) node[] {$|$}
 -- (-4,0) node[anchor=north, yshift=-1.5ex] {$-4$}
 -- (-4,0) node[anchor=south, yshift=1.5ex] {``5''}
 --	(-5,0) node[] {$|$}
 -- (-5,0) node[anchor=north, yshift=-1.5ex] {$-5$}
 -- (-5,0) node[anchor=south, yshift=1.5ex] {``6''}
 --	(-6,0) node[] {$|$}
 -- (-6,0) node[anchor=north, yshift=-1.5ex] {$-6$}
 -- (-7,0);
\filldraw[color=black] (0,0) circle (.1);
\filldraw[color=black] (-1,0) circle (.1);
\filldraw[color=black] (-2,0) circle (.1);
\filldraw[color=black] (-3,0) circle (.1);
\filldraw[color=black] (-4,0) circle (.1);
\filldraw[color=black] (-5,0) circle (.1);
\filldraw[color=black, fill=white] (-6,0) circle (.1);

% [n] index
\node[anchor=east] (note) at (-1, 2) {$[n]$};
\draw[->, thick]
	(0,2) node[] {$|$}
 -- (0,2) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (0,2) node[anchor=south, yshift=1.5ex] {``1''}
 -- (1,2) node[] {$|$}
 -- (1,2) node[anchor=north, yshift=-1.5ex] {$1$}
 -- (1,2) node[anchor=south, yshift=1.5ex] {``2''}
 -- (2,2) node[] {$|$}
 -- (2,2) node[anchor=north, yshift=-1.5ex] {$2$}
 -- (2,2) node[anchor=south, yshift=1.5ex] {``3''}
 -- (3,2) node[] {$|$}
 -- (3,2) node[anchor=north, yshift=-1.5ex] {$3$}
 -- (3,2) node[anchor=south, yshift=1.5ex] {``4''}
 -- (4,2) node[] {$|$}
 -- (4,2) node[anchor=north, yshift=-1.5ex] {$4$}
 -- (4,2) node[anchor=south, yshift=1.5ex] {``5''}
 -- (5,2) node[] {$|$}
 -- (5,2) node[anchor=north, yshift=-1.5ex] {$5$}
 -- (5,2) node[anchor=south, yshift=1.5ex] {``6''}
 -- (6,2) node[] {$|$}
 -- (6,2) node[anchor=north, yshift=-1.5ex] {$6$}
 -- (7,2);
\filldraw[color=black] (0,2) circle (.1);
\filldraw[color=black] (1,2) circle (.1);
\filldraw[color=black] (2,2) circle (.1);
\filldraw[color=black] (3,2) circle (.1);
\filldraw[color=black] (4,2) circle (.1);
\filldraw[color=black] (5,2) circle (.1);
\filldraw[color=black, fill=white] (6,2) circle (.1);

% [m] axis
\draw[->, thick]
	(0,-2) node[] {$|$}
 -- (0,-2) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (0,-2) node[anchor=south, yshift=1.5ex] {``1''}
 -- (1,-2) node[] {$|$}
 -- (1,-2) node[anchor=north, yshift=-1.5ex] {$1$}
 -- (1,-2) node[anchor=south, yshift=1.5ex] {``2''}
 -- (2,-2) node[] {$|$}
 -- (2,-2) node[anchor=north, yshift=-1.5ex] {$2$}
 -- (2,-2) node[anchor=south, yshift=1.5ex] {``3''}
 -- (3,-2) node[] {$|$}
 -- (3,-2) node[anchor=north, yshift=-1.5ex] {$3$}
 -- (3,-2) node[anchor=south, yshift=1.5ex] {``4''}
 -- (4,-2) node[] {$|$}
 -- (4,-2) node[anchor=north, yshift=-1.5ex] {$4$}
 -- (4,-2) node[anchor=south, yshift=1.5ex] {``5''}
 -- (5,-2) node[] {$|$}
 -- (5,-2) node[anchor=north, yshift=-1.5ex] {$5$}
 -- (5,-2) node[anchor=south, yshift=1.5ex] {``6''}
 -- (6,-2) node[] {$|$}
 -- (6,-2) node[anchor=north, yshift=-1.5ex] {$6$}
 -- (7,-2);
\filldraw[color=black] (0,-2) circle (.1);
\filldraw[color=black] (1,-2) circle (.1);
\filldraw[color=black] (2,-2) circle (.1);
\filldraw[color=black] (3,-2) circle (.1);
\filldraw[color=black] (4,-2) circle (.1);
\filldraw[color=black] (5,-2) circle (.1);
\filldraw[color=black, fill=white] (6,-2) circle (.1);

\end{tikzpicture}%\caption{}
\end{figure}

  The $[m-n]$ itself is an index with elements  

\begin{align*}
  [0-n] &: [0, -1, -2,\ldots, -(N-1)] \\
  [1-n] &: [1, 0, -1, \ldots, 1-(N-1)] \\
  \vdots\quad & \\
  [m-n] &: [m, m-1, m-2, \ldots, m-n, \ldots, m-(N-1)] \\
  \vdots\quad & \\
  [(N-1)-n] &: [(N-1), (N-1)-1, (N-1)-2, \ldots, 0]
\end{align*}
  
  The question remains, what does $f[-n]$, and by extension $f[m-n]$, look like?
  To investigate this question, consider the convolution of $f[n]$ with itself
  $$(f*f)[m] = \sum_{n=0}^{N-1} f[n] f[m-n]$$
  where $f[n]$ is the 6 sample boxcar function defined over the asymmetric grid, including its periodic extension, as
\begin{figure}[H]
\centering
\begin{tikzpicture}
% Titles
\node[anchor=south] (note) at (0, 5.5) {$f[n]$};

% [n] axis
\draw[-, thick]
    (-8,4) node[] {$|$}
 -- (-8,4) node[anchor=north, yshift=-1.5ex] {$-8$}
 -- (-7,4) node[] {$|$}
 -- (-7,4) node[anchor=north, yshift=-1.5ex] {$-7$}
 -- (-6,4) node[] {$|$}
 -- (-6,4) node[anchor=north, yshift=-1.5ex] {$-6$}
 -- (-5,4) node[] {$|$}
 -- (-5,4) node[anchor=north, yshift=-1.5ex] {$-5$}
 -- (-4,4) node[] {$|$}
 -- (-4,4) node[anchor=north, yshift=-1.5ex] {$-4$}
 -- (-3,4) node[] {$|$}
 -- (-3,4) node[anchor=north, yshift=-1.5ex] {$-3$}
 -- (-2,4) node[] {$|$}
 -- (-2,4) node[anchor=north, yshift=-1.5ex] {$-2$}
 -- (-1,4) node[] {$|$}
 -- (-1,4) node[anchor=north, yshift=-1.5ex] {$-1$}
 --	(0,4) node[] {$|$}
 -- (0,4) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (1,4) node[] {$|$}
 -- (1,4) node[anchor=north, yshift=-1.5ex] {$1$}
 -- (2,4) node[] {$|$}
 -- (2,4) node[anchor=north, yshift=-1.5ex] {$2$}
 -- (3,4) node[] {$|$}
 -- (3,4) node[anchor=north, yshift=-1.5ex] {$3$}
 -- (4,4) node[] {$|$}
 -- (4,4) node[anchor=north, yshift=-1.5ex] {$4$}
 -- (5,4) node[] {$|$}
 -- (5,4) node[anchor=north, yshift=-1.5ex] {$5$}
 -- (6,4) node[] {$|$}
 -- (6,4) node[anchor=north, yshift=-1.5ex] {$6$}
 -- (7,4) node[] {$|$}
 -- (7,4) node[anchor=north, yshift=-1.5ex] {$7$}
 -- (8,4) node[] {$|$}
 -- (8,4) node[anchor=north, yshift=-1.5ex] {$8$};
\filldraw[color=black, opacity=0.3] (-8,4) circle (.1);
\filldraw[color=black, opacity=0.3] (-7,4) circle (.1);
\draw[-, ultra thick, opacity=0.3] (-6,4)  -- (-6,5);
\filldraw[color=black, opacity=0.3] (-6,5) circle (.1);
\draw[-, ultra thick, opacity=0.3] (-5,4)  -- (-5,5);
\filldraw[color=black, opacity=0.3] (-5,5) circle (.1);
\draw[-, ultra thick, opacity=0.3] (-4,4)  -- (-4,5);
\filldraw[color=black, opacity=0.3] (-4,5) circle (.1);
\filldraw[color=black, opacity=0.3] (-3,4) circle (.1);
\filldraw[color=black, opacity=0.3] (-2,4) circle (.1);
\filldraw[color=black, opacity=0.3] (-1,4) circle (.1);
\draw[-, ultra thick] (0,4)  -- (0,5);
\filldraw[color=black] (0,5) circle (.1);
\draw[-, ultra thick] (1,4)  -- (1,5);
\filldraw[color=black] (1,5) circle (.1);
\draw[-, ultra thick] (2,4)  -- (2,5);
\filldraw[color=black] (2,5) circle (.1);
\filldraw[color=black] (3,4) circle (.1);
\filldraw[color=black] (4,4) circle (.1);
\filldraw[color=black] (5,4) circle (.1);
\draw[-, ultra thick, opacity=0.3] (6,4)  -- (6,5);
\filldraw[color=black, opacity=0.3, fill opacity=0.3] (6,5) circle (.1);
\draw[-, ultra thick, opacity=0.3] (7,4)  -- (7,5);
\filldraw[color=black, opacity=0.3, fill opacity=0.3] (7,5) circle (.1);
\draw[-, ultra thick, opacity=0.3] (8,4)  -- (8,5);
\filldraw[color=black, opacity=0.3, fill opacity=0.3] (8,5) circle (.1);

\end{tikzpicture}%\caption{}
\end{figure}

  Then one version of what $f[-n]$ might be is a literal reversal of the data sampled, ie: the samples are reported in exactly the opposite order of how they were originally recorded

\begin{figure}[H]
\centering
\begin{tikzpicture}
% Titles
\node[anchor=south] (note) at (0, 1.5) {$f[-n]$?};

% f[-n] version 1
\draw[-, thick]
    (-8,0) node[] {$|$}
 -- (-8,0) node[anchor=north, yshift=-1.5ex] {$-8$}
 -- (-7,0) node[] {$|$}
 -- (-7,0) node[anchor=north, yshift=-1.5ex] {$-7$}
 -- (-6,0) node[] {$|$}
 -- (-6,0) node[anchor=north, yshift=-1.5ex] {$-6$}
 -- (-5,0) node[] {$|$}
 -- (-5,0) node[anchor=north, yshift=-1.5ex] {$-5$}
 -- (-4,0) node[] {$|$}
 -- (-4,0) node[anchor=north, yshift=-1.5ex] {$-4$}
 -- (-3,0) node[] {$|$}
 -- (-3,0) node[anchor=north, yshift=-1.5ex] {$-3$}
 -- (-2,0) node[] {$|$}
 -- (-2,0) node[anchor=north, yshift=-1.5ex] {$-2$}
 -- (-1,0) node[] {$|$}
 -- (-1,0) node[anchor=north, yshift=-1.5ex] {$-1$}
 --	(0,0) node[] {$|$}
 -- (0,0) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (1,0) node[] {$|$}
 -- (1,0) node[anchor=north, yshift=-1.5ex] {$1$}
 -- (2,0) node[] {$|$}
 -- (2,0) node[anchor=north, yshift=-1.5ex] {$2$}
 -- (3,0) node[] {$|$}
 -- (3,0) node[anchor=north, yshift=-1.5ex] {$3$}
 -- (4,0) node[] {$|$}
 -- (4,0) node[anchor=north, yshift=-1.5ex] {$4$}
 -- (5,0) node[] {$|$}
 -- (5,0) node[anchor=north, yshift=-1.5ex] {$5$}
 -- (6,0) node[] {$|$}
 -- (6,0) node[anchor=north, yshift=-1.5ex] {$6$}
 -- (7,0) node[] {$|$}
 -- (7,0) node[anchor=north, yshift=-1.5ex] {$7$}
 -- (8,0) node[] {$|$}
 -- (8,0) node[anchor=north, yshift=-1.5ex] {$8$};
\draw[-, ultra thick, opacity=0.3] (-8,0)  -- (-8,1);
\filldraw[color=black, opacity=0.3] (-8,1) circle (.1);
\draw[-, ultra thick, opacity=0.3] (-7,0)  -- (-7,1);
\filldraw[color=black, opacity=0.3] (-7,1) circle (.1);
\filldraw[color=black, opacity=0.3] (-6,0) circle (.1);
\filldraw[color=black, opacity=0.3] (-5,0) circle (.1);
\filldraw[color=black, opacity=0.3] (-4,0) circle (.1);
\draw[-, ultra thick, opacity=0.3] (-3,0)  -- (-3,1);
\filldraw[color=black, opacity=0.3] (-3,1) circle (.1);
\draw[-, ultra thick, opacity=0.3] (-2,0)  -- (-2,1);
\filldraw[color=black, opacity=0.3] (-2,1) circle (.1);
\draw[-, ultra thick, opacity=0.3] (-1,0)  -- (-1,1);
\filldraw[color=black, opacity=0.3] (-1,1) circle (.1);
\filldraw[color=black] (0,0) circle (.1);
\filldraw[color=black] (1,0) circle (.1);
\filldraw[color=black] (2,0) circle (.1);
\draw[-, ultra thick] (3,0)  -- (3,1);
\filldraw[color=black] (3,1) circle (.1);
\draw[-, ultra thick] (4,0)  -- (4,1);
\filldraw[color=black] (4,1) circle (.1);
\draw[-, ultra thick] (5,0)  -- (5,1);
\filldraw[color=black] (5,1) circle (.1);
\filldraw[color=black, opacity=0.3] (6,0) circle (.1);
\filldraw[color=black, opacity=0.3] (7,0) circle (.1);
\filldraw[color=black, opacity=0.3] (8,0) circle (.1);

\end{tikzpicture}%\caption{}
\end{figure}

  Another version of what $f[-n]$ might be the reflection (or 180 degree rotation) of the data/signal about the vertical axis where the $n$-index equals zero
  (But this is effectively the same as the signal itself)

\begin{figure}[H]
\centering
\begin{tikzpicture}
% Titles
\node[anchor=south] (note) at (0, 1.5) {$f[-n]$?};

% f[-n] version 1
\draw[-, thick]
    (-8,0) node[] {$|$}
 -- (-8,0) node[anchor=north, yshift=-1.5ex] {$-8$}
 -- (-7,0) node[] {$|$}
 -- (-7,0) node[anchor=north, yshift=-1.5ex] {$-7$}
 -- (-6,0) node[] {$|$}
 -- (-6,0) node[anchor=north, yshift=-1.5ex] {$-6$}
 -- (-5,0) node[] {$|$}
 -- (-5,0) node[anchor=north, yshift=-1.5ex] {$-5$}
 -- (-4,0) node[] {$|$}
 -- (-4,0) node[anchor=north, yshift=-1.5ex] {$-4$}
 -- (-3,0) node[] {$|$}
 -- (-3,0) node[anchor=north, yshift=-1.5ex] {$-3$}
 -- (-2,0) node[] {$|$}
 -- (-2,0) node[anchor=north, yshift=-1.5ex] {$-2$}
 -- (-1,0) node[] {$|$}
 -- (-1,0) node[anchor=north, yshift=-1.5ex] {$-1$}
 --	(0,0) node[] {$|$}
 -- (0,0) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (1,0) node[] {$|$}
 -- (1,0) node[anchor=north, yshift=-1.5ex] {$1$}
 -- (2,0) node[] {$|$}
 -- (2,0) node[anchor=north, yshift=-1.5ex] {$2$}
 -- (3,0) node[] {$|$}
 -- (3,0) node[anchor=north, yshift=-1.5ex] {$3$}
 -- (4,0) node[] {$|$}
 -- (4,0) node[anchor=north, yshift=-1.5ex] {$4$}
 -- (5,0) node[] {$|$}
 -- (5,0) node[anchor=north, yshift=-1.5ex] {$5$}
 -- (6,0) node[] {$|$}
 -- (6,0) node[anchor=north, yshift=-1.5ex] {$6$}
 -- (7,0) node[] {$|$}
 -- (7,0) node[anchor=north, yshift=-1.5ex] {$7$}
 -- (8,0) node[] {$|$}
 -- (8,0) node[anchor=north, yshift=-1.5ex] {$8$};
\draw[-, ultra thick, opacity=0.3] (-8,0)  -- (-8,1);
\filldraw[color=black, opacity=0.3] (-8,1) circle (.1);
\draw[-, ultra thick, opacity=0.3] (-7,0)  -- (-7,1);
\filldraw[color=black, opacity=0.3] (-7,1) circle (.1);
\draw[-, ultra thick, opacity=0.3] (-6,0)  -- (-6,1);
\filldraw[color=black, opacity=0.3] (-6,1) circle (.1);
\filldraw[color=black] (-5,0) circle (.1);
\filldraw[color=black] (-4,0) circle (.1);
\filldraw[color=black] (-3,0) circle (.1);
\draw[-, ultra thick] (-2,0)  -- (-2,1);
\filldraw[color=black] (-2,1) circle (.1);
\draw[-, ultra thick] (-1,0)  -- (-1,1);
\filldraw[color=black] (-1,1) circle (.1);
\draw[-, ultra thick] (0,0)  -- (0,1);
\filldraw[color=black] (0,1) circle (.1);
\filldraw[color=black, opacity=0.3] (1,0) circle (.1);
\filldraw[color=black, opacity=0.3] (2,0) circle (.1);
\filldraw[color=black, opacity=0.3] (3,0) circle (.1);
\draw[-, ultra thick, opacity=0.3] (4,0)  -- (4,1);
\filldraw[color=black, opacity=0.3] (4,1) circle (.1);
\draw[-, ultra thick, opacity=0.3] (5,0)  -- (5,1);
\filldraw[color=black, opacity=0.3] (5,1) circle (.1);
\draw[-, ultra thick, opacity=0.3] (6,0)  -- (6,1);
\filldraw[color=black, opacity=0.3] (6,1) circle (.1);
\filldraw[color=black, opacity=0.3] (7,0) circle (.1);
\filldraw[color=black, opacity=0.3] (8,0) circle (.1);

\end{tikzpicture}%\caption{}
\end{figure}

  Another way to construct $f[-n]$ might be to preserve $f[n]$ in its original form but count the samples in the opposite direction (from right to left) starting from zero utilizing the periodic extension of the signal
  This is the same as reflecting the signal about the vertical axis at $n=0$ and recording the samples from left to right (do figure: MOVE THIS DOWN)

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
% Titles
\node[anchor=south] (note) at (0, 5.5) {$f[-n]$};

% [n] axis
\draw[<->, thick]
    (-7,4) node[anchor=south, yshift=1.5ex] {$\cdots$}
 -- (-6,4) node[] {$|$}
 -- (-6,4) node[anchor=north, yshift=-1.5ex] {$-6$}
 -- (-5,4) node[] {$|$}
 -- (-5,4) node[anchor=north, yshift=-1.5ex] {$-5$}
 -- (-4,4) node[] {$|$}
 -- (-4,4) node[anchor=north, yshift=-1.5ex] {$-4$}
 -- (-3,4) node[] {$|$}
 -- (-3,4) node[anchor=north, yshift=-1.5ex] {$-3$}
 -- (-2,4) node[] {$|$}
 -- (-2,4) node[anchor=north, yshift=-1.5ex] {$-2$}
 -- (-1,4) node[] {$|$}
 -- (-1,4) node[anchor=north, yshift=-1.5ex] {$-1$}
 --	(0,4) node[] {$|$}
 -- (0,4) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (1,4) node[] {$|$}
 -- (1,4) node[anchor=north, yshift=-1.5ex] {$1$}
 -- (2,4) node[] {$|$}
 -- (2,4) node[anchor=north, yshift=-1.5ex] {$2$}
 -- (3,4) node[] {$|$}
 -- (3,4) node[anchor=north, yshift=-1.5ex] {$3$}
 -- (4,4) node[] {$|$}
 -- (4,4) node[anchor=north, yshift=-1.5ex] {$4$}
 -- (5,4) node[] {$|$}
 -- (5,4) node[anchor=north, yshift=-1.5ex] {$5$}
 -- (6,4) node[] {$|$}
 -- (6,4) node[anchor=north, yshift=-1.5ex] {$6$}
 -- (7,4) node[] {$|$}
 -- (7,4) node[anchor=north, yshift=-1.5ex] {$7$}
 -- (8,4) node[] {$|$}
 -- (8,4) node[anchor=north, yshift=-1.5ex] {$8$}
 -- (9,4) node[anchor=south, yshift=1.5ex] {$\cdots$};
\draw[-, ultra thick, opacity=0.3] (-6,4)  -- (-6,5);
\filldraw[color=black, opacity=0.3] (-6,5) circle (.1);
\draw[-, ultra thick] (-5,4)  -- (-5,5);
\filldraw[color=black] (-5,5) circle (.1);
\draw[-, ultra thick] (-4,4)  -- (-4,5);
\filldraw[color=black] (-4,5) circle (.1);
\filldraw[color=black] (-3,4) circle (.1);
\filldraw[color=black] (-2,4) circle (.1);
\filldraw[color=black] (-1,4) circle (.1);
\draw[-, ultra thick] (0,4)  -- (0,5);
\filldraw[color=black] (0,5) circle (.1);
\draw[-, ultra thick, opacity=0.3] (1,4)  -- (1,5);
\filldraw[color=black, opacity=0.3] (1,5) circle (.1);
\draw[-, ultra thick, opacity=0.3] (2,4)  -- (2,5);
\filldraw[color=black, opacity=0.3] (2,5) circle (.1);
\filldraw[color=black, opacity=0.3] (3,4) circle (.1);
\filldraw[color=black, opacity=0.3] (4,4) circle (.1);
\filldraw[color=black, opacity=0.3] (5,4) circle (.1);
\draw[-, ultra thick, opacity=0.3] (6,4)  -- (6,5);
\filldraw[color=black, opacity=0.3, fill opacity=0.3] (6,5) circle (.1);
\draw[-, ultra thick, opacity=0.3] (7,4)  -- (7,5);
\filldraw[color=black, opacity=0.3, fill opacity=0.3] (7,5) circle (.1);
\draw[-, ultra thick, opacity=0.3] (8,4)  -- (8,5);
\filldraw[color=black, opacity=0.3, fill opacity=0.3] (8,5) circle (.1);

\end{tikzpicture}%\caption{}
\end{center}
\end{figure}

  (The goal is ``no pedagogy'' (ie: make everything seem a coherent progression, such that there is little need for providing motivation for definition''...What is the right check/comparison?)
  The possibilities are not endless.
  Which one is correct?
  Let's proceed by looking at the simplest example possible $f * f$
  ... It would be nice to be clear about what $m$ is here...
  By default, $m$ refers to the same axis as $n$, such that $f[n] g[m-n]$ is intrinsically meaningful - and, $m$ is therefore an alias for what is commonly referred to as $n$, the $n$-index is associated with distance in time or in space.

  note: signals do not even have to be the same length to perform convolution\cite{Karu}(amazing ref on discrete convolution p.49)	

  (No idea why this image is not located correctly...)
  Computing the convolution requires a definition for $f[m-n]$.
  The approach shown here is an index-centric approach in that all of the manipulations to compute the convolution involve the index exclusively, the signal is untouched and remains intact.
  The periodic extension is invoked to include negative indices.
  Signal samples associated with negative indices can be counted in the negative direction from right to left.
  The \underline{reverse signal}, $f[-n]$, is constructed from $f[n=0]$ and using the periodic extension to proceed sampling along negative indices.
  One complete cycle of the reverse signal, $f[-n]$, extends from $f[n=0]$ to $f[n=-(N-1)]$ and proceeds in the negative direction along indices $n=0, -1, -2,\ldots,-(N-1)$.

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
% f[n] axis
\node[anchor=south] (note) at (-10, 0.5) {$f[n]$};
\draw[-, thick]
    (-9,0) node[] {$|$}
 -- (-9,0) node[anchor=north, yshift=-1.5ex] {$-5$}
 -- (-8,0) node[] {$|$}
 -- (-8,0) node[anchor=north, yshift=-1.5ex] {$-4$}
 -- (-7,0) node[] {$|$}
 -- (-7,0) node[anchor=north, yshift=-1.5ex] {$-3$}
 -- (-6,0) node[] {$|$}
 -- (-6,0) node[anchor=north, yshift=-1.5ex] {$-2$}
 -- (-5,0) node[] {$|$}
 -- (-5,0) node[anchor=north, yshift=-1.5ex] {$-1$}
 --	(-4,0) node[] {$|$}
 -- (-4,0) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (-3,0) node[] {$|$}
 -- (-3,0) node[anchor=north, yshift=-1.5ex] {$1$}
 -- (-2,0) node[] {$|$}
 -- (-2,0) node[anchor=north, yshift=-1.5ex] {$2$}
 -- (-1,0) node[] {$|$}
 -- (-1,0) node[anchor=north, yshift=-1.5ex] {$3$}
 -- (0,0) node[] {$|$}
 -- (0,0) node[anchor=north, yshift=-1.5ex] {$4$}
 -- (1,0) node[] {$|$}
 -- (1,0) node[anchor=north, yshift=-1.5ex] {$5$};
\draw[-, ultra thick, opacity=0.3] (-9,0)  -- (-9,1);
\filldraw[color=black, opacity=0.3] (-9,1) circle (.1);
\draw[-, ultra thick, opacity=0.3] (-8,0)  -- (-8,1);
\filldraw[color=black, opacity=0.3] (-8,1) circle (.1);
\filldraw[color=black, opacity=0.3] (-7,0) circle (.1);
\filldraw[color=black, opacity=0.3] (-6,0) circle (.1);
\filldraw[color=black, opacity=0.3] (-5,0) circle (.1);
\draw[-, ultra thick] (-4,0)  -- (-4,1);
\filldraw[color=black] (-4,1) circle (.1);
\draw[-, ultra thick] (-3,0)  -- (-3,1);
\filldraw[color=black] (-3,1) circle (.1);
\draw[-, ultra thick] (-2,0)  -- (-2,1);
\filldraw[color=black] (-2,1) circle (.1);
\filldraw[color=black] (-1,0) circle (.1);
\filldraw[color=black] (0,0) circle (.1);
\filldraw[color=black] (1,0) circle (.1);
% arrows showing the direction of progress of f[n] along summation 
\draw[->, thick] (-3.9,1.3) to[out=45, in=135] (-3.1,1.3);
\draw[->, thick] (-2.9,1.3) to[out=45, in=135] (-2.1,1.3);
\draw[->, thick] (-1.9,1.3) to[out=45, in=95] (-1.1,0.3);
\draw[->, thick] (-0.9,0.3) to[out=45, in=135] (-0.1,0.3);
\draw[->, thick] (0.2,0.3) to[out=45, in=135] (0.9,0.3);
% (f * f)[m=0]
\node[anchor=south] (note) at (3.75, 1.0) {$(f*f)[m] = $};
\node[anchor=south] (note) at (4, 0.3) {$\sum_n f[n] \, f[m-n]$};
\end{tikzpicture}%\caption{}
\end{center}
\end{figure}

\vspace{-1cm}

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
% f[-n] axis
\node[anchor=south] (note) at (-10, 0.5) {$f[-n]$};
\draw[-, thick]
    (-9,0) node[] {$|$}
 -- (-9,0) node[anchor=north, yshift=-1.5ex] {$-5$}
 -- (-8,0) node[] {$|$}
 -- (-8,0) node[anchor=north, yshift=-1.5ex] {$-4$}
 -- (-7,0) node[] {$|$}
 -- (-7,0) node[anchor=north, yshift=-1.5ex] {$-3$}
 -- (-6,0) node[] {$|$}
 -- (-6,0) node[anchor=north, yshift=-1.5ex] {$-2$}
 -- (-5,0) node[] {$|$}
 -- (-5,0) node[anchor=north, yshift=-1.5ex] {$-1$}
 --	(-4,0) node[] {$|$}
 -- (-4,0) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (-3,0) node[] {$|$}
 -- (-3,0) node[anchor=north, yshift=-1.5ex] {$1$}
 -- (-2,0) node[] {$|$}
 -- (-2,0) node[anchor=north, yshift=-1.5ex] {$2$}
 -- (-1,0) node[] {$|$}
 -- (-1,0) node[anchor=north, yshift=-1.5ex] {$3$}
 -- (0,0) node[] {$|$}
 -- (0,0) node[anchor=north, yshift=-1.5ex] {$4$}
 -- (1,0) node[] {$|$}
 -- (1,0) node[anchor=north, yshift=-1.5ex] {$5$};
% f[-n] samples
\draw[-, ultra thick] (-9,0)  -- (-9,1);
\filldraw[color=black] (-9,1) circle (.1);
\draw[-, ultra thick] (-8,0)  -- (-8,1);
\filldraw[color=black] (-8,1) circle (.1);
\filldraw[color=black] (-7,0) circle (.1);
\filldraw[color=black] (-6,0) circle (.1);
\filldraw[color=black] (-5,0) circle (.1);
\draw[-, ultra thick] (-4,0)  -- (-4,1);
\filldraw[color=black] (-4,1) circle (.1);
\draw[-, ultra thick, opacity=0.3] (-3,0)  -- (-3,1);
\filldraw[color=black, opacity=0.3] (-3,1) circle (.1);
\draw[-, ultra thick, opacity=0.3] (-2,0)  -- (-2,1);
\filldraw[color=black, opacity=0.3] (-2,1) circle (.1);
\filldraw[color=black, opacity=0.3] (-1,0) circle (.1);
\filldraw[color=black, opacity=0.3] (0,0) circle (.1);
\filldraw[color=black, opacity=0.3] (1,0) circle (.1);
% arrows showing the direction of progress of f[-n] along summation 
\draw[->, thick] (-4.1,1.3) to[out=135, in=90] (-4.9,0.3);
\draw[->, thick] (-5.1,0.3) to[out=135, in=45] (-5.9,0.3);
\draw[->, thick] (-6.1,0.3) to[out=135, in=45] (-6.9,0.3);
\draw[->, thick] (-7.1,0.3) to[out=90, in=45] (-7.9,1.3);
\draw[->, thick] (-8.1,1.3) to[out=135, in=45] (-8.9,1.3);
% (f * f)[m=0]
\node[anchor=south] (note) at (4, 0.5) {$\Longrightarrow (f*f)[0] = 1$};
\end{tikzpicture}%\caption{}
\end{center}
\end{figure}

\vspace{-1cm}% reduce vspace between figures to simplify image coordinates for multiple images 

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
% f[1-n] axis
\node[anchor=south] (note) at (-10, 0.5) {$f[1-n]$};
\draw[-, thick]
    (-9,0) node[] {$|$}
 -- (-9,0) node[anchor=north, yshift=-1.5ex] {$-5$}
 -- (-8,0) node[] {$|$}
 -- (-8,0) node[anchor=north, yshift=-1.5ex] {$-4$}
 -- (-7,0) node[] {$|$}
 -- (-7,0) node[anchor=north, yshift=-1.5ex] {$-3$}
 -- (-6,0) node[] {$|$}
 -- (-6,0) node[anchor=north, yshift=-1.5ex] {$-2$}
 -- (-5,0) node[] {$|$}
 -- (-5,0) node[anchor=north, yshift=-1.5ex] {$-1$}
 --	(-4,0) node[] {$|$}
 -- (-4,0) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (-3,0) node[] {$|$}
 -- (-3,0) node[anchor=north, yshift=-1.5ex] {$1$}
 -- (-2,0) node[] {$|$}
 -- (-2,0) node[anchor=north, yshift=-1.5ex] {$2$}
 -- (-1,0) node[] {$|$}
 -- (-1,0) node[anchor=north, yshift=-1.5ex] {$3$}
 -- (0,0) node[] {$|$}
 -- (0,0) node[anchor=north, yshift=-1.5ex] {$4$}
 -- (1,0) node[] {$|$}
 -- (1,0) node[anchor=north, yshift=-1.5ex] {$5$};
% f[-n] samples
\draw[-, ultra thick, opacity=0.3] (-9,0)  -- (-9,1);
\filldraw[color=black, opacity=0.3] (-9,1) circle (.1);
\draw[-, ultra thick] (-8,0)  -- (-8,1);
\filldraw[color=black] (-8,1) circle (.1);
\filldraw[color=black] (-7,0) circle (.1);
\filldraw[color=black] (-6,0) circle (.1);
\filldraw[color=black] (-5,0) circle (.1);
\draw[-, ultra thick] (-4,0)  -- (-4,1);
\filldraw[color=black] (-4,1) circle (.1);
\draw[-, ultra thick] (-3,0)  -- (-3,1);
\filldraw[color=black] (-3,1) circle (.1);
\draw[-, ultra thick, opacity=0.3] (-2,0)  -- (-2,1);
\filldraw[color=black, opacity=0.3] (-2,1) circle (.1);
\filldraw[color=black, opacity=0.3] (-1,0) circle (.1);
\filldraw[color=black, opacity=0.3] (0,0) circle (.1);
\filldraw[color=black, opacity=0.3] (1,0) circle (.1);
% arrows showing the direction of progress of f[-n] along summation 
\draw[->, thick] (-3.1,1.3) to[out=135, in=45] (-3.9,1.3);
\draw[->, thick] (-4.1,1.3) to[out=135, in=90] (-4.9,0.3);
\draw[->, thick] (-5.1,0.3) to[out=135, in=45] (-5.9,0.3);
\draw[->, thick] (-6.1,0.3) to[out=135, in=45] (-6.9,0.3);
\draw[->, thick] (-7.1,0.3) to[out=90, in=45] (-7.9,1.3);
% (f * f)[m=1]
\node[anchor=south] (note) at (4, 0.5) {$\Longrightarrow (f*f)[1] = 2$};
\end{tikzpicture}%\caption{}
\end{center}
\end{figure}

\vspace{-1cm}

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
% f[2-n] axis
\node[anchor=south] (note) at (-10, 0.5) {$f[2-n]$};
\draw[-, thick]
    (-9,0) node[] {$|$}
 -- (-9,0) node[anchor=north, yshift=-1.5ex] {$-5$}
 -- (-8,0) node[] {$|$}
 -- (-8,0) node[anchor=north, yshift=-1.5ex] {$-4$}
 -- (-7,0) node[] {$|$}
 -- (-7,0) node[anchor=north, yshift=-1.5ex] {$-3$}
 -- (-6,0) node[] {$|$}
 -- (-6,0) node[anchor=north, yshift=-1.5ex] {$-2$}
 -- (-5,0) node[] {$|$}
 -- (-5,0) node[anchor=north, yshift=-1.5ex] {$-1$}
 --	(-4,0) node[] {$|$}
 -- (-4,0) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (-3,0) node[] {$|$}
 -- (-3,0) node[anchor=north, yshift=-1.5ex] {$1$}
 -- (-2,0) node[] {$|$}
 -- (-2,0) node[anchor=north, yshift=-1.5ex] {$2$}
 -- (-1,0) node[] {$|$}
 -- (-1,0) node[anchor=north, yshift=-1.5ex] {$3$}
 -- (0,0) node[] {$|$}
 -- (0,0) node[anchor=north, yshift=-1.5ex] {$4$}
 -- (1,0) node[] {$|$}
 -- (1,0) node[anchor=north, yshift=-1.5ex] {$5$};
% f[2-n] samples
\draw[-, ultra thick, opacity=0.3] (-9,0)  -- (-9,1);
\filldraw[color=black, opacity=0.3] (-9,1) circle (.1);
\draw[-, ultra thick, opacity=0.3] (-8,0)  -- (-8,1);
\filldraw[color=black, opacity=0.3] (-8,1) circle (.1);
\filldraw[color=black] (-7,0) circle (.1);
\filldraw[color=black] (-6,0) circle (.1);
\filldraw[color=black] (-5,0) circle (.1);
\draw[-, ultra thick] (-4,0)  -- (-4,1);
\filldraw[color=black] (-4,1) circle (.1);
\draw[-, ultra thick] (-3,0)  -- (-3,1);
\filldraw[color=black] (-3,1) circle (.1);
\draw[-, ultra thick] (-2,0)  -- (-2,1);
\filldraw[color=black] (-2,1) circle (.1);
\filldraw[color=black, opacity=0.3] (-1,0) circle (.1);
\filldraw[color=black, opacity=0.3] (0,0) circle (.1);
\filldraw[color=black, opacity=0.3] (1,0) circle (.1);
% arrows showing the direction of progress of f[2-n] along summation 
\draw[->, thick] (-2.1,1.3) to[out=135, in=45] (-2.9,1.3);
\draw[->, thick] (-3.1,1.3) to[out=135, in=45] (-3.9,1.3);
\draw[->, thick] (-4.1,1.3) to[out=135, in=90] (-4.9,0.3);
\draw[->, thick] (-5.1,0.3) to[out=135, in=45] (-5.9,0.3);
\draw[->, thick] (-6.1,0.3) to[out=135, in=45] (-6.9,0.3);
% (f * f)[m=2]
\node[anchor=south] (note) at (4, 0.5) {$\Longrightarrow (f*f)[2] = 3$};
\end{tikzpicture}%\caption{}
\end{center}
\end{figure}

\vspace{-1cm}

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
% f[3-n] axis
\node[anchor=south] (note) at (-10, 0.5) {$f[3-n]$};
\draw[-, thick]
    (-9,0) node[] {$|$}
 -- (-9,0) node[anchor=north, yshift=-1.5ex] {$-5$}
 -- (-8,0) node[] {$|$}
 -- (-8,0) node[anchor=north, yshift=-1.5ex] {$-4$}
 -- (-7,0) node[] {$|$}
 -- (-7,0) node[anchor=north, yshift=-1.5ex] {$-3$}
 -- (-6,0) node[] {$|$}
 -- (-6,0) node[anchor=north, yshift=-1.5ex] {$-2$}
 -- (-5,0) node[] {$|$}
 -- (-5,0) node[anchor=north, yshift=-1.5ex] {$-1$}
 --	(-4,0) node[] {$|$}
 -- (-4,0) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (-3,0) node[] {$|$}
 -- (-3,0) node[anchor=north, yshift=-1.5ex] {$1$}
 -- (-2,0) node[] {$|$}
 -- (-2,0) node[anchor=north, yshift=-1.5ex] {$2$}
 -- (-1,0) node[] {$|$}
 -- (-1,0) node[anchor=north, yshift=-1.5ex] {$3$}
 -- (0,0) node[] {$|$}
 -- (0,0) node[anchor=north, yshift=-1.5ex] {$4$}
 -- (1,0) node[] {$|$}
 -- (1,0) node[anchor=north, yshift=-1.5ex] {$5$};
% f[3-n] samples
\draw[-, ultra thick, opacity=0.3] (-9,0)  -- (-9,1);
\filldraw[color=black, opacity=0.3] (-9,1) circle (.1);
\draw[-, ultra thick, opacity=0.3] (-8,0)  -- (-8,1);
\filldraw[color=black, opacity=0.3] (-8,1) circle (.1);
\filldraw[color=black, opacity=0.3] (-7,0) circle (.1);
\filldraw[color=black] (-6,0) circle (.1);
\filldraw[color=black] (-5,0) circle (.1);
\draw[-, ultra thick] (-4,0)  -- (-4,1);
\filldraw[color=black] (-4,1) circle (.1);
\draw[-, ultra thick] (-3,0)  -- (-3,1);
\filldraw[color=black] (-3,1) circle (.1);
\draw[-, ultra thick] (-2,0)  -- (-2,1);
\filldraw[color=black] (-2,1) circle (.1);
\filldraw[color=black] (-1,0) circle (.1);
\filldraw[color=black, opacity=0.3] (0,0) circle (.1);
\filldraw[color=black, opacity=0.3] (1,0) circle (.1);
% arrows showing the direction of progress of f[3-n] along summation 
\draw[->, thick] (-1.1,0.3) to[out=90, in=45] (-1.9,1.3);
\draw[->, thick] (-2.1,1.3) to[out=135, in=45] (-2.9,1.3);
\draw[->, thick] (-3.1,1.3) to[out=135, in=45] (-3.9,1.3);
\draw[->, thick] (-4.1,1.3) to[out=135, in=90] (-4.9,0.3);
\draw[->, thick] (-5.1,0.3) to[out=135, in=45] (-5.9,0.3);
% (f * f)[m=3]
\node[anchor=south] (note) at (4, 0.5) {$\Longrightarrow (f*f)[3] = 2$};
\end{tikzpicture}%\caption{}
\end{center}
\end{figure}

\vspace{-1cm}

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
% f[4-n] axis
\node[anchor=south] (note) at (-10, 0.5) {$f[4-n]$};
\draw[-, thick]
    (-9,0) node[] {$|$}
 -- (-9,0) node[anchor=north, yshift=-1.5ex] {$-5$}
 -- (-8,0) node[] {$|$}
 -- (-8,0) node[anchor=north, yshift=-1.5ex] {$-4$}
 -- (-7,0) node[] {$|$}
 -- (-7,0) node[anchor=north, yshift=-1.5ex] {$-3$}
 -- (-6,0) node[] {$|$}
 -- (-6,0) node[anchor=north, yshift=-1.5ex] {$-2$}
 -- (-5,0) node[] {$|$}
 -- (-5,0) node[anchor=north, yshift=-1.5ex] {$-1$}
 --	(-4,0) node[] {$|$}
 -- (-4,0) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (-3,0) node[] {$|$}
 -- (-3,0) node[anchor=north, yshift=-1.5ex] {$1$}
 -- (-2,0) node[] {$|$}
 -- (-2,0) node[anchor=north, yshift=-1.5ex] {$2$}
 -- (-1,0) node[] {$|$}
 -- (-1,0) node[anchor=north, yshift=-1.5ex] {$3$}
 -- (0,0) node[] {$|$}
 -- (0,0) node[anchor=north, yshift=-1.5ex] {$4$}
 -- (1,0) node[] {$|$}
 -- (1,0) node[anchor=north, yshift=-1.5ex] {$5$};
% f[4-n] samples
\draw[-, ultra thick, opacity=0.3] (-9,0)  -- (-9,1);
\filldraw[color=black, opacity=0.3] (-9,1) circle (.1);
\draw[-, ultra thick, opacity=0.3] (-8,0)  -- (-8,1);
\filldraw[color=black, opacity=0.3] (-8,1) circle (.1);
\filldraw[color=black, opacity=0.3] (-7,0) circle (.1);
\filldraw[color=black, opacity=0.3] (-6,0) circle (.1);
\filldraw[color=black] (-5,0) circle (.1);
\draw[-, ultra thick] (-4,0)  -- (-4,1);
\filldraw[color=black] (-4,1) circle (.1);
\draw[-, ultra thick] (-3,0)  -- (-3,1);
\filldraw[color=black] (-3,1) circle (.1);
\draw[-, ultra thick] (-2,0)  -- (-2,1);
\filldraw[color=black] (-2,1) circle (.1);
\filldraw[color=black] (-1,0) circle (.1);
\filldraw[color=black] (0,0) circle (.1);
\filldraw[color=black, opacity=0.3] (1,0) circle (.1);
% arrows showing the direction of progress of f[4-n] along summation
\draw[->, thick] (-0.1,0.3) to[out=135, in=45] (-0.9,0.3);
\draw[->, thick] (-1.1,0.3) to[out=90, in=45] (-1.9,1.3);
\draw[->, thick] (-2.1,1.3) to[out=135, in=45] (-2.9,1.3);
\draw[->, thick] (-3.1,1.3) to[out=135, in=45] (-3.9,1.3);
\draw[->, thick] (-4.1,1.3) to[out=135, in=90] (-4.9,0.3);
% (f * f)[m=4]
\node[anchor=south] (note) at (4, 0.5) {$\Longrightarrow (f*f)[4] = 1$};
\end{tikzpicture}%\caption{}
\end{center}
\end{figure}

\vspace{-1cm}

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
% f[5-n] axis
\node[anchor=south] (note) at (-10, 0.5) {$f[5-n]$};
\draw[-, thick]
    (-9,0) node[] {$|$}
 -- (-9,0) node[anchor=north, yshift=-1.5ex] {$-5$}
 -- (-8,0) node[] {$|$}
 -- (-8,0) node[anchor=north, yshift=-1.5ex] {$-4$}
 -- (-7,0) node[] {$|$}
 -- (-7,0) node[anchor=north, yshift=-1.5ex] {$-3$}
 -- (-6,0) node[] {$|$}
 -- (-6,0) node[anchor=north, yshift=-1.5ex] {$-2$}
 -- (-5,0) node[] {$|$}
 -- (-5,0) node[anchor=north, yshift=-1.5ex] {$-1$}
 --	(-4,0) node[] {$|$}
 -- (-4,0) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (-3,0) node[] {$|$}
 -- (-3,0) node[anchor=north, yshift=-1.5ex] {$1$}
 -- (-2,0) node[] {$|$}
 -- (-2,0) node[anchor=north, yshift=-1.5ex] {$2$}
 -- (-1,0) node[] {$|$}
 -- (-1,0) node[anchor=north, yshift=-1.5ex] {$3$}
 -- (0,0) node[] {$|$}
 -- (0,0) node[anchor=north, yshift=-1.5ex] {$4$}
 -- (1,0) node[] {$|$}
 -- (1,0) node[anchor=north, yshift=-1.5ex] {$5$};
% f[5-n] samples
\draw[-, ultra thick, opacity=0.3] (-9,0)  -- (-9,1);
\filldraw[color=black, opacity=0.3] (-9,1) circle (.1);
\draw[-, ultra thick, opacity=0.3] (-8,0)  -- (-8,1);
\filldraw[color=black, opacity=0.3] (-8,1) circle (.1);
\filldraw[color=black, opacity=0.3] (-7,0) circle (.1);
\filldraw[color=black, opacity=0.3] (-6,0) circle (.1);
\filldraw[color=black, opacity=0.3] (-5,0) circle (.1);
\draw[-, ultra thick] (-4,0)  -- (-4,1);
\filldraw[color=black] (-4,1) circle (.1);
\draw[-, ultra thick] (-3,0)  -- (-3,1);
\filldraw[color=black] (-3,1) circle (.1);
\draw[-, ultra thick] (-2,0)  -- (-2,1);
\filldraw[color=black] (-2,1) circle (.1);
\filldraw[color=black] (-1,0) circle (.1);
\filldraw[color=black] (0,0) circle (.1);
\filldraw[color=black] (1,0) circle (.1);
% arrows showing the direction of progress of f[5-n] along summation
\draw[->, thick] (0.9,0.3) to[out=135, in=45] (0.1,0.3);
\draw[->, thick] (-0.1,0.3) to[out=135, in=45] (-0.9,0.3);
\draw[->, thick] (-0.1,0.3) to[out=135, in=45] (-0.9,0.3);
\draw[->, thick] (-1.1,0.3) to[out=90, in=45] (-1.9,1.3);
\draw[->, thick] (-2.1,1.3) to[out=135, in=45] (-2.9,1.3);
\draw[->, thick] (-3.1,1.3) to[out=135, in=45] (-3.9,1.3);
% (f * f)[m=5]
\node[anchor=south] (note) at (4, 0.5) {$\Longrightarrow (f*f)[5] = 0$};
\end{tikzpicture}%\caption{}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
% (f*f)[m] axis
\node[anchor=south] (note) at (-4, 2.5) {$(f*f)[m]$};
\node[anchor=north] (note) at (-1.5, -1) {$[m]$};
\draw[-, thick]
   	(-4,0) node[] {$|$}
 -- (-4,0) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (-3,0) node[] {$|$}
 -- (-3,0) node[anchor=north, yshift=-1.5ex] {$1$}
 -- (-2,0) node[] {$|$}
 -- (-2,0) node[anchor=north, yshift=-1.5ex] {$2$}
 -- (-1,0) node[] {$|$}
 -- (-1,0) node[anchor=north, yshift=-1.5ex] {$3$}
 -- (0,0) node[] {$|$}
 -- (0,0) node[anchor=north, yshift=-1.5ex] {$4$}
 -- (1,0) node[] {$|$}
 -- (1,0) node[anchor=north, yshift=-1.5ex] {$5$};
% (f*f)[m] samples
\draw[-, ultra thick] (-4,0)  -- (-4,1);
\filldraw[color=black] (-4,1) circle (.1);
\draw[-, ultra thick] (-3,0)  -- (-3,2);
\filldraw[color=black] (-3,2) circle (.1);
\draw[-, ultra thick] (-2,0)  -- (-2,3);
\filldraw[color=black] (-2,3) circle (.1);
\draw[-, ultra thick] (-1,0)  -- (-1,2);
\filldraw[color=black] (-1,2) circle (.1);
\draw[-, ultra thick] (0,0)  -- (0,1);
\filldraw[color=black] (0,1) circle (.1);
\filldraw[color=black] (1,0) circle (.1);
\end{tikzpicture}%\caption{}
\end{center}
\end{figure}

  The other option...is a signal-centric approach.
  Reflect signal about $n=0$ and shift signal to the right by $m$ to generate $f[m-n]$.
  Take the signal from $n=0$ to $n=N-1$ and its periodic extension in the negative indices from $-(N-1)$ to $-1$
  The index $n$ ranges from $[0, \ldots, N-1]$

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
% f[n] axis
\node[anchor=south] (note) at (-10, 0.5) {$f[n]$};
\draw[-, thick]
    (-9,0) node[] {$|$}
 -- (-9,0) node[anchor=north, yshift=-1.5ex] {$-5$}
 -- (-8,0) node[] {$|$}
 -- (-8,0) node[anchor=north, yshift=-1.5ex] {$-4$}
 -- (-7,0) node[] {$|$}
 -- (-7,0) node[anchor=north, yshift=-1.5ex] {$-3$}
 -- (-6,0) node[] {$|$}
 -- (-6,0) node[anchor=north, yshift=-1.5ex] {$-2$}
 -- (-5,0) node[] {$|$}
 -- (-5,0) node[anchor=north, yshift=-1.5ex] {$-1$}
 --	(-4,0) node[] {$|$}
 -- (-4,0) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (-3,0) node[] {$|$}
 -- (-3,0) node[anchor=north, yshift=-1.5ex] {$1$}
 -- (-2,0) node[] {$|$}
 -- (-2,0) node[anchor=north, yshift=-1.5ex] {$2$}
 -- (-1,0) node[] {$|$}
 -- (-1,0) node[anchor=north, yshift=-1.5ex] {$3$}
 -- (0,0) node[] {$|$}
 -- (0,0) node[anchor=north, yshift=-1.5ex] {$4$}
 -- (1,0) node[] {$|$}
 -- (1,0) node[anchor=north, yshift=-1.5ex] {$5$};
\draw[-, ultra thick, opacity=0.3] (-9,0)  -- (-9,1);
\filldraw[color=black, opacity=0.3] (-9,1) circle (.1);
\draw[-, ultra thick, opacity=0.3] (-8,0)  -- (-8,1);
\filldraw[color=black, opacity=0.3] (-8,1) circle (.1);
\filldraw[color=black, opacity=0.3] (-7,0) circle (.1);
\filldraw[color=black, opacity=0.3] (-6,0) circle (.1);
\filldraw[color=black, opacity=0.3] (-5,0) circle (.1);
\draw[-, ultra thick] (-4,0)  -- (-4,1);
\filldraw[color=black] (-4,1) circle (.1);
\draw[-, ultra thick] (-3,0)  -- (-3,1);
\filldraw[color=black] (-3,1) circle (.1);
\draw[-, ultra thick] (-2,0)  -- (-2,1);
\filldraw[color=black] (-2,1) circle (.1);
\filldraw[color=black] (-1,0) circle (.1);
\filldraw[color=black] (0,0) circle (.1);
\filldraw[color=black] (1,0) circle (.1);
% arrows showing the direction of progress of f[n] along summation 
\draw[->, thick] (-3.9,1.3) to[out=45, in=135] (-3.1,1.3);
\draw[->, thick] (-2.9,1.3) to[out=45, in=135] (-2.1,1.3);
\draw[->, thick] (-1.9,1.3) to[out=45, in=95] (-1.1,0.3);
\draw[->, thick] (-0.9,0.3) to[out=45, in=135] (-0.1,0.3);
\draw[->, thick] (0.2,0.3) to[out=45, in=135] (0.9,0.3);
% (f * f)[m=0]
%\node[anchor=south] (note) at (3.75, 1.0) {$(f*f)[m] = $};
%\node[anchor=south] (note) at (4, 0.3) {$\sum_n f[n] \, f[m-n]$};
\end{tikzpicture}%\caption{}
\end{center}
\end{figure}

\vspace{-1cm}

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
% f[-n] axis
\node[anchor=south] (note) at (-10, 0.5) {$f[-n]$};
\draw[-, thick]
    (-9,0) node[] {$|$}
 -- (-9,0) node[anchor=north, yshift=-1.5ex] {$-5$}
 -- (-8,0) node[] {$|$}
 -- (-8,0) node[anchor=north, yshift=-1.5ex] {$-4$}
 -- (-7,0) node[] {$|$}
 -- (-7,0) node[anchor=north, yshift=-1.5ex] {$-3$}
 -- (-6,0) node[] {$|$}
 -- (-6,0) node[anchor=north, yshift=-1.5ex] {$-2$}
 -- (-5,0) node[] {$|$}
 -- (-5,0) node[anchor=north, yshift=-1.5ex] {$-1$}
 --	(-4,0) node[] {$|$}
 -- (-4,0) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (-3,0) node[] {$|$}
 -- (-3,0) node[anchor=north, yshift=-1.5ex] {$1$}
 -- (-2,0) node[] {$|$}
 -- (-2,0) node[anchor=north, yshift=-1.5ex] {$2$}
 -- (-1,0) node[] {$|$}
 -- (-1,0) node[anchor=north, yshift=-1.5ex] {$3$}
 -- (0,0) node[] {$|$}
 -- (0,0) node[anchor=north, yshift=-1.5ex] {$4$}
 -- (1,0) node[] {$|$}
 -- (1,0) node[anchor=north, yshift=-1.5ex] {$5$};
% f[-n] samples
\filldraw[color=black, opacity=0.3] (-9,0) circle (.1);
\filldraw[color=black, opacity=0.3] (-8,0) circle (.1);
\filldraw[color=black, opacity=0.3] (-7,0) circle (.1);
\draw[-, ultra thick, opacity=0.3] (-6,0)  -- (-6,1);
\filldraw[color=black, opacity=0.3] (-6,1) circle (.1);
\draw[-, ultra thick, opacity=0.3] (-5,0)  -- (-5,1);
\filldraw[color=black, opacity=0.3] (-5,1) circle (.1);
\draw[-, ultra thick] (-4,0)  -- (-4,1);
\filldraw[color=black] (-4,1) circle (.1);
\filldraw[color=black] (-3,0) circle (.1);
\filldraw[color=black] (-2,0) circle (.1);
\filldraw[color=black] (-1,0) circle (.1);
\draw[-, ultra thick] (0,0)  -- (0,1);
\filldraw[color=black] (0,1) circle (.1);
\draw[-, ultra thick] (1,0)  -- (1,1);
\filldraw[color=black] (1,1) circle (.1);
% arrows showing the direction of progress of f[-n] along summation 
\draw[->, thick] (-3.9,1.3) to[out=45, in=90] (-3.1,0.3);
\draw[->, thick] (-2.9,0.3) to[out=45, in=135] (-2.1,0.3);
\draw[->, thick] (-1.9,0.3) to[out=45, in=135] (-1.1,0.3);
\draw[->, thick] (-0.9,0.3) to[out=90, in=135] (-0.1,1.3);
\draw[->, thick] (0.1,1.3) to[out=45, in=135] (0.9,1.3);
\end{tikzpicture}%\caption{}
\end{center}
\end{figure}

\vspace{-1cm}

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
% f[-n] axis
\node[anchor=south] (note) at (-10, 0.5) {$f[1-n]$};
\draw[-, thick]
    (-9,0) node[] {$|$}
 -- (-9,0) node[anchor=north, yshift=-1.5ex] {$-5$}
 -- (-8,0) node[] {$|$}
 -- (-8,0) node[anchor=north, yshift=-1.5ex] {$-4$}
 -- (-7,0) node[] {$|$}
 -- (-7,0) node[anchor=north, yshift=-1.5ex] {$-3$}
 -- (-6,0) node[] {$|$}
 -- (-6,0) node[anchor=north, yshift=-1.5ex] {$-2$}
 -- (-5,0) node[] {$|$}
 -- (-5,0) node[anchor=north, yshift=-1.5ex] {$-1$}
 --	(-4,0) node[] {$|$}
 -- (-4,0) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (-3,0) node[] {$|$}
 -- (-3,0) node[anchor=north, yshift=-1.5ex] {$1$}
 -- (-2,0) node[] {$|$}
 -- (-2,0) node[anchor=north, yshift=-1.5ex] {$2$}
 -- (-1,0) node[] {$|$}
 -- (-1,0) node[anchor=north, yshift=-1.5ex] {$3$}
 -- (0,0) node[] {$|$}
 -- (0,0) node[anchor=north, yshift=-1.5ex] {$4$}
 -- (1,0) node[] {$|$}
 -- (1,0) node[anchor=north, yshift=-1.5ex] {$5$};
% f[1-n] samples
\draw[-, ultra thick, opacity=0.3] (-9,0)  -- (-9,1);
\filldraw[color=black, opacity=0.3] (-9,1) circle (.1);
\filldraw[color=black, opacity=0.3] (-8,0) circle (.1);
\filldraw[color=black, opacity=0.3] (-7,0) circle (.1);
\filldraw[color=black, opacity=0.3] (-6,0) circle (.1);
\draw[-, ultra thick, opacity=0.3] (-5,0)  -- (-5,1);
\filldraw[color=black, opacity=0.3] (-5,1) circle (.1);
\draw[-, ultra thick] (-4,0)  -- (-4,1);
\filldraw[color=black] (-4,1) circle (.1);
\draw[-, ultra thick] (-3,0)  -- (-3,1);
\filldraw[color=black] (-3,1) circle (.1);
\filldraw[color=black] (-2,0) circle (.1);
\filldraw[color=black] (-1,0) circle (.1);
\filldraw[color=black] (0,0) circle (.1);
\draw[-, ultra thick] (1,0)  -- (1,1);
\filldraw[color=black] (1,1) circle (.1);
% arrows showing the direction of progress of f[1-n] along summation 
\draw[->, thick] (-3.9,1.3) to[out=45, in=135] (-3.1,1.3);
\draw[->, thick] (-2.9,1.3) to[out=45, in=90] (-2.1,0.3);
\draw[->, thick] (-1.9,0.3) to[out=45, in=135] (-1.1,0.3);
\draw[->, thick] (-0.9,0.3) to[out=45, in=135] (-0.1,0.3);
\draw[->, thick] (0.1,0.3) to[out=90, in=135] (0.9,1.3);
\end{tikzpicture}%\caption{}
\end{center}
\end{figure}

\vspace{-1cm}

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
% f[2-n] axis
\node[anchor=south] (note) at (-10, 0.5) {$f[2-n]$};
\draw[-, thick]
    (-9,0) node[] {$|$}
 -- (-9,0) node[anchor=north, yshift=-1.5ex] {$-5$}
 -- (-8,0) node[] {$|$}
 -- (-8,0) node[anchor=north, yshift=-1.5ex] {$-4$}
 -- (-7,0) node[] {$|$}
 -- (-7,0) node[anchor=north, yshift=-1.5ex] {$-3$}
 -- (-6,0) node[] {$|$}
 -- (-6,0) node[anchor=north, yshift=-1.5ex] {$-2$}
 -- (-5,0) node[] {$|$}
 -- (-5,0) node[anchor=north, yshift=-1.5ex] {$-1$}
 --	(-4,0) node[] {$|$}
 -- (-4,0) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (-3,0) node[] {$|$}
 -- (-3,0) node[anchor=north, yshift=-1.5ex] {$1$}
 -- (-2,0) node[] {$|$}
 -- (-2,0) node[anchor=north, yshift=-1.5ex] {$2$}
 -- (-1,0) node[] {$|$}
 -- (-1,0) node[anchor=north, yshift=-1.5ex] {$3$}
 -- (0,0) node[] {$|$}
 -- (0,0) node[anchor=north, yshift=-1.5ex] {$4$}
 -- (1,0) node[] {$|$}
 -- (1,0) node[anchor=north, yshift=-1.5ex] {$5$};
% f[2-n] samples
\draw[-, ultra thick, opacity=0.3] (-9,0)  -- (-9,1);
\filldraw[color=black, opacity=0.3] (-9,1) circle (.1);
\draw[-, ultra thick, opacity=0.3] (-8,0)  -- (-8,1);
\filldraw[color=black, opacity=0.3] (-8,1) circle (.1);
\filldraw[color=black, opacity=0.3] (-7,0) circle (.1);
\filldraw[color=black, opacity=0.3] (-6,0) circle (.1);
\filldraw[color=black, opacity=0.3] (-5,0) circle (.1);
\draw[-, ultra thick] (-4,0)  -- (-4,1);
\filldraw[color=black] (-4,1) circle (.1);
\draw[-, ultra thick] (-3,0)  -- (-3,1);
\filldraw[color=black] (-3,1) circle (.1);
\draw[-, ultra thick] (-2,0)  -- (-2,1);
\filldraw[color=black] (-2,1) circle (.1);
\filldraw[color=black] (-1,0) circle (.1);
\filldraw[color=black] (0,0) circle (.1);
\filldraw[color=black] (1,0) circle (.1);
% arrows showing the direction of progress of f[2-n] along summation 
\draw[->, thick] (-3.9,1.3) to[out=45, in=135] (-3.1,1.3);
\draw[->, thick] (-2.9,1.3) to[out=45, in=135] (-2.1,1.3);
\draw[->, thick] (-1.9,1.3) to[out=45, in=90] (-1.1,0.3);
\draw[->, thick] (-0.9,0.3) to[out=45, in=135] (-0.1,0.3);
\draw[->, thick] (0.1,0.3) to[out=45, in=135] (0.9,0.3);
\end{tikzpicture}%\caption{}
\end{center}
\end{figure}

\vspace{-1cm}

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
% f[3-n] axis
\node[anchor=south] (note) at (-10, 0.5) {$f[3-n]$};
\draw[-, thick]
    (-9,0) node[] {$|$}
 -- (-9,0) node[anchor=north, yshift=-1.5ex] {$-5$}
 -- (-8,0) node[] {$|$}
 -- (-8,0) node[anchor=north, yshift=-1.5ex] {$-4$}
 -- (-7,0) node[] {$|$}
 -- (-7,0) node[anchor=north, yshift=-1.5ex] {$-3$}
 -- (-6,0) node[] {$|$}
 -- (-6,0) node[anchor=north, yshift=-1.5ex] {$-2$}
 -- (-5,0) node[] {$|$}
 -- (-5,0) node[anchor=north, yshift=-1.5ex] {$-1$}
 --	(-4,0) node[] {$|$}
 -- (-4,0) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (-3,0) node[] {$|$}
 -- (-3,0) node[anchor=north, yshift=-1.5ex] {$1$}
 -- (-2,0) node[] {$|$}
 -- (-2,0) node[anchor=north, yshift=-1.5ex] {$2$}
 -- (-1,0) node[] {$|$}
 -- (-1,0) node[anchor=north, yshift=-1.5ex] {$3$}
 -- (0,0) node[] {$|$}
 -- (0,0) node[anchor=north, yshift=-1.5ex] {$4$}
 -- (1,0) node[] {$|$}
 -- (1,0) node[anchor=north, yshift=-1.5ex] {$5$};
% f[3-n] samples
\draw[-, ultra thick, opacity=0.3] (-9,0)  -- (-9,1);
\filldraw[color=black, opacity=0.3] (-9,1) circle (.1);
\draw[-, ultra thick, opacity=0.3] (-8,0)  -- (-8,1);
\filldraw[color=black, opacity=0.3] (-8,1) circle (.1);
\draw[-, ultra thick, opacity=0.3] (-7,0)  -- (-7,1);
\filldraw[color=black, opacity=0.3] (-7,1) circle (.1);
\filldraw[color=black, opacity=0.3] (-6,0) circle (.1);
\filldraw[color=black, opacity=0.3] (-5,0) circle (.1);
\filldraw[color=black] (-4,0) circle (.1);
\draw[-, ultra thick] (-3,0)  -- (-3,1);
\filldraw[color=black] (-3,1) circle (.1);
\draw[-, ultra thick] (-2,0)  -- (-2,1);
\filldraw[color=black] (-2,1) circle (.1);
\draw[-, ultra thick] (-1,0)  -- (-1,1);
\filldraw[color=black] (-1,1) circle (.1);
\filldraw[color=black] (0,0) circle (.1);
\filldraw[color=black] (1,0) circle (.1);
% arrows showing the direction of progress of f[3-n] along summation 
\draw[->, thick] (-3.9,0.3) to[out=90, in=135] (-3.1,1.3);
\draw[->, thick] (-2.9,1.3) to[out=45, in=135] (-2.1,1.3);
\draw[->, thick] (-1.9,1.3) to[out=45, in=135] (-1.1,1.3);
\draw[->, thick] (-0.9,1.3) to[out=45, in=90] (-0.1,0.3);
\draw[->, thick] (0.1,0.3) to[out=45, in=135] (0.9,0.3);
\end{tikzpicture}%\caption{}
\end{center}
\end{figure}

\vspace{-1cm}

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
% f[4-n] axis
\node[anchor=south] (note) at (-10, 0.5) {$f[4-n]$};
\draw[-, thick]
    (-9,0) node[] {$|$}
 -- (-9,0) node[anchor=north, yshift=-1.5ex] {$-5$}
 -- (-8,0) node[] {$|$}
 -- (-8,0) node[anchor=north, yshift=-1.5ex] {$-4$}
 -- (-7,0) node[] {$|$}
 -- (-7,0) node[anchor=north, yshift=-1.5ex] {$-3$}
 -- (-6,0) node[] {$|$}
 -- (-6,0) node[anchor=north, yshift=-1.5ex] {$-2$}
 -- (-5,0) node[] {$|$}
 -- (-5,0) node[anchor=north, yshift=-1.5ex] {$-1$}
 --	(-4,0) node[] {$|$}
 -- (-4,0) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (-3,0) node[] {$|$}
 -- (-3,0) node[anchor=north, yshift=-1.5ex] {$1$}
 -- (-2,0) node[] {$|$}
 -- (-2,0) node[anchor=north, yshift=-1.5ex] {$2$}
 -- (-1,0) node[] {$|$}
 -- (-1,0) node[anchor=north, yshift=-1.5ex] {$3$}
 -- (0,0) node[] {$|$}
 -- (0,0) node[anchor=north, yshift=-1.5ex] {$4$}
 -- (1,0) node[] {$|$}
 -- (1,0) node[anchor=north, yshift=-1.5ex] {$5$};
% f[4-n] samples
\filldraw[color=black, opacity=0.3] (-9,0) circle (.1);
\draw[-, ultra thick, opacity=0.3] (-8,0)  -- (-8,1);
\filldraw[color=black, opacity=0.3] (-8,1) circle (.1);
\draw[-, ultra thick, opacity=0.3] (-7,0)  -- (-7,1);
\filldraw[color=black, opacity=0.3] (-7,1) circle (.1);
\draw[-, ultra thick, opacity=0.3] (-6,0)  -- (-6,1);
\filldraw[color=black, opacity=0.3] (-6,1) circle (.1);
\filldraw[color=black, opacity=0.3] (-5,0) circle (.1);
\filldraw[color=black] (-4,0) circle (.1);
\filldraw[color=black] (-3,0) circle (.1);
\draw[-, ultra thick] (-2,0)  -- (-2,1);
\filldraw[color=black] (-2,1) circle (.1);
\draw[-, ultra thick] (-1,0)  -- (-1,1);
\filldraw[color=black] (-1,1) circle (.1);
\draw[-, ultra thick] (0,0)  -- (0,1);
\filldraw[color=black] (0,1) circle (.1);
\filldraw[color=black] (1,0) circle (.1);
% arrows showing the direction of progress of f[4-n] along summation 
\draw[->, thick] (-3.9,0.3) to[out=45, in=135] (-3.1,0.3);
\draw[->, thick] (-2.9,0.3) to[out=90, in=135] (-2.1,1.3);
\draw[->, thick] (-1.9,1.3) to[out=45, in=135] (-1.1,1.3);
\draw[->, thick] (-0.9,1.3) to[out=45, in=135] (-0.1,1.3);
\draw[->, thick] (0.1,1.3) to[out=45, in=90] (0.9,0.3);
\end{tikzpicture}%\caption{}
\end{center}
\end{figure}

\vspace{-1cm}

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
% f[5-n] axis
\node[anchor=south] (note) at (-10, 0.5) {$f[5-n]$};
\draw[-, thick]
    (-9,0) node[] {$|$}
 -- (-9,0) node[anchor=north, yshift=-1.5ex] {$-5$}
 -- (-8,0) node[] {$|$}
 -- (-8,0) node[anchor=north, yshift=-1.5ex] {$-4$}
 -- (-7,0) node[] {$|$}
 -- (-7,0) node[anchor=north, yshift=-1.5ex] {$-3$}
 -- (-6,0) node[] {$|$}
 -- (-6,0) node[anchor=north, yshift=-1.5ex] {$-2$}
 -- (-5,0) node[] {$|$}
 -- (-5,0) node[anchor=north, yshift=-1.5ex] {$-1$}
 --	(-4,0) node[] {$|$}
 -- (-4,0) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (-3,0) node[] {$|$}
 -- (-3,0) node[anchor=north, yshift=-1.5ex] {$1$}
 -- (-2,0) node[] {$|$}
 -- (-2,0) node[anchor=north, yshift=-1.5ex] {$2$}
 -- (-1,0) node[] {$|$}
 -- (-1,0) node[anchor=north, yshift=-1.5ex] {$3$}
 -- (0,0) node[] {$|$}
 -- (0,0) node[anchor=north, yshift=-1.5ex] {$4$}
 -- (1,0) node[] {$|$}
 -- (1,0) node[anchor=north, yshift=-1.5ex] {$5$};
% f[5-n] samples
\filldraw[color=black, opacity=0.3] (-9,0) circle (.1);
\filldraw[color=black, opacity=0.3] (-8,0) circle (.1);
\filldraw[color=black, opacity=0.3] (-8,0) circle (.1);
\draw[-, ultra thick, opacity=0.3] (-7,0)  -- (-7,1);
\filldraw[color=black, opacity=0.3] (-7,1) circle (.1);
\draw[-, ultra thick, opacity=0.3] (-6,0)  -- (-6,1);
\filldraw[color=black, opacity=0.3] (-6,1) circle (.1);
\draw[-, ultra thick, opacity=0.3] (-5,0)  -- (-5,1);
\filldraw[color=black, opacity=0.3] (-5,1) circle (.1);
\filldraw[color=black] (-4,0) circle (.1);
\filldraw[color=black] (-3,0) circle (.1);
\filldraw[color=black] (-2,0) circle (.1);
\draw[-, ultra thick] (-1,0)  -- (-1,1);
\filldraw[color=black] (-1,1) circle (.1);
\draw[-, ultra thick] (0,0)  -- (0,1);
\filldraw[color=black] (0,1) circle (.1);
\draw[-, ultra thick] (1,0)  -- (1,1);
\filldraw[color=black] (1,1) circle (.1);
% arrows showing the direction of progress of f[5-n] along summation 
\draw[->, thick] (-3.9,0.3) to[out=45, in=135] (-3.1,0.3);
\draw[->, thick] (-2.9,0.3) to[out=45, in=135] (-2.1,0.3);
\draw[->, thick] (-1.9,0.3) to[out=90, in=135] (-1.1,1.3);
\draw[->, thick] (-0.9,1.3) to[out=45, in=135] (-0.1,1.3);
\draw[->, thick] (0.1,1.3) to[out=45, in=135] (0.9,1.3);
\end{tikzpicture}%\caption{}
\end{center}
\end{figure}


  This approach of using the periodic extension when shifting the reverse signal, $f[-n]$, to the right in the positive direction to generate the $f[m-n]$ is equivalent to cycling the signal values: ``popping the tail and appending it to the head of the list, reindexing samples to match the new order''.
  
\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
% f[n] axis
\node[anchor=south] (note) at (-5, 0.5) {$f[n]$};
\draw[-, thick]
    (-4,0) node[] {$|$}
 -- (-4,0) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (-3,0) node[] {$|$}
 -- (-3,0) node[anchor=north, yshift=-1.5ex] {$1$}
 -- (-2,0) node[] {$|$}
 -- (-2,0) node[anchor=north, yshift=-1.5ex] {$2$}
 -- (-1,0) node[] {$|$}
 -- (-1,0) node[anchor=north, yshift=-1.5ex] {$3$}
 -- (0,0) node[] {$|$}
 -- (0,0) node[anchor=north, yshift=-1.5ex] {$4$}
 -- (1,0) node[] {$|$}
 -- (1,0) node[anchor=north, yshift=-1.5ex] {$5$};
\draw[-, ultra thick] (-4,0)  -- (-4,1);
\filldraw[color=black] (-4,1) circle (.1);
\draw[-, ultra thick] (-3,0)  -- (-3,1);
\filldraw[color=black] (-3,1) circle (.1);
\draw[-, ultra thick] (-2,0)  -- (-2,1);
\filldraw[color=black] (-2,1) circle (.1);
\filldraw[color=black] (-1,0) circle (.1);
\filldraw[color=black] (0,0) circle (.1);
\filldraw[color=black] (1,0) circle (.1);
% arrows showing the direction of progress of f[n] along summation 
\draw[->, thick] (-3.9,1.3) to[out=45, in=135] (-3.1,1.3);
\draw[->, thick] (-2.9,1.3) to[out=45, in=135] (-2.1,1.3);
\draw[->, thick] (-1.9,1.3) to[out=45, in=95] (-1.1,0.3);
\draw[->, thick] (-0.9,0.3) to[out=45, in=135] (-0.1,0.3);
\draw[->, thick] (0.2,0.3) to[out=45, in=135] (0.9,0.3);
% (f * f)[m=0]
%\node[anchor=south] (note) at (3.75, 1.0) {$(f*f)[m] = $};
%\node[anchor=south] (note) at (4, 0.3) {$\sum_n f[n] \, f[m-n]$};
\end{tikzpicture}%\caption{}
\end{center}
\end{figure}

\vspace{-1cm}

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
% f[-n] axis
\node[anchor=south] (note) at (-5, 0.5) {$f[-n]$};
\draw[-, thick]
    (-4,0) node[] {$|$}
 -- (-4,0) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (-3,0) node[] {$|$}
 -- (-3,0) node[anchor=north, yshift=-1.5ex] {$1$}
 -- (-2,0) node[] {$|$}
 -- (-2,0) node[anchor=north, yshift=-1.5ex] {$2$}
 -- (-1,0) node[] {$|$}
 -- (-1,0) node[anchor=north, yshift=-1.5ex] {$3$}
 -- (0,0) node[] {$|$}
 -- (0,0) node[anchor=north, yshift=-1.5ex] {$4$}
 -- (1,0) node[] {$|$}
 -- (1,0) node[anchor=north, yshift=-1.5ex] {$5$};
% f[-n] samples
\draw[-, ultra thick] (-4,0)  -- (-4,1);
\filldraw[color=black] (-4,1) circle (.1);
\filldraw[color=black] (-3,0) circle (.1);
\filldraw[color=black] (-2,0) circle (.1);
\filldraw[color=black] (-1,0) circle (.1);
\draw[-, ultra thick] (0,0)  -- (0,1);
\filldraw[color=black] (0,1) circle (.1);
\draw[-, ultra thick] (1,0)  -- (1,1);
\filldraw[color=black] (1,1) circle (.1);
% arrows showing the direction of progress of f[-n] along summation 
\draw[->, thick] (-3.9,1.3) to[out=45, in=90] (-3.1,0.3);
\draw[->, thick] (-2.9,0.3) to[out=45, in=135] (-2.1,0.3);
\draw[->, thick] (-1.9,0.3) to[out=45, in=135] (-1.1,0.3);
\draw[->, thick] (-0.9,0.3) to[out=90, in=135] (-0.1,1.3);
\draw[->, thick] (0.1,1.3) to[out=45, in=135] (0.9,1.3);
\end{tikzpicture}%\caption{}
\end{center}
\end{figure}

\vspace{-1cm}

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
% f[-n] axis
\node[anchor=south] (note) at (-5, 0.5) {$f[1-n]$};
\draw[-, thick]
    (-4,0) node[] {$|$}
 -- (-4,0) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (-3,0) node[] {$|$}
 -- (-3,0) node[anchor=north, yshift=-1.5ex] {$1$}
 -- (-2,0) node[] {$|$}
 -- (-2,0) node[anchor=north, yshift=-1.5ex] {$2$}
 -- (-1,0) node[] {$|$}
 -- (-1,0) node[anchor=north, yshift=-1.5ex] {$3$}
 -- (0,0) node[] {$|$}
 -- (0,0) node[anchor=north, yshift=-1.5ex] {$4$}
 -- (1,0) node[] {$|$}
 -- (1,0) node[anchor=north, yshift=-1.5ex] {$5$};
% f[1-n] samples
\draw[-, ultra thick] (-4,0)  -- (-4,1);
\filldraw[color=black] (-4,1) circle (.1);
\draw[-, ultra thick] (-3,0)  -- (-3,1);
\filldraw[color=black] (-3,1) circle (.1);
\filldraw[color=black] (-2,0) circle (.1);
\filldraw[color=black] (-1,0) circle (.1);
\filldraw[color=black] (0,0) circle (.1);
\draw[-, ultra thick] (1,0)  -- (1,1);
\filldraw[color=black] (1,1) circle (.1);
% arrows showing the direction of progress of f[1-n] along summation 
\draw[->, thick] (-3.9,1.3) to[out=45, in=135] (-3.1,1.3);
\draw[->, thick] (-2.9,1.3) to[out=45, in=90] (-2.1,0.3);
\draw[->, thick] (-1.9,0.3) to[out=45, in=135] (-1.1,0.3);
\draw[->, thick] (-0.9,0.3) to[out=45, in=135] (-0.1,0.3);
\draw[->, thick] (0.1,0.3) to[out=90, in=135] (0.9,1.3);
\end{tikzpicture}%\caption{}
\end{center}
\end{figure}

\vspace{-1cm}

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
% f[2-n] axis
\node[anchor=south] (note) at (-5, 0.5) {$f[2-n]$};
\draw[-, thick]
    (-4,0) node[] {$|$}
 -- (-4,0) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (-3,0) node[] {$|$}
 -- (-3,0) node[anchor=north, yshift=-1.5ex] {$1$}
 -- (-2,0) node[] {$|$}
 -- (-2,0) node[anchor=north, yshift=-1.5ex] {$2$}
 -- (-1,0) node[] {$|$}
 -- (-1,0) node[anchor=north, yshift=-1.5ex] {$3$}
 -- (0,0) node[] {$|$}
 -- (0,0) node[anchor=north, yshift=-1.5ex] {$4$}
 -- (1,0) node[] {$|$}
 -- (1,0) node[anchor=north, yshift=-1.5ex] {$5$};
% f[2-n] samples
\draw[-, ultra thick] (-4,0)  -- (-4,1);
\filldraw[color=black] (-4,1) circle (.1);
\draw[-, ultra thick] (-3,0)  -- (-3,1);
\filldraw[color=black] (-3,1) circle (.1);
\draw[-, ultra thick] (-2,0)  -- (-2,1);
\filldraw[color=black] (-2,1) circle (.1);
\filldraw[color=black] (-1,0) circle (.1);
\filldraw[color=black] (0,0) circle (.1);
\filldraw[color=black] (1,0) circle (.1);
% arrows showing the direction of progress of f[2-n] along summation 
\draw[->, thick] (-3.9,1.3) to[out=45, in=135] (-3.1,1.3);
\draw[->, thick] (-2.9,1.3) to[out=45, in=135] (-2.1,1.3);
\draw[->, thick] (-1.9,1.3) to[out=45, in=90] (-1.1,0.3);
\draw[->, thick] (-0.9,0.3) to[out=45, in=135] (-0.1,0.3);
\draw[->, thick] (0.1,0.3) to[out=45, in=135] (0.9,0.3);
\end{tikzpicture}%\caption{}
\end{center}
\end{figure}

\vspace{-1cm}

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
% f[3-n] axis
\node[anchor=south] (note) at (-5, 0.5) {$f[3-n]$};
\draw[-, thick]
    (-4,0) node[] {$|$}
 -- (-4,0) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (-3,0) node[] {$|$}
 -- (-3,0) node[anchor=north, yshift=-1.5ex] {$1$}
 -- (-2,0) node[] {$|$}
 -- (-2,0) node[anchor=north, yshift=-1.5ex] {$2$}
 -- (-1,0) node[] {$|$}
 -- (-1,0) node[anchor=north, yshift=-1.5ex] {$3$}
 -- (0,0) node[] {$|$}
 -- (0,0) node[anchor=north, yshift=-1.5ex] {$4$}
 -- (1,0) node[] {$|$}
 -- (1,0) node[anchor=north, yshift=-1.5ex] {$5$};
% f[3-n] samples
\filldraw[color=black] (-4,0) circle (.1);
\draw[-, ultra thick] (-3,0)  -- (-3,1);
\filldraw[color=black] (-3,1) circle (.1);
\draw[-, ultra thick] (-2,0)  -- (-2,1);
\filldraw[color=black] (-2,1) circle (.1);
\draw[-, ultra thick] (-1,0)  -- (-1,1);
\filldraw[color=black] (-1,1) circle (.1);
\filldraw[color=black] (0,0) circle (.1);
\filldraw[color=black] (1,0) circle (.1);
% arrows showing the direction of progress of f[3-n] along summation 
\draw[->, thick] (-3.9,0.3) to[out=90, in=135] (-3.1,1.3);
\draw[->, thick] (-2.9,1.3) to[out=45, in=135] (-2.1,1.3);
\draw[->, thick] (-1.9,1.3) to[out=45, in=135] (-1.1,1.3);
\draw[->, thick] (-0.9,1.3) to[out=45, in=90] (-0.1,0.3);
\draw[->, thick] (0.1,0.3) to[out=45, in=135] (0.9,0.3);
\end{tikzpicture}%\caption{}
\end{center}
\end{figure}

\vspace{-1cm}

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
% f[4-n] axis
\node[anchor=south] (note) at (-5, 0.5) {$f[4-n]$};
\draw[-, thick]
    (-4,0) node[] {$|$}
 -- (-4,0) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (-3,0) node[] {$|$}
 -- (-3,0) node[anchor=north, yshift=-1.5ex] {$1$}
 -- (-2,0) node[] {$|$}
 -- (-2,0) node[anchor=north, yshift=-1.5ex] {$2$}
 -- (-1,0) node[] {$|$}
 -- (-1,0) node[anchor=north, yshift=-1.5ex] {$3$}
 -- (0,0) node[] {$|$}
 -- (0,0) node[anchor=north, yshift=-1.5ex] {$4$}
 -- (1,0) node[] {$|$}
 -- (1,0) node[anchor=north, yshift=-1.5ex] {$5$};
% f[4-n] samples
\filldraw[color=black] (-4,0) circle (.1);
\filldraw[color=black] (-3,0) circle (.1);
\draw[-, ultra thick] (-2,0)  -- (-2,1);
\filldraw[color=black] (-2,1) circle (.1);
\draw[-, ultra thick] (-1,0)  -- (-1,1);
\filldraw[color=black] (-1,1) circle (.1);
\draw[-, ultra thick] (0,0)  -- (0,1);
\filldraw[color=black] (0,1) circle (.1);
\filldraw[color=black] (1,0) circle (.1);
% arrows showing the direction of progress of f[4-n] along summation 
\draw[->, thick] (-3.9,0.3) to[out=45, in=135] (-3.1,0.3);
\draw[->, thick] (-2.9,0.3) to[out=90, in=135] (-2.1,1.3);
\draw[->, thick] (-1.9,1.3) to[out=45, in=135] (-1.1,1.3);
\draw[->, thick] (-0.9,1.3) to[out=45, in=135] (-0.1,1.3);
\draw[->, thick] (0.1,1.3) to[out=45, in=90] (0.9,0.3);
\end{tikzpicture}%\caption{}
\end{center}
\end{figure}

\vspace{-1cm}

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
% f[5-n] axis
\node[anchor=south] (note) at (-5, 0.5) {$f[5-n]$};
\draw[-, thick]
    (-4,0) node[] {$|$}
 -- (-4,0) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (-3,0) node[] {$|$}
 -- (-3,0) node[anchor=north, yshift=-1.5ex] {$1$}
 -- (-2,0) node[] {$|$}
 -- (-2,0) node[anchor=north, yshift=-1.5ex] {$2$}
 -- (-1,0) node[] {$|$}
 -- (-1,0) node[anchor=north, yshift=-1.5ex] {$3$}
 -- (0,0) node[] {$|$}
 -- (0,0) node[anchor=north, yshift=-1.5ex] {$4$}
 -- (1,0) node[] {$|$}
 -- (1,0) node[anchor=north, yshift=-1.5ex] {$5$};
% f[4-n] samples
\filldraw[color=black] (-4,0) circle (.1);
\filldraw[color=black] (-3,0) circle (.1);
\filldraw[color=black] (-2,0) circle (.1);
\draw[-, ultra thick] (-1,0)  -- (-1,1);
\filldraw[color=black] (-1,1) circle (.1);
\draw[-, ultra thick] (0,0)  -- (0,1);
\filldraw[color=black] (0,1) circle (.1);
\draw[-, ultra thick] (1,0)  -- (1,1);
\filldraw[color=black] (1,1) circle (.1);
% arrows showing the direction of progress of f[4-n] along summation 
\draw[->, thick] (-3.9,0.3) to[out=45, in=135] (-3.1,0.3);
\draw[->, thick] (-2.9,0.3) to[out=45, in=135] (-2.1,0.3);
\draw[->, thick] (-1.9,0.3) to[out=90, in=135] (-1.1,1.3);
\draw[->, thick] (-0.9,1.3) to[out=45, in=135] (-0.1,1.3);
\draw[->, thick] (0.1,1.3) to[out=45, in=135] (0.9,1.3);
\end{tikzpicture}%\caption{}
\end{center}
\end{figure}

\vspace{-1cm}

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
% (f*f)[m] axis
\node[anchor=south] (note) at (-5, 2) {$(f*f)[m]$};
\node[anchor=north] (note) at (-1.5, -1) {$[m]$};
\draw[-, thick]
   	(-4,0) node[] {$|$}
 -- (-4,0) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (-3,0) node[] {$|$}
 -- (-3,0) node[anchor=north, yshift=-1.5ex] {$1$}
 -- (-2,0) node[] {$|$}
 -- (-2,0) node[anchor=north, yshift=-1.5ex] {$2$}
 -- (-1,0) node[] {$|$}
 -- (-1,0) node[anchor=north, yshift=-1.5ex] {$3$}
 -- (0,0) node[] {$|$}
 -- (0,0) node[anchor=north, yshift=-1.5ex] {$4$}
 -- (1,0) node[] {$|$}
 -- (1,0) node[anchor=north, yshift=-1.5ex] {$5$};
% (f*f)[m] samples
\draw[-, ultra thick] (-4,0)  -- (-4,1);
\filldraw[color=black] (-4,1) circle (.1);
\draw[-, ultra thick] (-3,0)  -- (-3,2);
\filldraw[color=black] (-3,2) circle (.1);
\draw[-, ultra thick] (-2,0)  -- (-2,3);
\filldraw[color=black] (-2,3) circle (.1);
\draw[-, ultra thick] (-1,0)  -- (-1,2);
\filldraw[color=black] (-1,2) circle (.1);
\draw[-, ultra thick] (0,0)  -- (0,1);
\filldraw[color=black] (0,1) circle (.1);
\filldraw[color=black] (1,0) circle (.1);
\end{tikzpicture}%\caption{}
\end{center}
\end{figure}
  
  This approach works for even or odd $N$, obviously.
  It also works for signals on the symmetric grid.
  To implement this for $N$ odd on the symmetric grid is simple.
  To implement this for $N$ even on the symmetric grid is maybe worth showing...
  Also the convolution does not have to involve signals that are the same length (same no. samples)...Definitely show how to do this!!!


  MIT OCW Signals and Systems Spring 2011, Alan V. Oppenheim, Lec 4 Convolution
  Sometimes convolution of $x$ and $h$ is denoted
  $$x[n] * h[n] = \sum_k x[k] \, h[n-k]$$
  this is confusing because it does not appear that $x$ is a function of $n$ but rather a function of $k$.
  But $x[n]$ can be written as (cite?)
  $$x[n] = \sum_k x[k] \, \delta[n-k]$$
  , which makes the notation a little more justified.
  %But what exactly {\em is} $h[-k]$?
  %Ans. it is $h[k]$ reflected about the origin of the $[k]$ axis $[k=0]$ min 25:41 (but we do not know if $N$ is even or odd, although it is on a symmetric axis, in the example shown).
  


  \section{The DFT of Common Signals }

  NOTE: the definition for the DFT used in these examples lacks the $1/N$ factor most if not all of the time in the subsequent examples and treatment...(UPDATE this or modify my own convention?)

  \subsection{The DFT of the Boxcar, ${\bf \Pi}$}

  The DFT of the boxcar function is so important because it is used for signal reconstruction in the discrete form of the sampling theorem.

  (boxcar with $2N$ data points - p.60 , Digital Filters (Hanning))
  
  To center the boxcar function, ${\Pi}$, on the symmetric axis grid it must have an odd number of data points in the region where it is non-zero for it to remain symmetrical.
  An even number of points in the non-zero region would produce an asymmetric construction were it assembled on a grid axis centered at zero (DO THIS NEXT...This is essentially what sampling with $N$ even is).
  The discrete boxcar function of width ($b$), $\Pi_b$, is

  $$\boldsymbol{\Pi}_b =
    	\begin{cases}
	    	1 & \text{if $|k| \leq b/2$} \\
	    	0 & \text{if $|k| > b/2$}
    	\end{cases}
  $$

\begin{center}
\begin{tikzpicture}
% horizontal labels
\draw[<->] (-8,0)
	  -- (-6,0) node[anchor=north] {${-N\over2}\!+\!1$}
	  -- (-4,0) node[anchor=north] {${-b\over2}$}
	  -- (-3,0) node[anchor=north] {${-b\over2}\!+\!1$}
	  -- (-2,0) node[anchor=north] {$\cdots$}
	  -- (-1,0) node[anchor=north] {$-1$}
	  -- (0,0) node[anchor=north] {$0$}
	  -- (1,0) node[anchor=north] {$1$}
	  -- (2,0) node[anchor=north] {$\cdots$}
	  -- (3,0) node[anchor=north] {${b\over2}\!-\!1$}
	  -- (4,0) node[anchor=north] {${b\over2}$}
	  -- (7,0) node[anchor=north] {${N\over2}$}
	  -- (8,0);

% Dirac functions
\draw[-, very thick] (-4,0) -- (-4,1.5);
\draw[-, very thick] (-3,0) -- (-3,1.5);
\draw[-, very thick] (-2,0) -- (-2,1.5);
\draw[-, very thick] (-1,0) -- (-1,1.5);
\draw[-, very thick] (0,0)  -- (0,1.5);
\draw[-, very thick] (1,0)  -- (1,1.5);
\draw[-, very thick] (2,0)  -- (2,1.5);
\draw[-, very thick] (3,0)  -- (3,1.5);
\draw[-, very thick] (4,0)  -- (4,1.5);
\filldraw[black] (-7,0)  circle (.1);
\filldraw[black] (-6,0)  circle (.1);
\filldraw[black] (-5,0)  circle (.1);
\filldraw[black] (-4,1.5)  circle (.1);
\filldraw[black] (-3,1.5)  circle (.1);
\filldraw[black] (-2,1.5)  circle (.1);
\filldraw[black] (-1,1.5)  circle (.1);
\filldraw[black] (0,1.5)  circle (.1);
\filldraw[black] (1,1.5)  circle (.1);
\filldraw[black] (2,1.5)  circle (.1);
\filldraw[black] (3,1.5)  circle (.1);
\filldraw[black] (4,1.5)  circle (.1);
\filldraw[black] (5,0)  circle (.1);
\filldraw[black] (6,0)  circle (.1);
\filldraw[black] (7,0)  circle (.1);

\end{tikzpicture}
\end{center}

  For $k=0$ the DFT of the boxcar, $\mathcal{F}\,\Pi_b$, is
  $$F[0] = \sum_n \Pi_b\, e^{-i2\pi (0) n / N} = \sum_{n=-b/2}^{b/2} 1 = b+1$$
  The DFT of the boxcar for $k\ne0$ is
\begin{align*}
  F[k] = \sum_n \Pi_b\, e^{-i2\pi k n / N} &= \sum_{n=-b/2}^{b/2}e^{-i2\pi k n / N} = { e^{i2\pi k b / (2N)} - e^{-i2\pi k (b/2+1) / N}) \over 1 - e^{-i2\pi k / N} } \\
  &= { e^{-i\pi k / N} ( e^{{i2\pi k\over N} {b\over2} + {i\pi k \over N}} - e^{{-i2\pi k\over N} \bigl({b\over 2}+1\bigr) + {i\pi k \over N}} ) \over e^{-i\pi k / N} (e^{i\pi k / N} - e^{-i\pi k / N}) } \\
  &= { e^{{i2\pi k\over N} {b\over2} + {i2\pi k \over 2N}} - e^{( {-i2\pi k\over 2N} (b+2) + {i2\pi k \over 2N})} \over e^{i\pi k / N} - e^{-i\pi k / N} } \\
  &= { e^{{i2\pi k\over 2N} (b+1)} - e^{{-i2\pi k\over 2N} (b+2-1)} \over e^{i\pi k / N} - e^{-i\pi k / N} } \\
  &= { ( e^{{i\pi k\over N} (b+1)} - e^{{-i\pi k\over N} (b+1)} )/(i2) \over ( e^{i\pi k / N} - e^{-i\pi k / N} ) / (i2) } \\
  &= { \sin(\pi k (b + 1) / N) \over \sin(\pi k / N) }
\end{align*}
  Then the DFT of the boxcar with ($b$) odd on the symmetric grid is
  $$\boxed{\mathcal{F}\,\Pi_b =
    	\begin{cases}
	    	b+1 & \quad\text{$k$ mod $N=0$} \\
  			{ \sin(\pi k (b + 1) / N) \over \sin(\pi k / N) } & \quad\text{$k$ mod $N\ne0$}
    	\end{cases}
  }$$


  (...motivation...)
  Taking the average at the discontinuities, the boxcar function of width ($b$) odd becomes 

  $$\boldsymbol{\Pi}_b =
    	\begin{cases}
	    	1 & \text{if $|k| < b/2$} \\
	    	1/2 & \text{if $|k| = b/2$} \\
	    	0 & \text{if $|k| > b/2$}
    	\end{cases}
  $$

\begin{center}
\begin{tikzpicture}
% horizontal labels
\draw[<->] (-8,0)
	  -- (-6,0) node[anchor=north] {${-N\over2}\!+\!1$}
	  -- (-4,0) node[anchor=north] {${-b\over2}$}
	  -- (-3,0) node[anchor=north] {${-b\over2}\!+\!1$}
	  -- (-2,0) node[anchor=north] {$\cdots$}
	  -- (-1,0) node[anchor=north] {$-1$}
	  -- (0,0) node[anchor=north] {$0$}
	  -- (1,0) node[anchor=north] {$1$}
	  -- (2,0) node[anchor=north] {$\cdots$}
	  -- (3,0) node[anchor=north] {${b\over2}\!-\!1$}
	  -- (4,0) node[anchor=north] {${b\over2}$}
	  -- (7,0) node[anchor=north] {${N\over2}$}
	  -- (8,0);

% Dirac functions
\draw[-, very thick] (-4,0) -- (-4,.75);
\draw[-, very thick] (-3,0) -- (-3,1.5);
\draw[-, very thick] (-2,0) -- (-2,1.5);
\draw[-, very thick] (-1,0) -- (-1,1.5);
\draw[-, very thick] (0,0)  -- (0,1.5);
\draw[-, very thick] (1,0)  -- (1,1.5);
\draw[-, very thick] (2,0)  -- (2,1.5);
\draw[-, very thick] (3,0)  -- (3,1.5);
\draw[-, very thick] (4,0)  -- (4,.75);
\filldraw[black] (-7,0)  circle (.1);
\filldraw[black] (-6,0)  circle (.1);
\filldraw[black] (-5,0)  circle (.1);
\filldraw[black] (-4,.75)  circle (.1);
\filldraw[black] (-3,1.5)  circle (.1);
\filldraw[black] (-2,1.5)  circle (.1);
\filldraw[black] (-1,1.5)  circle (.1);
\filldraw[black] (0,1.5)  circle (.1);
\filldraw[black] (1,1.5)  circle (.1);
\filldraw[black] (2,1.5)  circle (.1);
\filldraw[black] (3,1.5)  circle (.1);
\filldraw[black] (4,.75)  circle (.1);
\filldraw[black] (5,0)  circle (.1);
\filldraw[black] (6,0)  circle (.1);
\filldraw[black] (7,0)  circle (.1);

\end{tikzpicture}
\end{center}

  Taking the average at the discontinuities, the DFT of the boxcar of width ($b$) odd becomes
\begin{align*}
  F[0] &= \sum_n \Pi_b\, e^{-i2\pi (0) n/N} = {1\over2} e^{-i2\pi (0) (-b/2)/N} + {1\over2} e^{-i2\pi (0) (b/2)/N} + \sum_{n=-b/2+1}^{b/2-1} 1 = 1 + (b-1) = b \\
  F[k\ne0] &= \sum_n \Pi_b\, e^{-i2\pi k n / N} = {1\over2} e^{-i2\pi k (-b/2) / N} + {1\over2} e^{-i2\pi k (b/2) / N} + \sum_{n=-b/2+1}^{b/2-1} e^{-i2\pi k n / N} \\
  &= {e^{i2\pi k (b/2)/N} + e^{-i2\pi k (b/2)/N} \over 2} + { e^{-i2\pi k (-b/2+1) / N} - e^{-i2\pi k (b/2) / N} \over 1 - e^{-i2\pi k / N} } \\
  &= \cos(\pi k b / N) + { e^{-i\pi k / N} (e^{i2\pi k (b/2-1)/N + i\pi k/N} - e^{-i2\pi k (b/2) / N + i \pi k / N}) \over e^{-i\pi k / N} (e^{i\pi k / N} - e^{-i\pi k / N}) } \\
  &= \cos(\pi k b / N) + { e^{{i2\pi k\over 2N} (b-2) + {i2\pi k\over2N}} - e^{-{i2\pi k b\over 2N} + {i 2\pi k \over 2N}} \over e^{i\pi k / N} - e^{-i\pi k / N} } \\
  &= \cos(\pi k b / N) + { e^{{i2\pi k\over 2N} (b-2 + 1)} - e^{-{i2\pi k \over 2N} (b - 1)} \over e^{i\pi k / N} - e^{-i\pi k / N} } \\
  &= \cos(\pi k b / N) + { e^{{i\pi k\over N} (b-1)} - e^{-{i\pi k \over N} (b - 1)} \over e^{i\pi k / N} - e^{-i\pi k / N} } \\
  &= \cos(\pi k b / N) + {\sin(\pi k (b-1) / N) \over \sin(\pi k / N)} \\
\intertext{Distributing the denominator, collecting terms, and focusing on the numerator}
\intertext{\qquad\qquad$\cos(\pi k b / N) \sin(\pi k / N) + \sin(\pi k (b-1) / N) = \cos\Bigl({\pi k b \over N}\Bigr) \sin\Bigl({\pi k \over N}\Bigr) + \sin\Bigl({\pi k b \over N} - {\pi k \over N}\Bigr)$}
\intertext{Using the identity $\sin(\theta - \phi)=\sin\theta\cos\phi - \cos\theta\sin\phi$}
  &= \cos(\pi k b / N) \sin(\pi k / N) + \sin(\pi k b / N) \cos(\pi k / N) - \cos(\pi k b / N) \sin(\pi k / N) \\
  &= \sin(\pi k b / N) \cos(\pi k / N)
\end{align*}
  Taking the average at the discontinuities, the DFT of the boxcar of width ($b$) odd simplifies to
  $$\boxed{\mathcal{F}\,\Pi_b =
    	\begin{cases}
	    	b & \quad\text{$k$ mod $N=0$} \\
  			{ \sin\bigl({\pi k\over N} b \bigr) \cos\bigl({\pi k \over N}\bigr) \over \sin\bigl({\pi k \over N}\bigr) } & \quad\text{$k$ mod $N\ne0$}
    	\end{cases}
  }$$

  Osgood refers to this as the discrete $\sinc$ function with the notation $\text{dinc}$.
  I HATE the function name ``dinc''!!!\cite{Osgood}.
  The name $\sinc$ is short for sine cardinal.
  Maybe a nomenclature like discrete sine cardinal, $\disc$, or $\text{dsc}$, would be better.
  Let's just call it what it is, the DFT of the discrete boxcar function, which is such a special thing that it deserves to stand out and gets the elaborate notation $\mathscr{B}$.
  It would have been nice to go with a lowercase notation because it's a function but it's not available for math fonts {\tt mathscr} or {\tt mathcal}, and lowercase {\tt mathfrak}, $\mathfrak{b}$, looks terrible.
  The {\tt mathcal} version, $\mathcal{B}$, looks ok and might allude visually to the DFT from which it came, but to jar your memory and to emphasize its importance let's make the DFT of the boxcar stand out and look as distinct from the standard notation regarding the DFT and as baroque as possible.
  $$\boxed{\mathcal{F}\,\Pi \equiv \mathscr{B}}$$


  NEXT: 
  - extend boxcar to edge of box to make it a window function (that is what is actually used in the discrete form of the sampling theorem, as the boxcar encapsulates the entire frequency spectrum...):
	- N even, and odd on symmetric grid
	- asymmetric grid (N even and odd same)
  - windows with average at end points (but maybe check back on how the input signal gets modified prior to the DFT calculation to make sure you are not technically doing something redundant by averaging the endpoints via the window function!...)
  - boxcar for $N \in [0 : N-1]$ (asymmetric window)
  - N even symmetric
  - N even asymmetric
  boxcar 
  boxcar with even number of points in its width ...

  
  \ul{The DFT of the $N$-odd symmetric boxcar window} ...

  OMG THIS SHIT BY Dr. Osgood IS ALL WRONG!!!
  THE EDGE OF THE WINDOW (or boxcar) CANNOT HAVE INDEX OF $N/2$ IF $N$ IS ODD, because if $N$ is odd then $N/2$ is not an integer and all grid indices must have integer values !!!!!!!!!

\begin{center}
\begin{tikzpicture}
\node[anchor=south] (note) at (0, 2) {($N=9$ (odd) on a symmetric grid)};
% horizontal labels
\draw[<->] (-5,0) node[] {}
	  -- (-4,0) node[anchor=north, yshift=-1ex] {$-{N-1\over2}$}
	  -- (-3,0) node[anchor=north] {}
	  -- (0,0) node[anchor=north, yshift=-1ex] {$0$}
	  -- (3,0) node[anchor=north] {}
	  -- (4,0) node[anchor=north, yshift=-1ex] {${N-1\over2}$}
	  -- (5,0);

% Dirac functions
\draw[-, very thick] (-4,0) -- (-4,1.5);
\draw[-, very thick] (-3,0) -- (-3,1.5);
\draw[-, very thick] (-2,0) -- (-2,1.5);
\draw[-, very thick] (-1,0) -- (-1,1.5);
\draw[-, very thick]  (0,0) --  (0,1.5);
\draw[-, very thick]  (1,0) --  (1,1.5);
\draw[-, very thick]  (2,0) --  (2,1.5);
\draw[-, very thick]  (3,0) --  (3,1.5);
\draw[-, very thick]  (4,0) --  (4,1.5);
\filldraw[black] (-4,1.5)  circle (.1);
\filldraw[black] (-3,1.5)  circle (.1);
\filldraw[black] (-2,1.5)  circle (.1);
\filldraw[black] (-1,1.5)  circle (.1);
\filldraw[black]  (0,1.5)  circle (.1);
\filldraw[black]  (1,1.5)  circle (.1);
\filldraw[black]  (2,1.5)  circle (.1);
\filldraw[black]  (3,1.5)  circle (.1);
\filldraw[black]  (4,1.5)  circle (.1);

\end{tikzpicture}
\end{center}

\begin{align*}
\intertext{If $k=0$}
  F[0] &= \sum_n \Pi\, e^{-i2\pi (0) n / N} = \sum_{n=-(N-1)/2}^{(N-1)/2} 1 = N \\
\intertext{If $k\ne0$}
  F[k] &= \sum_n \Pi \, e^{-i2\pi k n / N} = \sum_{n=-(N-1)/2}^{(N-1)/2}\Bigl(e^{-i{2\pi \over N}k}\Bigr)^n \\
\intertext{Using $\quad \boxed{ 
	\sum_{k=p}^{p+N-1} \omega_N^{k(m-n)} 
	= {\omega_N^p - \omega_N^{p+N}\over 1 - \omega_N} 
	\equiv 
	N\,\delta_N[m-n] 
	= \begin{cases}
	  N & \text{if $m=n$} \\
	  0 & \text{if $m\ne n$ because $\omega_N^p=\omega_N^{p+N}$ }
    \end{cases}
	}$
}
  &= { \exp\Bigl(-i{2\pi\over N} k \bigl(-{(N-1)\over2}\bigr)\Bigr) - \exp\Bigl(-i{2\pi\over N} k \bigl( {(N-1)\over2} + 1 \bigr) \Bigr) \over 1 - \exp\Bigl(-i{2\pi\over N} k \Bigr) } \\
  &= { \exp\Bigl(i{2\pi\over N} k \bigl({N-1\over2}\bigr)\Bigr) - \exp\Bigl(-i{2\pi\over N} k \bigl({N+1\over2}\bigr) \Bigr) \over 1 - \exp\Bigl(-i{2\pi\over N} k \Bigr) } \\
  &= { \exp\Bigl(i{\pi\over N} k (N-1)\Bigr) - \exp\Bigl(-i{\pi\over N} k (N+1) \Bigr) \over 1 - \exp\Bigl(-i{2\pi\over N} k \Bigr) } \\
  &= { \exp\Bigl( -i{\pi\over N} k \Bigr) \biggl( \exp\Bigl(i{\pi\over N} k (N) \Bigr) - \exp\Bigl(-i{\pi\over N} k (N) \Bigr) \biggr) \over \exp\Bigl(-i{\pi\over N} k \Bigr) \biggl( \exp\Bigl(i{\pi\over N} k \Bigr) - \exp\Bigl(-i{\pi\over N} k \Bigr) \biggr) } \\
  &= { \exp\bigl( i\pi k \bigr) - \exp\bigl( -i\pi k \bigr) \over \exp\Bigl(i{\pi\over N} k \Bigr) - \exp\Bigl(-i{\pi\over N} k \Bigr) } \\
  &= { \exp\bigl( i\pi k \bigr) - \exp\bigl( -i\pi k \bigr) \over \exp\bigl(i\pi k / N \bigr) - \exp\bigl(-i\pi k / N \bigr) } \\
\intertext{Using $\qquad\qquad \boxed{ \sin\theta = { e^{i\theta} - e^{-i\theta} \over i2 } }$}
  &= { \Bigl( \exp\bigl( i\pi k \bigr) - \exp\bigl( -i\pi k \bigr) \Bigr) / i2 \over \Bigl( \exp\bigl(i\pi k / N \bigr) - \exp\bigl(-i\pi k / N \bigr) \Bigr) / i2 } \\
  &= { \sin(\pi k) \over \sin(\pi k / N) }
\end{align*}

  If $N$ is odd, the DFT of the symmetric window is

\begin{equation}
  \boxed{\mathcal{F}\,\Pi = \mathcal{W} = % mathcal{W} looks better than mathscr{W} ... not sure which to choose since mathscr{B} is being used for the DFT of the boxcar
    	\begin{cases}
	    	N & \quad\text{if $k$ mod $N=0$} \\
  			{ \sin(\pi k) \over \sin(\pi k / N) } & \quad\text{if $k$ mod $N\ne0$}
    	\end{cases}
  }
\end{equation}


  \ul{The DFT of the $N$-even symmetric boxcar window} ...

\begin{center}
\begin{tikzpicture}
\node[anchor=south] (note) at (0, 2) {($N=8$ even on a symmetric grid)};
% horizontal labels
\draw[<->] (-4,0) node[] {}
	  -- (-3,0) node[anchor=north, yshift=-1ex] {$-{N\over2}+1$}
	  --  (0,0) node[anchor=north, yshift=-1ex] {$0$}
	  --  (4,0) node[anchor=north, yshift=-1ex] {${N\over2}$}
	  --  (6,0);

% Dirac functions
\draw[-, very thick] (-3,0) -- (-3,1.5);
\draw[-, very thick] (-2,0) -- (-2,1.5);
\draw[-, very thick] (-1,0) -- (-1,1.5);
\draw[-, very thick]  (0,0) --  (0,1.5);
\draw[-, very thick]  (1,0) --  (1,1.5);
\draw[-, very thick]  (2,0) --  (2,1.5);
\draw[-, very thick]  (3,0) --  (3,1.5);
\draw[-, very thick]  (4,0) --  (4,1.5);
\filldraw[black] (-3,1.5)  circle (.1);
\filldraw[black] (-2,1.5)  circle (.1);
\filldraw[black] (-1,1.5)  circle (.1);
\filldraw[black]  (0,1.5)  circle (.1);
\filldraw[black]  (1,1.5)  circle (.1);
\filldraw[black]  (2,1.5)  circle (.1);
\filldraw[black]  (3,1.5)  circle (.1);
\filldraw[black]  (4,1.5)  circle (.1);

\end{tikzpicture}
\end{center}

\begin{align*}
\intertext{If $k=0$}
  F[0] &= \sum_n \Pi\, e^{-i2\pi (0) n / N} = \sum_{n=-N/2+1}^{N/2} 1 = N \\
\intertext{If $k\ne0$}
  F[k] &= \sum_n \Pi \, e^{-i2\pi k n / N} = \sum_{n=-N/2+1}^{N/2} \Bigl( e^{-i{2\pi \over N}k} \Bigr)^n \\
\intertext{Using $\quad \boxed{ 
	\sum_{k=p}^{p+N-1} \omega_N^{k(m-n)} 
	= {\omega_N^p - \omega_N^{p+N}\over 1 - \omega_N} 
	\equiv 
	N\,\delta_N[m-n] 
	= \begin{cases}
	  N & \text{if $m=n$} \\
	  0 & \text{if $m\ne n$ because $\omega_N^p=\omega_N^{p+N}$ }
    \end{cases}
	}$
}
\intertext{****FINISH****}
  &= { \exp\Bigl( -i{2\pi\over N} k \bigl( -{N\over2} + 1 \bigr) \Bigr) - \exp\Bigl( -i{2\pi\over N} k \bigl( {N\over2} + 1 \bigr) \Bigr) \over 1 - \exp\Bigl(-i{2\pi\over N} k \Bigr) } \\
  &= { \exp\Bigl( i{2\pi\over N} k \bigl( {N\over2} - 1 \bigr) \Bigr) - \exp\Bigl( -i{2\pi\over N} k \bigl( {N\over2} + 1 \bigr) \Bigr) \over \exp\Bigl( -i {\pi \over N} k \Bigr) \biggl( \exp\Bigl( i {\pi\over N} k \Bigr) - \exp\Bigl(-i{\pi\over N} k \Bigr) \biggr) } \\
\intertext{One approach is to absorb the factor in the denominator into the terms in the numerator}
  &= { \exp\Bigl( +i {\pi \over N} k \Bigr) \biggl( \exp\Bigl( i{2\pi\over N} k \bigl( {N\over2} - 1 \bigr) \Bigr) - \exp\Bigl( -i{2\pi\over N} k \bigl( {N\over2} + 1 \bigr) \Bigr) \biggr) \over \exp\Bigl( i {\pi\over N} k \Bigr) - \exp\Bigl(-i{\pi\over N} k \Bigr) } \\
  &= { \exp\Bigl( i{2\pi\over N} k \bigl( {N\over2} - 1 \bigr) + i{2\pi\over 2N}k \Bigr) - \exp\Bigl( -i{2\pi\over N} k \bigl( {N\over2} + 1 \bigr) + i{2\pi\over 2N}k \Bigr) \over \exp\Bigl( i {\pi\over N} k \Bigr) - \exp\Bigl(-i{\pi\over N} k \Bigr) } \\
  &= { \exp\Bigl( i{2\pi\over 2N} k \bigl( N - 2 + 1 \bigr) \Bigr) - \exp\Bigl( -i{2\pi\over 2N} k \bigl( N + 2 -1 \bigr) \Bigr) \over \exp\Bigl( i {\pi\over N} k \Bigr) - \exp\Bigl(-i{\pi\over N} k \Bigr) } \\
  &= { \exp\Bigl( i{\pi\over N} k \bigl( N - 1 \bigr) \Bigr) - \exp\Bigl( -i{\pi\over N} k \bigl( N + 1 \bigr) \Bigr) \over \exp\Bigl( i {\pi\over N} k \Bigr) - \exp\Bigl(-i{\pi\over N} k \Bigr) } \\
\intertext{However, the numerator does not simplify to the $\sin$ function as is - there is a phase factor that needs to be factored out}
  &= { \exp\Bigl( - i {\pi\over N} k \Bigr) \biggl( \exp\Bigl( i{\pi\over N} k ( N ) \Bigr) - \exp\Bigl( -i{\pi\over N} k ( N ) \Bigr) \biggr) \over \exp\Bigl( i {\pi\over N} k \Bigr) - \exp\Bigl(-i{\pi\over N} k \Bigr) } \\
  &= e^{-i\pi k / N} { \sin(\pi k) \over \sin(\pi k / N) }
\intertext{****The OLD****}
  &= { \exp\Bigl(-i{2\pi\over N} k \bigl(-{(N-1)\over2}\bigr)\Bigr) - \exp\Bigl(-i{2\pi\over N} k \bigl( {(N-1)\over2} + 1 \bigr) \Bigr) \over 1 - \exp\Bigl(-i{2\pi\over N} k \Bigr) } \\
  &= { \exp\Bigl(i{2\pi\over N} k \bigl({N-1\over2}\bigr)\Bigr) - \exp\Bigl(-i{2\pi\over N} k \bigl({N+1\over2}\bigr) \Bigr) \over 1 - \exp\Bigl(-i{2\pi\over N} k \Bigr) } \\
  &= { \exp\Bigl(i{\pi\over N} k (N-1)\Bigr) - \exp\Bigl(-i{\pi\over N} k (N+1) \Bigr) \over 1 - \exp\Bigl(-i{2\pi\over N} k \Bigr) } \\
  &= { \exp\Bigl( -i{\pi\over N} k \Bigr) \biggl( \exp\Bigl(i{\pi\over N} k (N) \Bigr) - \exp\Bigl(-i{\pi\over N} k (N) \Bigr) \biggr) \over \exp\Bigl(-i{\pi\over N} k \Bigr) \biggl( \exp\Bigl(i{\pi\over N} k \Bigr) - \exp\Bigl(-i{\pi\over N} k \Bigr) \biggr) } \\
  &= { \exp\bigl( i\pi k \bigr) - \exp\bigl( -i\pi k \bigr) \over \exp\Bigl(i{\pi\over N} k \Bigr) - \exp\Bigl(-i{\pi\over N} k \Bigr) } \\
  &= { \exp\bigl( i\pi k \bigr) - \exp\bigl( -i\pi k \bigr) \over \exp\bigl(i\pi k / N \bigr) - \exp\bigl(-i\pi k / N \bigr) } \\
\intertext{Using $\qquad\qquad \boxed{ \sin\theta = { e^{i\theta} - e^{-i\theta} \over i2 } }$}
  &= { \Bigl( \exp\bigl( i\pi k \bigr) - \exp\bigl( -i\pi k \bigr) \Bigr) / i2 \over \Bigl( \exp\bigl(i\pi k / N \bigr) - \exp\bigl(-i\pi k / N \bigr) \Bigr) / i2 } \\
  &= { \sin(\pi k) \over \sin(\pi k / N) }
\end{align*}



  \ul{The DFT of the asymmetric boxcar window} ...

\begin{center}
\begin{tikzpicture}
\node[anchor=south] (note) at (0, 2) {($N=8$ on a symmetric grid)};
% horizontal labels
\draw[->] (-3,0) node[anchor=north, yshift=-1ex] {$0$}
	  -- (4,0) node[anchor=north, yshift=-1ex] {${(N\!-\!1)}$}
	  -- (6,0) node[anchor=west] {$[n]$};

% Dirac functions
\draw[-, very thick] (-3,0) -- (-3,1.5);
\draw[-, very thick] (-2,0) -- (-2,1.5);
\draw[-, very thick] (-1,0) -- (-1,1.5);
\draw[-, very thick]  (0,0) --  (0,1.5);
\draw[-, very thick]  (1,0) --  (1,1.5);
\draw[-, very thick]  (2,0) --  (2,1.5);
\draw[-, very thick]  (3,0) --  (3,1.5);
\draw[-, very thick]  (4,0) --  (4,1.5);
\draw[-, very thick]  (4,0) --  (4,1.5);
\draw[-, very thick, opacity=0.3]  (5,0) --  (5,1.5);
\filldraw[black] (-3,1.5)  circle (.1);
\filldraw[black] (-2,1.5)  circle (.1);
\filldraw[black] (-1,1.5)  circle (.1);
\filldraw[black]  (0,1.5)  circle (.1);
\filldraw[black]  (1,1.5)  circle (.1);
\filldraw[black]  (2,1.5)  circle (.1);
\filldraw[black]  (3,1.5)  circle (.1);
\filldraw[black]  (4,1.5)  circle (.1);
\filldraw[black, opacity=0.3]  (5,1.5)  circle (.1);

\end{tikzpicture}
\end{center}

  (TO BE CONTINUED...)


  \section{The DFT of $\Sh$ - the Discrete Sampling Theorem}

  Ahead: why it's important to use $1/2$ at the discontinuities in the discrete boxcar function ($\alpha=1/2$ in Osgood text).

  class of signals: periodic, bandlimited ($F[k]=0$ for $|k|\ge (b=\Omega)/2$), length $N$ (indiced from $-N/2+1: N/2$

  In discrete-land functions are more like vectors consisting of $N$ samples of the function.
  It is important to remember that the elements of this vector themselves are indexed from $0$, which indicates the starting point or initial value/condition as no progress has been made at the beginning of the process being observed.


  (...)
  Ingredients for the Discrete Sampling and Interpolation Theorem
\begin{enumerate}
  \item  discrete bandlimited functions of the DFT are
  \begin{itemize}
	\item periodic
	\item N samples (even, indices $[-N/2+1:N/2]$)
	\item $\mathcal{F}f[k] = 0 \qquad\text{for } |k| = w = \Omega/2$ 
  \end{itemize}
  \item discrete Shah $\Sh$
	- spacing distance, $d$, between sampling spikes, where $N$ is divisible by $d$, therefore $d$ is even also.
\end{enumerate}
  
  \subsection{The Discrete Shah Function (does this need to be moved up?)}

  The definition for the DFT selected in this section is 
  $$ F[k] = \mathcal{F}\!f = \sum_n f[n] \, w_N^{-k n} \qquad\text{and}\qquad f[n] = \mathcal{F}^{^{-1}}\!F = {1\over N} \sum_k F[k] \, w_N^{k n} $$

  (Show others' definitions and ask How to denote the discrete Shah function? Ans: denote discreteness with square-bracket notation)

  Notice, the argument in square brackets of the DFT and IDFT is not the summation index.
  In the definition of the convolution the argument is also not the summation index.
  Fair enough, these are transforms, which take functions of one variable and produce functions of another variable.
  The $\Sh$ function might be though of as a transform, but it is also natural to think of it as simply a collection of delta functions.
  Regardless of how we chose to think about $\Sh$, the notation used to denote $\Sh$ should facilitate working with this object.
  Specifically, the DFT of the Shah function, $\mathcal{F}\Sh$, should be natural to write and compute.
  The DFT operates on signals that are a function of $[n]$ so it would be nice to be able to write, $\mathcal{F} (\Sh[n])$, in the same way that it is natural to write, $\mathcal{F}(f[n])$.
  The $\Sh$ can certainly be a function of $[n]$ as it can exist on the $[n]$ grid but the question arrises, what does $[n]$ refer to in the argument of the $\Sh$ function if not to the summation index?
  I would like to stick with the convention of not specifying the summation index as the argument in the case of defining a transform.
  In the discrete context, transforms involve summation over an index; the $\Sh$ is weird in that it is essentially a function that involves a summation in its definition.
  I think the solution to this is to think of $\Sh$ as a transform.
  The $\Sh$ is essentially a tool for sampling a function by targeting the index on which it resides.

  In $\delta[n]$ the argument $[n]$ is largely superfluous, other than it specifies the grid axis
\begin{center}
\begin{tikzpicture}
\draw (1,1.5) node{$\delta[n]$};
% horizontal labels
\draw[->]
   	(-2,0) node[anchor=east] {$[n]$}
 --	(-1,0) node[] {}
 -- (0,0) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (1,0) node[] {}
 --	(2,0);
\draw[->, very thick] (0,0) -- (0,1);
\end{tikzpicture}
\end{center}

  It is tricky to define the discrete Shah function due to the odd symmetry of the Kronecker delta (multiplying its argument by $(-1)$ has no effect on the function/outcome)
  $$\delta[n-a] = \delta[a-n]$$
  Notice, $(a)$ is deterministic, for $(a) > 0$

\begin{center}
\begin{tikzpicture}
\draw (-2,1.5) node{$\delta[n-a]$};
\draw (3,1.5) node{$\delta[a-n]$};
% horizontal labels
\draw[->]
   	(-5,0) node[anchor=east] {$[n]$}
 --	(-4,0) node[] {$|$}
 -- (-4,0) node[anchor=north, yshift=-1.5ex] {$0$}
 -- (-2,0) node[anchor=north, yshift=-1.5ex] {$(a)$}
 --	(-1,0);
%\draw[->, very thick] (-4,0) -- (-4,1);
\draw[->, very thick] (-2,0) -- (-2,1);
\draw[->]
   	 (0,0) node[] {$ $}
 --	 (1,0) node[] {$|$}
 --  (1,0) node[anchor=north, yshift=-1.5ex] {$0$}
 --  (2,0) node[anchor=north, yshift=-1.5ex] {$ $}
 --  (3,0) node[anchor=north, yshift=-1.5ex] {$(a)$}
 --  (4,0);
%\draw[->, very thick] (1,0) -- (1,1);
\draw[->, very thick] (3,0) -- (3,1);
% Discrete Shaw
\end{tikzpicture}
\end{center}

  For, $(a) < 0$
\begin{center}
\begin{tikzpicture}
\draw (-2,1.5) node{$\delta[n-a]$};
\draw (3,1.5) node{$\delta[a-n]$};
% horizontal labels
\draw[<-]
   	(-5,0) 
 -- (-4,0) node[anchor=north, yshift=-1.5ex] {$(a)$}
 --	(-2,0) node[] {$|$}
 -- (-2,0) node[anchor=north, yshift=-1.5ex] {$0$}
 --	(-1,0);
\draw[->, very thick] (-4,0) -- (-4,1);
%\draw[->, very thick] (-2,0) -- (-2,1);
\draw[<-]
   	 (0,0) node[] {$ $}
 --  (1,0) node[anchor=north, yshift=-1.5ex] {$(a)$}
 --	 (3,0) node[] {$|$}
 --  (3,0) node[anchor=north, yshift=-1.5ex] {$0$}
 --  (4,0) node[anchor=west] {$[n]$};
\draw[->, very thick] (1,0) -- (1,1);
%\draw[->, very thick] (3,0) -- (3,1);
% Discrete Shaw
\end{tikzpicture}
\end{center}

  Essentially it would be easier and just as clear to simply define the Kronecker delta without any reference to the grid axis, ie: $\delta[0]$, $\delta[a]$, and $\delta[-a]$.
  However, this solution for simplifying the notation for the Kronecker delta canot be extended to $\Sh$, which must reference an index to generate the spike train of the Shah function.

  The first question is, should the index reference the grid axis directly of indirectly via an alias?
  Secondly, how does the symmetry of the Kronecker delta influence this choice?
  I want to make a case for defining the Shah function as $\Sh[n]$, where $[n]$ is not the summation index but rather specifies the index of the target function, which will be swapped out for the index used to define $\Sh$.
  In this construction, the summation index used to define $\Sh$ is an alias for the index/grid axis it ultimately targets.

  The Shah function can be written as 
  $$\Sh_d[m] = \sum_n \delta[m-nd]$$
  From this perspective the index is moving along its grid, and $[m]$ is a dummy index that only references the underlying grid associated with $[n]$ via its juxtaposition to $[n]$.
  (Note: maybe using the index $[n]$ will clash with other operators, ie: the DFT, use of the same index in their natural definition...)

  It is entirely equivalent to represent the Shah function can be written as 
  $$\Sh_d[n] = \sum_m \delta[n-md]$$
  In this notation, $[m]$ is a dummy index used to explicitly sample the $[n]$ index and by direct proxy its underlying grid.

  Both of these conventions do not reference the summation index used in their definition, keeping with the convention for convolution.
  However, $\Sh[n]$, presents an opportunityh to present a notational lure for the index/grid we want to target by hooking into another ajoining summation.
  According to this view, the Shah function, $\Sh[n]$, is primarily an object used to supplant the target index, $[n]$, of another function, ie: $f[n]$, with its own, the dummy index specified in the summation definition for the Shah function.

  In comparison to the continuous case, $\Sh$, where $d$ is the distance between spikes/samples, 
  $$ \Sh_d(x) = \sum_{n=-\infty}^{\infty} \delta(x - n d) $$
  the index $n$ is used to iterate over all integers ($ n \in \mathbb{Z} $).
  The variable $x$ refers to the actual axis/dimension itself, whereas the index $n$ is a dummy variable used to reference points along the axis.
  In contrast, for the DFT, the variable $n$ 
  $$ \mathcal{F}[k] = \sum_n^{(N)} e^{-i2\pi k n / N} $$
  {\em does} refer directly to the (spatial/temporal) axis/dimension itself.

  To maintain the use of a dummy variable to select points along the axis means keeping the reference to the physical dimensions in tact, $n$ refers to $x$.
  So, the discrete Shah function needs a new dummy index to replace the $n$ used in the continuous case to select points from the temporal/spatial grid where, ie: $ n \in [0:N-1] $.
  This leads to the discrete Shah function using the dummy index, ie: $[m]$, in the summation definition


  By comparison to the continuous case, $\Sh$, where $d$ is the distance between spikes/samples, 
  $$ \Sh_d(x) = \sum_{n=-\infty}^{\infty} \delta(x - n d) $$
  the index $n$ is used to iterate over all integers ($ n \in \mathbb{Z} $).
  The variable $x$ refers to the actual axis/dimension itself, whereas the index $n$ is a dummy variable used to reference points along the axis.
  In contrast, for the DFT, the variable $n$ 
  $$ \mathcal{F}[k] = \sum_n^{(N)} e^{-i2\pi k n / N} $$
  {\em does} refer directly to the (spatial/temporal) axis/dimension itself.

  To maintain the use of a dummy variable to select points along the axis means keeping the reference to the physical dimensions in tact, $n$ refers to $x$.
  So, the discrete Shah function needs a new dummy index to replace $n$ in the continuous case to select points from the temporal/spatial grid where, ie: $ n \in [0:N-1] $.
  This leads to the discrete Shah function using the dummy index, ie: $[m]$, in the summation definition
  $$ \Sh_d[n] = \sum_{m=0}^{N/d-1} \delta[n - m d] $$
  to index over the grid set, $\{n\}$, of $N$ consecutive integers.
  The implications of this construction are subtle but profound from the perspective of how this conceptualizion translates into practical implementation.
  Using this formulation results in the Sha function effectively supplanting the index of a (discrete) function with its own.
  For example, the grid set $[n]$ of the DFT is targeted by $\Sh_d[n]$ in
  $$ \mathcal{F} \Sh_d[n] = \sum_n w_N^{-k n} \sum_m \delta[n - m d] $$
  Gathering terms then switching the order of summation
  $$= \sum_m \sum_n w_N^{-k n} \delta[n - m d] $$
  And performing the sum over $n$
  $$= \sum_m w_N^{-k (m d)} $$
  gives the DFT in terms of a new $[m]$-index with an interval ($d$).
  While the name of the index has changed, it is arbitrary and can be changed to suit the reference, including back to the previous grid reference.

  The afformentioned paradigm of supplanting the index is in contrast to the Sha function as essentially sampling along an index
  $$ \Sh_d[m] = \sum_n \delta[m - n d] $$
  where the summation retains the $n$-index and $[m]$ is a dummy index that only references the grid via juxtaposition to $[n]$.
  In practice this leads to statements like the DFT of the Shah function is
  $$ \mathcal{F} \Sh_d = \sum_n w_N^{-k n} \sum_n \delta[m - n d]$$
  And then assert that the right most sum produces
  $$= \sum_n w_N^{-k (m = nd)} $$
  
  
  Which convention, $\Sh[n]$ v. $\Sh[m]$, I will adopt is yet to be determined.
  Ultimately, provided $N$ is divisible by $d$
  $$\Sh_d = \begin{cases}
		1 & \quad\text{$m = 0,d,2d,\ldots,(N/d-1)$} \\
	   	0 & \quad\text{else}
	\end{cases}
  $$
  , which is the $N$-periodic (and $N/d$-periodic) sequence, ie:
\begin{center}
\begin{tikzpicture}
% horizontal labels
\draw[->]
   	(-5,0) node[] {$|$}
 --	(-5,0) node[anchor=north, yshift=-1.5ex] {$0$}
 --	(-5,0) node[anchor=north, yshift=-4ex] {$(0)d$}
 --	(-4,0) node[] {$|$}
 -- (-4,0) node[anchor=north, yshift=-1.5ex] {$1$}
 --	(-3,0) node[] {$|$}
 -- (-3,0) node[anchor=north, yshift=-1.5ex] {$2$}
 --	(-2,0) node[] {$|$}
 -- (-2,0) node[anchor=north, yshift=-1.5ex] {$3$}
 --	(-2,0) node[anchor=north, yshift=-4ex] {$(1)d$}
 --	(-1,0) node[] {$|$}
 -- (-1,0) node[anchor=north, yshift=-1.5ex] {$4$}
 --	 (0,0) node[] {$|$}
 --  (0,0) node[anchor=north, yshift=-1.5ex] {$5$}
 --	 (1,0) node[] {$|$}
 --  (1,0) node[anchor=north, yshift=-1.5ex] {$6$}
 --	 (1,0) node[anchor=north, yshift=-4ex] {$(2)d$}
 --	 (2,0) node[] {$|$}
 --  (2,0) node[anchor=north, yshift=-1.5ex] {$7$}
 --	 (3,0) node[] {$|$}
 --  (3,0) node[anchor=north, yshift=-1.5ex] {$8$}
 --  (4,0);
\draw (-7,0.75) node{$\Sh_{d=3}$};
\draw (-8,0) node{$\text{No. Samples, } N=9$};
% Discrete Shaw
\draw[->, very thick] (-5,0) -- (-5,1);
\filldraw[black] (-4,0)  circle (.1);
\filldraw[black] (-3,0)  circle (.1);
\draw[->, very thick] (-2,0) -- (-2,1);
\filldraw[black] (-1,0)  circle (.1);
\filldraw[black] (0,0)  circle (.1);
\draw[->, very thick] (1,0)  -- (1,1);
\filldraw[black] (2,0)  circle (.1);
\filldraw[black] (3,0)  circle (.1);
\end{tikzpicture}
\end{center}

  To be pedantic without pedagogy, it is nice to represent the delta functions as arrows, and to think of them as such so that you are not tempted to actually perform the sum when thinking about the Shah $\Sh$ function, and rather to use the $\Sh$ function for what it is meant to do, which is sample another function.
  But in the discrete context it is also accurate and sometimes useful to simply regard the delta functions/spikes as 1's

\begin{center}
\begin{tikzpicture}
% horizontal labels
\draw[->]
   	(-5,0) node[] {$|$}
 --	(-5,0) node[anchor=north, yshift=-1.5ex] {$0$}
 --	(-5,0) node[anchor=north, yshift=-4ex] {$(0)d$}
 --	(-4,0) node[] {$|$}
 -- (-4,0) node[anchor=north, yshift=-1.5ex] {$1$}
 --	(-3,0) node[] {$|$}
 -- (-3,0) node[anchor=north, yshift=-1.5ex] {$2$}
 --	(-2,0) node[] {$|$}
 -- (-2,0) node[anchor=north, yshift=-1.5ex] {$3$}
 --	(-2,0) node[anchor=north, yshift=-4ex] {$(1)d$}
 --	(-1,0) node[] {$|$}
 -- (-1,0) node[anchor=north, yshift=-1.5ex] {$4$}
 --	 (0,0) node[] {$|$}
 --  (0,0) node[anchor=north, yshift=-1.5ex] {$5$}
 --	 (1,0) node[] {$|$}
 --  (1,0) node[anchor=north, yshift=-1.5ex] {$6$}
 --	 (1,0) node[anchor=north, yshift=-4ex] {$(2)d$}
 --	 (2,0) node[] {$|$}
 --  (2,0) node[anchor=north, yshift=-1.5ex] {$7$}
 --	 (3,0) node[] {$|$}
 --  (3,0) node[anchor=north, yshift=-1.5ex] {$8$}
 --  (4,0);
\draw (-7,0.75) node{$\Sh_{d=3}$};
\draw (-8,0) node{$\text{No. Samples, } N=9$};
% Discrete Shaw
\draw[-, very thick] (-5,0) -- (-5,1);
\filldraw[black] (-5,1)  circle (.1);
\filldraw[black] (-4,0)  circle (.1);
\filldraw[black] (-3,0)  circle (.1);
\draw[-, very thick] (-2,0) -- (-2,1);
\filldraw[black] (-2,1)  circle (.1);
\filldraw[black] (-1,0)  circle (.1);
\filldraw[black] (0,0)  circle (.1);
\draw[-, very thick] (1,0)  -- (1,1);
\filldraw[black] (1,1)  circle (.1);
\filldraw[black] (2,0)  circle (.1);
\filldraw[black] (3,0)  circle (.1);
\end{tikzpicture}
\end{center}
  , for example (ahead) when interpreting the DFT of the $\Sh$ function.


  ULTIMATELY:
  The $\Sh$ function exists on a grid/axis associated with a specific index.
  The $\Sh$ function employs a dummy index to access this grid.
  ...perhaps worth noting that the dummy index can ultimately be replaced/renamed, ie: to reference the grid.


  \ul{\bf Sampling Property of the Discrete Shah Function}

  Recall, in the continuous case, $\Sh(x)$ can be thought of as convolution with the delta function 
  $$ (f(x) * \delta(x)) = \sum_n f(x) \delta(n - x) = f[n] = f \cdot \Sh $$
  to produce a discretized version of the original function.
  
  In the discrete context, $\Sh$ switches out one index for another, ie: converts a function of $[n]$ to a function of $[\ell]$ (the summation index used to define $\Sh$)
\begin{align*}
  f[n] \cdot \Sh_d[n] &= f[n] \cdot \sum_{\ell} \delta[n - d \ell] \\
  &=\sum_{\ell=0}^{N/d-1} f[n] \, \delta[n - d \ell] = f[d \ell]
\end{align*}
  Written like this, $\Sh[n]$, suggests the Shah function is targeting the $n$-index.
  The discrete Shah function, $\Sh[n]$, samples the function, $f[n]$, at scaled intervals along the grid axis, $n = d \ell$, supplanting the $n$-index with a dummy index, $[d \ell]$, where the $[\ell]$-index is an alias for the $[n]$-index.


  \ul{\bf Periodization Property of the Discrete Shah Function}%Brad Osgood's treatment of this subject is rough, see: convolution of $f[n]$ with the discrete Shah function, p.447 \cite{Osgood}

  The convolution takes place between two functions of the same variable and returns a new function of a different variable.
  Recall the definition of the convolution of $f[n]$ and $g[n]$
  $$\boxed{(f*g)[m] \equiv \sum_n f[n]\,g[m-n]} $$
  where $[m]$ is a dummy index that references the same grid axis as the $[n]$-index via its juxtaposition to the $[n]$-index (but maybe this is purely the default option and $[m]$ could in principle refer to something else?).
  The $[n]$-index typically corresponds to the spatial/temporal grid but the convolution can be applied on any grid.
  Notice, the argument to the convolution is not the summation index while referring to the same axis.

  The Shah function, $\Sh_d[n]$, where $(d)$ is some distance or duration
  $$ \Sh[n] = \sum_\ell \delta[n - \ell d] $$
  targets the $[n]$-index.

  The convolution of $f[n]$ and $\Sh[n]$ is
\begin{align*}
  (f*\Sh)[m] &= \sum_n f[n]\,\Sh[m-n] \\
  &= \sum_n f[n]\,\biggl( \sum_\ell \delta[ (m - n) - \ell d ] \biggr) \\
  &= \sum_n \sum_\ell f[n] \, \delta[ (m - n) - \ell d ] \\
\intertext{Switching the order of summation, and performing the sum over the $[n]$-index}
  &= \sum_\ell \sum_n f[n] \, \delta[ (m - \ell d) - n ] \\
  &= \sum_\ell f[m - \ell d] \\
\intertext{Note the restriction on the summation index upon swapping out $[n]$ for $[\ell]$}
  &= \sum_\ell \sum_{n=0}^{N-1} f[n] \, \delta[ (m - \ell d) - n ] \\
  &= \sum_{\ell=0}^{N/d-1} f[m - \ell d]
\end{align*}
  Recall the $[\ell]$-index is a dummy index and references the grid axis via its juxtaposition to the target index.
  At the end of the derivation it is appropriate to abandon this semantic exercise and reference the grid axis directly using its 


  Let's see what happens when the index used to iterate over the grid axis is used as the summation index
  $$ \Sh[\ell] = \sum_n \delta[\ell - n d] $$
  in the definition of the Shah function. 
  This presents some ambiguity in defining the convolution as now the convolution, $(f * \Sh)[m]$, must take place between two functions that are not of the same variable, $f[n]$ and $\Sh[\ell]$
  Given the definition of the convolution
  $$ (f[n]*\Sh[\ell])[m] = \sum_n f[n]\,\Sh[m-n] $$
  it is not clear how to substitute the variable, $(\ell)$, of $\Sh[\ell]$, into this expression.
  This predicament motivates a break with convention and refer to the summation index in the argument to the Shah function; in obvious contrast to the definition of the convolution which is notated as a function of $[m]$ while the summation takes place over $[n]$; but at least now the convolution can be written in the natural way
  $$ (f[n]*\Sh[n])[m] = \sum_n f[n]\,\Sh[m-n] $$

  The next question that inevitably arrises, is how to substitute $[m-n]$ into the Shah function?
  A possible substitution might be
  $$ (f*\Sh)[m] = \sum_n f[n]\,\biggl( \sum_n \delta[\ell - (m-n) d] \biggr) $$
  Another possibility might be
  $$ (f*\Sh)[m] = \sum_n f[n]\,\biggl( \sum_n \delta[m - \ell + n d] \biggr) $$
  but either option would make the Shah function so overly complicated that it is no longer facile to use.

  All this ambiguity is before even getting to the issue of there being redundant summations involving the $[n]$-index.
  After the first summation is performed the function that remains is no longer a function of $[n]$ but of $[m]$ and $[\ell]$.
  There is no way to resolve what remains: a summation over $[n]$ involving a function of $[m]$ and $[\ell]$.

  This is the core context in which it becomes very beneficial to adopt my notation and construction of the Shah function!!!
  $$ \Sh[n] = \sum_\ell \delta[n - \ell d] $$
  This definition keeps with the convention that is already in place for defining other objects that exist on a specific grid, ie: the DFT, and the convolution, where the argument does not not reference the summation index.
  Furthermore, the construction adopted here resembles the classical/continuous definition of the Shah function
  $$\Sh(x) = \sum_n \delta(x - nd)$$ 
  where the summation index, $(n)$, is a dummy index that only references the $(x)$-axis via juxtaposition.
  In the discrete context, $[n]$, refers directly to the grid axis, and $[\ell]$ is the dummy index that can be scaled to select points at regular intervals.
  The variable, $(\ell)$, is chosen to define the dummy index in the summation of the discrete Shah function because it resembles a spike; $(\ell)$ is better than $(l)$ because it reminds you that the Shah function is involved; but use whatever you want.
  The Shah function written this way differs from objects like the DFT and the convolution in that, in contrast these objects, the Shah function does not involve a summation over the grid axis.
  
  According to my construction the Shah function should be thought of as targeting a specific grid, which is specified by its argument.
  An important feature of this construction is that an accompanying summation is needed for $\Sh[n]$ to reach its target.
  Think of using the target grid axis as a lure in the definition of $\Sh$ that ultimately hooks into the summation coming from another object, ie: the DFT, or the convolution.
  By performing the summation coming from an auxiliary function, the Shah function effectively supplants the index with the dummy index.
  Ultimately, while the survival of this dummy index presents a source of complexity, it carries the identity of the axis it targets, making it trivial to rename in the end.


  \ul{\bf The DFT of the Discrete Shah Function}
  %NOTE: used mathcal font a lot in section on distributions before and might need to see how updated font looks...
  (...Osgood notation sux...adapt yours!...)

  THINK: it's time to get some clarity about the choice of notation...
  Although, the DFT is defined as a series over $[n]$ it is notated as a function of $[k]$, as in $\mathcal{F}[k] = \sum_n e^{-i2\pi kn/N}$ 

  *****COME BACK: FIX to include proper definition of $\Sh$ and be explicit with def of DFT...

  The DFT of the discrete Shah function is
\begin{align*}
  (\mathcal{F}\,\Sh_d[m])[k] &= \sum_{n=0}^{N/d-1} e^{-i2\pi k n / N} \, \delta[m-nd] \\
\intertext{The exponential only pops up on $nd$ intervals}
  &= \sum_{n=0}^{N/d-1} e^{-i2\pi k (n d) / N} \\
  &= \sum_{n=0}^{N/d-1} \bigl(e^{-i2\pi k d / N}\bigr)^n \\
\intertext{For $kd=0 \text{ mod } N$, and for $k=0 \text{ mod } N/d$, for ie: $k=0, N/d, 2N/d, 3N/d,\ldots \in [0:N-1]$}
  (\mathcal{F}\,\Sh_d)[k=0 \text{ mod } N/d] 
  &= \sum_{n=0}^{N/d-1} w_N^{- (k=zN/d) d n} \qquad\text{, where $z\in\mathbb{Z}$} \\
  &= \sum_{n=0}^{N/d-1} (1)^n
  = {N\over d} \\
\intertext{For $kd \ne 0 \text{ mod } N$, for and $k \ne 0 \text{ mod } N/d$}
  &= \sum_{n=0}^{N/d-1} w_N^{- k d n}
  = {w^0 - w^{- k d (N / d)} \over 1 - w^{- k d}} \\
  &= {1 - e^{-i2\pi k d / N (N/d) } \over 1 - e^{-i2\pi k d / N}}
  = {(1 - e^{-i2\pi k } = 0) \over 1 - e^{-i2\pi k d / N}} \\
\intertext{Collecting these cases the DFT of the Shah function of spacing $(b)$ is}
  (\mathcal{F}\,\Sh_d)[k]
  &= \begin{cases}
		N/d & \quad\text{for $k=0$ mod $N/d$} \\
		0	& \quad\text{for $k\ne0$ mod $N/d$}
     \end{cases}
\end{align*}

  , where $k$ is any set of $N$ consecutive integers.
  The result is a uniform set of $(N/d)$-magnitude values spaced at regular $(N/d)$-sized intervals along the axis, $[k]$.
  While not formally defined as a spike train of Kronecker deltas, this function is nontheless in essence the discrete Shah function that has been scaled both vertically and horizontally (analagous to the continuous case ref. formula)

\begin{equation}
  \boxed{
  (\mathcal{F}\,\Sh_d)[k]
  = \biggl({N\over d}\biggr) \, \Sh_{(N/d)}
  }
\end{equation}


  \ul{\bf The IDFT of the Discrete Shah Function}

  COME BACK: use duality (and the symmetry of the Shah function)
  $$ \mathcal{F}f^{(-)} = (N) \, (\mathcal{F}^{^{-1}}\!f) $$
  $$ \mathcal{F}^{^{-1}}\!f = {1\over N}\mathcal{F}f^{(-)} $$
  ...

  $$ (\mathcal{F}^{^{-1}}\,\Sh_d[\ell])[n] = {1\over N}\sum_{k=0}^{N/b-1} e^{+i2\pi k n / N} \, \delta[\ell-kb] $$
  - use the subscript $b$ for the discrete Shah function in the frequency domain to suggest some ``(b)andwidth'' between samples.
  $$\Sh[\ell]_b = \sum_{k=0}^{N/b-1} \delta[\ell - k b]$$


  $$ \mathcal{F}^{^{-1}} \Sh_d = {1\over N} \mathcal{F} \Sh_d^{(-)} $$
  ...By the symmetry and cyclic periodidcity of $\Sh$...
  $$ = {1\over N} \mathcal{F} \, \Sh_d $$
  $$ = {1\over N} \biggl({N\over d}\biggr) \, \Sh_{(N/d)} $$
  $$ = \biggl({1\over d}\biggr) \, \Sh_{(N/d)} $$


  \subsection{Derivation of the Discrete Sampling Theorem}

  Using the definitions for the DFT and the IDFT
  $$ F[k] = \mathcal{F}\!f = \sum_n f[n] \, w_N^{-k n} \qquad\text{and}\qquad f[n] = \mathcal{F}^{^{-1}}\!F = {1\over N} \sum_k F[k] \, w_N^{k n} $$

  Following the derivation of the classical sampling theorem, the DFT is equivalent to
  $$ \mathcal{F}\!f = \Pi_\Omega \, ( \mathcal{F}\!f * \Sh_\Omega ) $$
  the periodization of the DFT, truncated by a boxcar function at the bandwidth.
  To recover the signal, take the IDFT 
\begin{align*}
  f &= \mathcal{F}^{^{-1}} \bigl( \Pi_\Omega \, ( \mathcal{F}\!f * \Sh_\Omega ) \bigr) \\
\intertext{By the convolution theorem for the afformentioned definitions of the DFT pair $$ \mathcal{F}^{^{-1}}(f\,g) = \bigl(\mathcal{F}^{^{-1}}\!f\bigr)*\bigl(\mathcal{F}^{^{-1}}\!g\bigr) $$
  the IDFT of the product is equal to the convolution of the IDFTs}
  &= \mathcal{F}^{^{-1}} ( \Pi_\Omega ) * \mathcal{F}^{^{-1}} ( \mathcal{F}\!f * \Sh_\Omega ) \\
\intertext{
  By (discrete) duality
  $$ \mathcal{F}^{^{-1}}(f^{(-)}) = {1\over N} \, (\mathcal{F}f) $$
  and because $\Pi_\Omega$ is symmetric, the IDFT of the boxcar function is
  $$ ( \mathcal{F}^{^{-1}} \Pi_\Omega )[n] = {1\over N} \, \mathcal{F} \, \Pi_\Omega^{(-)} = {1\over N} \, \mathcal{F} \, \Pi_\Omega = {1\over N} \mathscr{B}_\Omega[n] $$
%\iffalse
  NOTE: 
  - (p.449\cite{Osgood}) Osgood claims this is where the reasoning comes in for using the convention of modeling the discontinuities / setting the edges of the boxcar to $1/2$; so that a factor of $p$ becomes available (here, equivalent to $\Omega$) to cancel with a factor of $p$ that comes from the expression right of the convolution sign involving the DFT of $\Sh_\Omega$.
  - In my derivation this factor is absent...so it seems appropriate to use which ever version of the boxcar function.
  - maybe instead of trying to write a discrete version of the $\sinc$ function, a better convention/notation would be to refer to a generic DFT of the boxcar function using a notation like, ie: $\mathcal{B}$, $\mathscr{B}$, $\mathfrak{B}$.
%\fi
  By the (discrete) convolution theorem
  $$ \mathcal{F}^{^{-1}}\!(f) \, \mathcal{F}^{^{-1}}\!(g) = {1\over N} \mathcal{F}^{^{-1}} (f*g) $$
  the IDFT of the DFT convolved with $\Sh_\Omega$ - the right side of the outer convolution sign - is
  $$ \mathcal{F}^{^{-1}} ( \mathcal{F}\!f * \Sh_\Omega ) = (N) \Bigl( \mathcal{F}^{^{-1}} ( \mathcal{F}\!f ) \Bigr) \Bigl( \mathcal{F}^{^{-1}} ( \Sh_\Omega ) \Bigr)
  = ( N ) f[n] \cdot \bigl( \mathcal{F}^{^{-1}} \Sh_\Omega \bigr)[n] $$
  where the IDFT of the $\Sh$ function is notated explicitly as a function of $[n]$.
}
\intertext{Rewritting the signal construction as a convolution based on the two aformentioned formulas}
  f &= \Bigl( {1\over N} \mathscr{B}_\Omega \Bigr) * \Bigl( (N) \, f \cdot \bigl( \mathcal{F}^{^{-1}} \Sh_\Omega \bigr) \Bigr) \\
  &= \mathscr{B}_\Omega * \Bigl( f \cdot \bigl( \mathcal{F}^{^{-1}} \Sh_\Omega \bigr) \Bigr) \\
\intertext{Writting the IDFT of the $\Sh$}
  &= \mathscr{B}_\Omega * \Bigl( f \cdot {1\over \Omega} \Sh_{(N/\Omega)} \Bigr) \\
\intertext{By the commutativity of the convolution}
  &= {1\over\Omega} \bigl( f \cdot \Sh_{(N/\Omega)} \bigr)[n] * \mathscr{B}_\Omega[n] \\
\intertext{Under the given definitions of the DFT pair, the formula for the discrete convolution is $$ (f*g)[m] = f[n] * g[n] = \sum_n f[n]\,g[m-n] $$ where the dummy index $[m]$ references the same grid as $[n]$ through its juxtaposition to $[n]$.  Notice, written out, the signal is a function of the dummy variable, $[m]$, used to define the convolution}
  f[m] &= {1\over\Omega} \, \sum_n \, \bigl( f \cdot \Sh_{(N/\Omega)} \bigr)[n] \,\, \mathscr{B}_\Omega[m-n] \\
  &= {1\over\Omega} \, \sum_n \biggl( f \cdot \sum_\ell \delta\Bigl[n - {N\over\Omega}\ell\Bigr] \biggr) \, \mathscr{B}_\Omega[m - n] \\
  &= {1\over\Omega} \, \sum_\ell \sum_n f[n] \, \mathscr{B}_\Omega[m - n] \, \delta\Bigl[n - {N\over\Omega} \ell \Bigr] \\
\intertext{Performing the summation over $[n]$ swaps out the index $[n]$ for $[(N / \Omega) \ell]$}
  &= {1\over\Omega} \, \sum_\ell \, f\Bigl[\, {N\over\Omega} \, \ell \,\Bigr] \, \mathscr{B}_\Omega\Bigl[\,m - {N\over\Omega}\,\ell\,\Bigr] \\
\intertext{and $(m)$ is no longer a simple index because of its juxtaposition now with $[(N / \Omega) \ell]$, formerly $[n]$.}
\intertext{The $[\ell]$-index was used by $\Sh$ to target the $[n]$-index, so at this stage the index $[\ell]$ can be swapped out for its alias, $[n]$}
  f[m] &= {1\over\Omega} \, \sum_n \, f\Bigl[\, {N\over\Omega} \, n \,\Bigr] \, \mathscr{B}_\Omega\Bigl[\,m - {N\over\Omega} \, n \,\Bigr]
\end{align*}
  The variable $(m)$ can be recast as an integer index $[m]$ by recognizing that the signal reconstruction is a function of the quantity $[(N/\Omega) m ]$ 
\begin{equation}
  \boxed{
  f\Bigl[{N\over\Omega}\, m \,\Bigr] = {1\over\Omega} \, \sum_n \, f\Bigl[{N\over\Omega}\, n \,\Bigr] \, \mathscr{B}_\Omega\Bigl[{N \over \Omega} (m - n) \,\Bigr]
  }
\end{equation}
  where $[m]$ indexes over the same range as $[n]$ (and given the reciprocal relation $N = L \Omega$, $L = N/\Omega$).
  Recall, $L$ and $\Omega$ are determined by available memory and the sampling aparatus, respectively.
  This formula provides the essential criteria for checking the fidelity of the signal reconstruction based on sampling at a given bandwidth.
  Notice this construction does not reference the $\sinc$ function or even to a discrete version of the $\sinc$ function, as in ie: Osgood\cite{Osgood}, and others ...
  The subtle advantage of this construction is that $\mathscr{B}$ need not refer to the DFT of the box function, but can refer to the DFT of any window function that is applied to the signal.

  NOW YOU HAVE TO CHECK THIS!!!
  This formula is very important for understanding aliasing, upsampling, downsampling in DSP.

  Compare this result with formulas by others:

  Brad Osgood\cite{Osgood}
  A discrete sampling theorem (p.449)
  $$f[m] = \sum_{k=0}^{p-1} f [ {N k \over p} ] \text{dinc}_p [m - {N k \over p}] $$ 
  

  Briggs and Henson\cite{BriggsHenson}
  Theorem 3.2 Shannon Sampling Theorem.
  ``Let $f$ be a band-limited function whose Fourier transform is zero outside of the interval $[-\Omega/2,\Omega/2]$.  If $\triangle x$ is chose so that
  $$\triangle x \le {1\over\Omega}$$
  then $f$ may be reconstructed exactly from its samples $f_n = f(n \triangle x) = f(x_n)$ by
  $$ f(x) = \sum_{n=-\infty}^\infty f_n \sinc \biggl( {\pi (x - x_n) \over \triangle x} \biggr) $$
  $$ f(x) = \triangle x \sum_{n=-\infty}^\infty f_n { \sin( \pi (x - x_n) / \triangle x) \over \pi (x - x_n) } \biggr) $$
  
  
  
  

 

  ...what i was missing before was the IDFT of the Sha function, AND a actual definition/implementation of the convolution...COME BACK AND FINISH UP!
  NOTE: this derivation differs from Osgood's
  - Osgood defines the $\text{dinc}$ function with a factor of $1/p$ - for me, $1/\Omega$ - which allows him to write the discrete analog to the continuous case as $p \text{dinc}_p$, but this is really meaningless as it amounts to multiplyig by $p/p$.
  - ... this factor of $p$ on the $\text{dinc}$ function gets canceled with the $1/p$ associated with the IDFT of the Sha function $\Sh_{p} = (1/p)\Sh_{N/p}$.
  + But, in my derivation, I simply keep the $p$-factor in the IDFT of $\Sh_{p}$ 
  + there are nonetheless differences between our respective derivations...
  + Osgood does propose a check in the end ... p.449\cite{Osgood} is very important!


  \section{Errors in the DFT and IDFT}%Chapter 6 of The DFT An Owner' Manual}

  A question that comes up in Briggs and Henson wrt errors is, when are we approximating the Fourier coefficients v approximating the Fourier transform? Recall the two expressions differ by a factor of $1/L$ (p.53-59\cite{BriggsHenson}).

  \subsection{Roundoff Error}%Chapter 6 of The DFT An Owner' Manual}

  ...section(1.4.1) in Van Loan's book on computational Frameworks for the FFT\cite{VanLoan}

  \subsection{Errors in the DFT}%Chapter 6 of The DFT An Owner' Manual}

  Briggs and Henson present this topic through the three different scenarios:
  1. (discretization error) the signal is $L$-periodic, and use the DFT to approximate the Fourier series coefficients.
  2. (discretization error) the signal is spatially limited, ie: $f(x) = 0$ when $|x| > L/2$, and use the DFT to approximate the Fourier transform (one frequency at a time for the frequencies permissible for given $L$ and $N$).
  3. (truncation error) the signal is not time-limited, ie: $f(x) \ne 0$ for some $|x| > L/2$, and the DFT is used to 
  
  Important intuition about errors in the DFT comes from the Discrete Poisson Summation Formula\cite{BriggsHenson}, which is derived by taking the DFT of the Fourier series of $f$
  (move this derivation here...?)

  \subsubsection{(beta) Leakage}

  Leakage refers to the effect when ``frequency components that are not present in the original waveform to leak into the DFT''(p.98\cite{BriggsHenson}.

  ``leakage occurs when a periodic signal is truncated and sampled on an interval that is not an integer multiple of the period''(prob. 59, 99\cite{BriggsHenson})


  \subsection{(beta) DFT Discretization Error}

  Discretization error is a result of finite sample size, ie: it arrises because it is only possible to take a finite number of sample points.
  To investigate discretization error consider taking $N\rightarrow\infty$
  $$F_k = {1\over N} \sum_n f_n e^{-i2\pi k n / N} $$ 
  
  Consider calculating the Fourier coefficients
  %Want to consider the possibility of taking infinite samples ($N\rightarrow \infty$ such that $\triangle x \rightarrow 0$)
  THINK: don't want to send all exponential terms to 0, so make substitution
  $$n / N = x_n / L \qquad\text{given}\quad x_n = n \triangle x = n L / N$$
to remove $N$ from the exponential terms
  $$F_k = {1\over N} \sum_n f_n e^{-i2\pi k x_n / L}$$
  To eliminate reference to the number of samples $N$ except in the summation, multiply by $L$ such that
  $$L F_k = {L\over N} \sum_n f_n e^{-i2\pi k x_n / L} = \triangle x \sum_n f_n e^{-i2\pi k x_n / L}$$
  Now take the limit as $N\rightarrow\infty$, and $\triangle x \rightarrow 0$
  $$\lim_{\substack{N\rightarrow\infty \\ \triangle x \rightarrow 0}} L F_k = \lim_{\substack{N\rightarrow\infty \\ \triangle x \rightarrow 0}} \triangle x \sum_n f_n e^{-i2\pi k x_n / L}$$
  $$= \int_L f_n e^{-i2\pi k x_n / L} = L \underbrace{\Biggl( {1\over L} \int_L f_n e^{-i2\pi k x_n / L} \Biggr)}_{\text{$a_k$}} = L a_k$$
  Which implies
  $$\lim_{\substack{N\rightarrow\infty \\ \triangle x \rightarrow 0}} F[k] = a_k$$

  The DFT approximates the Fourier coefficients with an increasing number of samples.
  There is a discretization error introduced by the non-zero albeit small $\triangle x$, which can be reduced with increasing number of samples over $L$.

  The DFT does not become the Fourier transform as $N\rightarrow\infty$ (p.54 \cite{BriggsHenson}).
 

  \subsection{(beta) DFT Truncation Error}

  Truncation error arrises working with non-time-limited signals due to the fact that the DFT is inherently defined over a finite boundary.
  Therefore, want to consider the limit as $L\rightarrow\infty$ (p.54\cite{BriggsHenson}).

  % typo in case 3 p.54\cite{BriggsHenson}.
  $$\lim_{\substack{N\rightarrow\infty \\ \triangle x \rightarrow 0}} L (\mathcal{F} f)[k] = \lim_{\substack{N\rightarrow\infty \\ \triangle x \rightarrow 0}} \triangle x \sum_n f_n e^{-i2\pi s_k x_n}$$ 
  $$= \int_L f(x) e^{-i2\pi s_k x} \,dx$$ 
  , which is an approximation to the Fourier transform
  $$\approx \int\limits_{-\infty}^\infty f(x) e^{-i2\pi s_k x} \,dx = \hat{f} (s_k)$$ 

  Taking the limits $L\rightarrow\infty$, and concomitantly $\triangle s \rightarrow 0$
  $$\lim_{\substack{L\rightarrow\infty \\ \triangle s \rightarrow 0}} \int_L f(x) e^{-i2\pi s_k x} \,dx = \hat{f}(s_k)$$ 

  \subsection{Pester 1: M = N/2 (Periodic Band-limited Signals)}% 1st Pester p.183, BriggsHenson

  Recall, if $f$ is piecewise smooth over $L$ then its Fourier series
  $$f(x) = \sum_{k=-\infty}^\infty c_k e^{i2\pi kx/L}$$
  converges to its periodic extension, and to the average value of its neighbors at points of discontinuity.
  And, the Fourier coefficients are given by
  $$c_k = {1\over L} \int_L f(x) e^{i2\pi kx/L} \,dx$$
  where the $k^{\text{th}}$-mode has $k$ periods/wavelengths over $L$.


  Pester: $M = N/2$  (would this be considered an example of aliasing?)
  - recall: $N/2$ is the highest frequency the DFT can resolve.
  
  Note: Briggs and Henson always work with the symmetric interval, $-L/2, L/2$.

  Difference between $N$ even v. odd(prob.126).

  Case $N$ even: $k = [-N/2+1, N/2] \in \mathbb{Z}$
  DO: consider the continuous case, where there is no problem between the $k$ and $-k$ indices.
  - Recall, to get the Fourier coefficients you need the positive and the negative $k$ componennts.
  - *** the $-N/2$ and $N/2$ modes are indistinguishable due to the cyclic exponential in the DFT.
  - note: here, $-N/2$ is outside the set of indices.
  - this is the same as for indices $0$ to $N$.
  - the same is true of any pair of indices separated by $N$ number of indices.
  - Take away, $\triangle x \le 1/\Omega$.
  - To satisfy the criteria of having two sample point per period 

  Take away: for an even number of samples $N$
  - N = 2 times the index of the Nyquist frequency ($\Omega/2)$, is the minimum required to resolve the Nyquist frequency.
  - BUT: this is often not enough, 2 times this value, that is 4 times the index of the Nyquist frequency, is used in practice.
  - consider case of N = the index of the Nyquist frequency (``undersampling'').. (prob.122 Briggs+Henson)


  \subsection{DFT Zero Padding - for mitigating error (which?)}

  p.56,...\cite{BriggsHenson}

  Zero padding is a technique for refining the frequency grid by artificially increasing the number of samples albeit all with value zero. 


  Zero padding for $N'$ even, with $n$-index domain $[0, \ldots, N-1]$
  $$ g[n] =
	\begin{cases}
	  f[n] & \text{if } n \in [0,\ldots,N-1] \\
	  0		& \text{if } n \in [N,\ldots,N'-1]
	\end{cases}
  $$
  the $N'$-point DFT of $g[n]$ is
\begin{align*}
  G_k = (\mathcal{F} g)[k]
  &= {1\over N'} \sum_{n=0}^{N'-1} g[n] w_{N'}^{-kn} \\
  &= {1\over N'} \sum_{n=0}^{N-1}  f[n] w_{N'}^{-kn} \\
  &= \Bigl({N\over N}\Bigr) {1\over N'} \sum_{n=0}^{N-1}  f[n] \exp \biggl( -i2\pi {kn\over N'} \Bigl({N \over N}\Bigr) \biggr) \\
  &= \Bigl({N\over N'}\Bigr) {1\over N} \sum_{n=0}^{N-1}  f[n] \exp \biggl( -i2\pi \Bigl({k N \over N'}\Bigr) \Bigl( {n\over N} \Bigr) \biggr) \\
  &= \Bigl({N\over N'}\Bigr) (\mathcal{F} f)[kN/N']
  = \Bigl({N\over N'}\Bigr) F_{kN/N'}
\end{align*}
  

  

  \subsection{Errors in the IDFT}%Chapter 6 of The DFT An Owner' Manual}

  Again, the IDFT is presented via 3 cases 
  1. (truncation error) the coefficients $F[k]$ are known, and $f$ is over $L$, then the IDFT approximates the $f$ at the sequence of points $\{x_n\}$ suffers from truncation error due to a finite number of frequency samples.
  2. (discretization error) regard $F[k]$ as a set of samples of a band-limited ($\hat{f}(s)=0$ for $|s| > \Omega/2$) signal, use the IDFT to approximate the inverse Fourier transform for $f(x_n)=f[n]$.
  3. (discretization error and truncation error) non-band-limited signals are common and want to reconstruct the signal using the IDFT, then 
  $$\lim_{\substack{\Omega\rightarrow\infty \\ \triangle x \rightarrow 0}} \lim_{\substack{N\rightarrow\infty \\ \triangle s \rightarrow 0}} \triangle s F[k] = f[n]$$
  - truncation error due to fundamental inability to take the limit $\Omega\rightarrow\infty$ in practice.
  - discretization error due to finite grid size used for the integral approximation.
  

  \subsection{(beta) IDFT Truncation Error}

  Note: def. band-limited (p.57\cite{BriggsHenson}.

  Truncation error occurs because the IDFT is a truncated version of the (infinite) Fourier series representation (p.57\cite{BriggsHenson}), ie: it is only possible to sample a finite number of frequencies.

  Consider $f$ on a closed interval, and being given the Fourier coefficients, then the IDFT is
  $$f[n] = \sum_k F_k e^{i2\pi k x /L}$$
  approximates the Fourier series synthesis at individual points $x_n$.
  

  \subsection{(beta) IDFT Discretization Error}

  $$\triangle s f[n] = \triangle s \sum_k F_k e^{i2\pi k n / L} = \triangle s \sum_k F_k e^{i2\pi s_k x_n}$$

  Taking the limit as $N\rightarrow\infty$
  $$=\lim_{\substack{N\rightarrow\infty \\ \triangle s \rightarrow 0}} \triangle s \sum_k F_k e^{i2\pi s_k x_n}$$

  Assuming $f$ is band-limited
  $$=\int_{-\Omega/2}^{\Omega/2} \hat{f}(s) e^{i2\pi s x_n}\,ds$$
  $$=f(x_n)=f[n]$$


  \section{Errors in the DFT}
  Investigate sequence of cases that cover all common DFT use cases.
  What are the sources of error in each case?
  "The form of the input sequence dictates how the DFT is used and how its output should be interpreted.

  2 Cases: Use the DFT to ...
  1. approx the Fourier coefficients of the input.
  2. approx (samples of) the FT of the input (?).

  def. ``pester'', ie: treatment of endpoints, even/od number of data points, ...
  - cannot be ignored.
  - contain a hidden lesson.


  \subsection{Periodic Band-limited Input and the Nyquist Frequency}

  Note: it is important to specify that we are referring to ''functions`` here, as these are idealizations - in practice sampling produces a discrete data set, the continuous model comes later.
  For periodic band-limited input the question is, how well does the DFT approximate the Fourier coefficients of $f$? 

  Assume $f$ is piecewise smooth and periodic on $[-L/2, L/2]$ 
  , {\bf then} Fourier series representation
  $$f(x) = \sum_{k=-\infty}^\infty c_k e^{i 2\pi k x / A}$$
\begin{itemize}
  \item converges to $f$ on the interval $[-L/2, L/2]$ (recall the reason why from DFT derivation), 
  \item and to its periodic extension beyond (recall the reason why from DFT derivation), 
  \item and to its average value at points of discontinuity (recall the reason why from DFT derivation), 
  \item (and how to handle endpoints?).
  \item And the coefficients are given by
  $$c_k = {1\over L} \int\limits_{-L/2}^{L/2} \,d x f(x) e^{-i 2\pi k x /L} \qquad\text{, } k = 0, \pm 1, \pm 2, \ldots$$
  where the $k$-th ``mode'' specifies the number of periods/cycles/wavelengths on the interval $[-L/2, L/2]$.
\end{itemize}

  Briggs and Henson investigate aliasing by looking at the DFT of the sampled function\cite{BriggsHenson}, the function sampled at the points $x_n = n \triangle x = n L/N$.
  For periodic $f$, given $N(even)$ uniform samples, where $n = -N/2+1 : N/2$ (the sampled value assigned to $f$ at the right endpoint $(f_{N/2})$ is the average of its endpoint values (itself and the first sample value? (what is this based on? and why not 'fabricate' one additional data point equal to the average of the first and last values that were truly gotten from sampling?)))
  $$f_n = f(\{x_n\}) = f(x_n) = \sum_{k=-\infty}^\infty c_k e^{i 2\pi k x_n / L} = \sum_{k=-\infty}^\infty c_k e^{i 2\pi k n / N}$$
  The frequency index targeted by the DFT will be denoted, $k'$, with a prime, ie: the DFT for $k' = -N/2+1: N/2$, to distinguish it from the frequency index used to construct Fourier series representation of the function, $f$
\begin{align*}
  (\mathcal{F}f_n)(k') &= {1\over N} \sum_n^{(N)} f_n e^{-i 2\pi k' n / N}\\
  &= {1\over N} \sum_n^{(N)} \Biggl(\sum_{k=-\infty}^\infty c_k e^{i 2\pi k n / N}\Biggr) e^{-i 2\pi k' n / N}\\
\intertext{Combining the complex exponentials and reversing the order of the summations}
  &= {1\over N} \sum_{k=-\infty}^\infty c_k \Biggl(\sum_n^{(N)} e^{i 2\pi (k-k') n / N}\Biggr)\\
  &= {1\over N} \sum_{k=-\infty}^\infty c_k \, N \, \hat\delta_N (k-k')\\
\intertext{Due to the $N$-cyclic nature of the $\hat\delta_N$ this simplifies to}
  &= \sum_{k=-\infty}^\infty c_{k' + k N}\\
\intertext{, which can also be written as}
  \Aboxed{F_{k'} &= c_{k'} + \sum_{k=1}^\infty c_{k' + kN} + c_{k'-kN}}
\end{align*}

  Using $k'$ for the DFT makes it clear that the resulting infinite sum comes from the Fourier series representation, which naturally uses $k$ to express the function/signal in terms of its modes.
  Briggs and Henson refer to the last expression as the ``Discrete Poisson Summation Formula''\cite{BriggsHenson}.
  For bandlimited signals, the Fourier coefficients should be zero beyond the Nyquist frequency.
  Hence, the DFT reproduces the signal exactly, there is no error in the Fourier coefficients.
  But...there are snags!
  Like the sampling window, etc.
  And, in general, we should be open to considering that the Fourier coefficients obtained by the DFT include contributions from the harmonics of a given mode as well.
  Is there a demo you can show of this?

  Recall the definition of bandlimited (``band-limited'' ref.\cite{BriggsHenson}).
  The singal or function is bandlimited if there is some $k'$, referring to values of the frequency $k$-index, and some $\kappa \in \mathbb{Z} \ge 0$, a nonnegative integer, such that
  $$ C_{k'} = 0 \quad\text{where}\quad |k'| > \kappa $$
  For nonbandlimited signals, and in situations where the signal reaches a frequency mode with index $\kappa$ that surpasses the potential limitation on the possible $k'$ values that can be obtained by a given instrumentation, errors in the DFT calculation will be present.
  \ul{This is a source of error that is caused by the signal itself, and not by the specific instrument}.
  For a given instrumentation, this will occur if a signal frequency $\omega'$ is greater than the bandwidth, $\Omega$, that can be resolved
  $$\omega' \ne 0 \quad\text{where}\quad \omega' > {\Omega\over 2}$$
  And more specifically, when the signal has modes with $k'$ values
  $$ k' > {N\over 2} $$
  greater than half the number of samples (recall, 2 samples of a mode are required to resolve a given mode).

  (...)

  The term bandlimited is related to the concept of the bandwidth.
  The maximum frequency that can be resolved is half the bandwidth
  $$\omega_{\text{max}} = {\Omega \over 2} = {N\over 2L}$$
  , which can be written in terms of its dimensions, cycles per unit length, using the reciprocity relation $L \Omega = N$.
  To sample the frequency, $\omega_{\text{max}}$, requires sampling the corresponding wavelength, $\lambda_{\text{min}}$
  $$\lambda_{\text{min}} = {2 L \over N}$$
  two times over its period, which leads to the grid spacing (sampling rate)
  $$\triangle x = {\lambda_{{\text{min}}} \over 2} = {L\over N}$$
  This determines the critical sampling rate (or Nyquist sampling rate) required to sample a given frequency.

  \subsection{Periodic Non-Bandlimited Input and Aliasing}
  % Briggs and Henson - Aliasing, p.95

  In the case of periodic nonbandlimited input
  $$F_{k'} = c_{k'} + \underbrace{\sum_{k=1}^\infty c_{k' + kN} + c_{k'-kN}}_{\text{``aliased modes''}}$$
  higher order components, belonging to aliased modes - which are artifacts of an instrument's limitations - may contaminate the DFT computation of a specific $k'$-th mode.

  The aliasing effect occurs when aliased modes contribute spurious artifacts essentially because they agree at the data points, ie:
  (prob.116, 117)
\begin{align*}
  e^{i2\pi s_k x_n} &= \exp \biggl(i2\pi {k\over L}{nL\over N} \biggr) = e^{i2\pi kn/N} \\
  e^{i2\pi s_{k+jN} x_n} &= \exp \biggl(i2\pi {k+jN\over L}{nL\over N} \biggr) = e^{i2\pi kn/N} (1) = e^{i2\pi s_k x_n} \\
\end{align*}


  ...the most important factor influencing the extent of aliasing is the smoothness of the underlying function.  
  the extent of aliasing is based on $|F_k - c_k|$ ...
  to examine these bounds, consider...(MATH: integration by parts of )
\begin{align*}
  L c_k &= \int_L f(x) e^{-i2\pi k x/L} \,dx \\
\intertext{The choice of $u$ and $v'$}
  &= \int_L \underbrace{f}_{u} \underbrace{e^{-i2\pi k x/L}}_{v'} \,dx \\
\intertext{is to eliminate $f$ given the exponential is infinitely differentiable}
  &= \int_L \underbrace{f}_{u} \underbrace{e^{-i2\pi k x/L} \,dx}_{v'} \\
\intertext{$uv' = (uv)' - u'v$}
  &= {f e^{-i2\pi k x/L} \over -i2\pi k /L } - \int_L f' {e^{-i2\pi k x/L} \over -i2\pi k /L} \,dx \\
\end{align*}
  (...)	
  


  
  def. ``smoothness'' as number of continuous derivatives (refs. 30, 70, 115, 158).
  Theorem: rate of decay of Fourier coefficients (v1)
  - assm. for $p\ge1$ the derivative of $f^{(p-1)}$ is periodic and continuous over $L$.
  - assm. $f^{(p)}$ bounded with a finite number of discontinuities over $L$.
  - then, $$|c_k| \le {C\over |k|^p} \quad\forall\, k$$
  - pf. (repeated integration by parts)...

  def. piecewise monotone - $L$ can be split (evenly?) into a finite number of subintervals over which $f$ is nonincreasing or nondecreasing.
  Theorem: rate of decay of Fourier coefficients (v2)
  - assm. for $p\ge1$ the derivative of $f^{(p-1)}$ is periodic and continuous over $L$.
  - assm. $f^{(p)}$ bounded and piecewise monotone over $L$ (note: $f^{(0)}$ is the function itself).
  - then, $$|c_k| \le {C\over |k|^{p+1}} \quad\forall\,k$$
  - pf. $p$ integrations by-parts...
  - *finite number of discontinuites with infinite jumps.
  - *Dirichlet's conditions ref.30 - cover all conditions under which the theorem is true.

  NOW: DFT Error for periodic non-bandlimited signals
  - $f^{(p-1)}$ for $p\ge1$ are periodic and continuous over $L$.
  - $f^{(p)}$ bounded and piecewise monotone over $L$.
  $$|F_k - c_k|\le {C\over N^{p+1}} \quad\forall\,k$$
  pf. (really want ref.76, Henson's PhD thesis on image reconstruction.)
  Using the discrete Poisson summation formula
  $$F_k - c_k = \sum_{j=1}^\infty (c_{k+jN} + c_{k-jN}) \quad\forall\,k$$
  Using the previous theorem
  $$F_k - c_k \le \sum_{j=1}^\infty {C^\prime \over |k+jN|^{p+1}} + {C^\prime \over |k-jN|^{p+1}}$$
  $$= {C^\prime \over N^{p+1}} \sum_{j=1}^\infty {1 \over |j+{k\over N}|^{p+1}} + {1 \over |j-{k\over N}|^{p+1}}$$
  The series converges for $p\ge1$ (cite: Henson PhD thesis ref.76, or just do it!), hence
  $$|F_k - c_k|\le {C\over N^{p+1}} \quad\forall\,k$$
  Consider this formula under $N$ large...(compare this formula with Table of DFT Errors)

  How to evaluate ``smoothness'' based on discrete data?
  Maybe, approximate with polynomial to determine how many derivatives it has?
  SNAGS! p.189 I'm not sure I understand this section well! It's hard to see how important it really is But it might entail some important features of signals that are important to analyze!... we want to take derivatives!!!
  1. ($p=1$), (details/conditions, ie: $\cos(\pi x)$ sampled on $[-1/2, 1/2]$, error $\sim N^{-2}$, ie: real even harmonics and the triangle wave...
  2. ($p>1$), difficult to compute analytically, require numerical methods.
  3. ($p=0$), only the function $f$ is bounded and piecewise monotone over $L$ ... The Theorem above does not cover this case!!!
	- Cases: discontinuities either within the sequence or at the endpoints.
	- complex harmonic
	- real odd harmonic
	- linear
	- rectangular wave
	- square pulse
	- exponential
	- these cases follow asymptotic behavior:
	$$F_k - c_k \sim CkN^{-2}$$
	for all $k$ as $N\rightarrow\infty$.
	* two regimes (p.189 \cite{BriggsHenson}):
	case1: for $|k| << N/2$ low frequenc, then $E\sim N^{-2}$
	case2: for $|k| \approx N/2$ low frequenc, then $E\sim N^{-1}$


  \subsection{Periodic Non-band-limited Input II}
  aliasing - occurs when the period/wavelength of a periodic function is greater than the sampling duration/extent.
  ie: $f(x) = \cos(\pi x)$ sampled on $[-1/2,1/2]$ results in discontinuity in $1^{\text st}$ derivative at the end points.
  This is the case of $p=1$, mentioned above.
  Increasing the number of samples ($N$) cannot overcome the limit in sampling duration/extent.
  Higher frequency modes are said to be ``aliased'' onto (corresponding) lower frequency modes (might have to do the exercese to actually see this (cannot understand the terminology)!).
  (Should this still be called aliasing? maybe this is like high frequencies leaking IN TO the DFT calculation (v. leaking OUT))
  obs: error for low-frequency coefficients is lower than for high-frequency coefficients.


  \subsection{Leakage in General}
  LIFE SAVING RESOURCE: https://libgen.rs/
  % https://dspillustrations.com/pages/posts/misc/spectral-leakage-zero-padding-and-frequency-resolution.html

  NOTE: we should be able to answer ``what is the difference between aliasing and leakage''? (would be a good YouTube video)
  Note: make YouTube videos that are problem-solution pairs.

  Leakage is a source of error in the DFT that is in addition to aliasing error.
  Brigham in states ``leakage ... is inherent in the DFT because of the required time-domain truncation''(p.103 \cite{Brigham}), but this is a bit of an over simplification.
  Leakage occurs when a periodic function is sampled over a duration that is not a multiple of the period of the signal or of its components.
  It should be appreciated that without prior knowledge this is generally the case.
  This is such an important point that deserves to be stated openly from the outset, that it is almost certain that the DFT calculation will suffer from leakage error.
 
  E.g. assm. $f$ is periodic over $L$ with Fourier coefficients $\{c_k\}$
  Consider taking $N$ samples over the region $[-L/2,L/2]$, and $pN$ samplse over $[-pL/2,pL/2]$ where $p>1 \in \mathbb{Z}$ (p.190\cite{BriggsHenson})
  %derive this (prob.123)?
  $$ c^\prime_k =
	\begin{cases}
		c_{k/p} & \text{if $k/p \in \mathbb{Z}$} \\
		\text{otherwise,} & \text{the frequency is not shared by the two sets of indices}
	\end{cases}
  $$
  This obviously holds for the $F_k^\prime$ as well (prob.124).
  This isn't something deep it is simply a relationship between indices for the DFT over $L$ and the DFT over $pL$.
  OBS: the variable $p$ is being used in two different contexts in close proximity, previously to describe the smoothness of a function and here to describe the elongation of the sampling interval (editor pho-pa)!

  If you must...demonstrate this in practice (ie: $f(x) = \cos(2x)$)
  ... $\cos(2x)$ is a difficult example to work with!
  ... How about a simpler function, ie: $f(x) = \cos(\pi x)$?
  ACTUALLY: stick with $\cos(2x)$ and supply $x$ values.
  (this relationship is maintained for the DFT approximation of the Fourier transform)
  Note: this sounds like a way to refine the DFT calculation to make sure the frequencies you're sampling are valid.
  %*** is this a precursor to the ``multi-taper method''? Ans. NO

  THIS WHOLE SHIT IS SUPER IMPORTANT (IT IS THE DIFFERENCE BETWEEN CONTINUOUS AND DISCRETE) !!! 
  THIS IS SUCH AN IMPORTANT EXERCISE because it is our first encounter with what it is like to work with a real example... 
  Now consider the less ideal and more realistic situation.
  Assume $f$ is $L$-periodic over the symmetric interval $[-L/2, L/2]$ (is this setup essential?).
  It therefore can be represented as a Fourier series with a set of coefficients, $\{c_k\}$.
  And sample $N$ data points on region $[-L/2, L/2]$, and $pN$ data points on $[-pL/2, pL/2]$ where $p > 1 \notin \mathbb{Z}$ (p.190\cite{BriggsHenson}).
  Restricting the focus to a single mode
  $$f(x) = e^{i2\pi k_1 x / L}$$
  (MUST DERIVE:) The Fourier coefficients of $f$ on $[-pL/2, pL/2]$ are (eqn. 6.5, p.191 \cite{BriggsHenson})
  $$c_k^\prime = {\sin\pi(k - pk_1)\over \pi (k - pk_1)}$$
  None of the coefficients $c_k^\prime$ vanish because $(k - p k_1) \notin \mathbb{Z}$ (even if $f$ is band-limited).
  Coefficients with $k$ nearest $p k_1$ are often the largest, with characteristic oscillatory ``sidelobes'' decaying as $(k-pk_1)^{-1}$ symptomatic of leakage.
  (MUST DERIVE: problem 125) The DFT coefficients (what's the difference between these and the Fourier coefficients?) suffers similarly in that the $pN$-point DFT is
  $$F_k^\prime = { \sin(\pi(k-pk_1)) \sin(2\pi (k-pk_1)/pN) \over 2pN\sin^2(\pi (k-pk_1)/pN) }$$
  for $k=-pN/2+1: pN/2$.
  *** I don't understand why arbitrary $k$ modes are described with reference to some arbitrary mode $k_1$ associated with $f$ over $[-L/2, L/2]$ ??? ***

  Note: $p$ could change $N$ from even to $pN$ odd, ie: $10 \times 1.5 = 15$.
  Want to stick to something simpler like $10 * 1.2 = 12$.

  NOTICE: error is reported as $|c_k - F_k|$

  START SIMPLE:
  We should be able to first do the continuous case.
  (Consider) One mode from an $L$-periodic function
  $$f(x) = e^{i2\pi k_L x /L}$$
  over the expanded region $pL$ has a Fourier series representation with the Fourier coefficients
  $$c^\prime_k = {\sin(\pi (k-k_L)) \over \pi (k-k_L)}$$
  Proof:
  Fourier's trick for recovering the coefficients in the complex exponential series representation
  $$c^\prime_k = {1\over pL}\int_{pL} \,dx f(x) e^{-i2\pi k x / pL} $$
  Substituting for $f$
\begin{align*}
  c^\prime_k &= {1\over pL}\int_{pL} \,dx \,e^{i2\pi k_L x /L} \,e^{-i2\pi k x / pL} \\
  &= {1\over pL}\int_{pL} \,dx \,e^{-i2\pi (k-pk_L) x /pL} \\
  &= {1\over pL}\int_{pL} \,dx \,e^{-i2\pi (k-pk_L) x /pL} \\
  &= {1\over pL} \Biggl[{- pL \over i2\pi (k - p k_L)} \,e^{-i2\pi (k - p k_L) x / pL }\Biggr]_{x = -pL/2}^{x = pL/2} \\
  &= {-1 \over i2\pi (k - p k_L)} \Biggl[ \exp\biggl(-i2\pi {(k - p k_L) \over pL} \biggl({pL \over 2}\biggr)\biggr) - \exp\biggl(-i2\pi {(k - p k_L) \over pL} {\biggl(-{pL \over 2}\biggr)}\biggr) \Biggr] \\
  &= {1 \over i2\pi (k - p k_L)} \Biggl( e^{i\pi (k - p k_L)} - e^{-i\pi (k - p k_L)} \Biggr) \\
  &= {1 \over \pi (k - p k_L)} \Biggl( {e^{i\pi (k - p k_L)} - e^{-i\pi (k - p k_L)} \over i2} \Biggr) \\
  &= {\sin(\pi (k - p k_L)) \over \pi (k - p k_L)} \\
\end{align*}
  NOTE: What about the asymmetric region $x \in [0,L]$.

  And then the discrete case
  $$f(x) = e^{i2\pi k_L x /L}$$
  $$f_n = e^{i2\pi k_L n \triangle x /L} = e^{i2\pi k_L n (L/N) /L} = e^{i2\pi k_L n /N}$$
  $$F_k^\prime = {\sin(\pi (k - p k_L)) \sin(2\pi (k - p k_L) / pN) \over 2pN \sin^2(\pi (k - p k_L) / pN)}$$

  Proof: 
  (This thing is called a ``geometric progression'' and has the trig-representation above)
  (This result is also related to the ``Dirichlet kernel'', aka the aliased sinc function)
  (Formulas with this structure occur repeatedly in applicaitons: p.267 + prob.156 (digital filters))
  %(https://flylib.com/books/en/2.729.1/the_dft_frequency_response_to_a_complex_input.html)
  $$F_k^\prime = {1\over pN} \sum_{n=-pN/2+1}^{pN/2} \biggl(e^{i2\pi k_L n/N}\biggr) \, e^{-i2\pi k n / (pN)}$$
  $$= {1\over pN} \sum_n e^{i2\pi p k_L n / (pN)} \, e^{-i2\pi k n / (pN)}$$
  $$= {1\over pN} \sum_n e^{i2\pi (p k_L - k) n / (pN)}$$
  Use the formula for geometric sum with $a$-cyclic
  $$\sum_{n=p}^{p+N-1} {a^n} = {a^p-a^{p+N}\over 1-a}$$
  $$= {1\over pN} {e^{i2\pi (p k_L - k) (-pN/2+1) / (pN)} - e^{i2\pi (p k_L - k) (pN/2+1) / (pN)} \over 1 - e^{i2\pi (p k_L - k) n / (pN)}}$$
  convert $(p k_L - k)$ to $(k - p k_L)$
  $$= {1\over pN} {e^{-i2\pi (k-p k_L) (-pN/2+1) / (pN)} - e^{-i2\pi (k-p k_L) (pN/2+1) / (pN)} \over 1 - e^{-i2\pi (k-p k_L) n / (pN)}}$$


  What about working backwards
  $$F_k^\prime = {\sin(\pi (k - p k_L)) \sin(2\pi (k - p k_L) / pN) \over 2pN \sin^2(\pi (k - p k_L) / pN)}$$
  Simplify angle variables and remove the ${1\over N}$ factor (since it comes from the DFT definition) to concentrate on converting the exponentials to $\sin$ functions.
  $$F_k^\prime = {\sin(x) \sin(2x / N) \over 2 \sin^2(x / N)}$$



  %Alt resource(flylib.com)(not same result??): https://flylib.com/books/en/2.729.1/the_dft_of_rectangular_functions.html
  Exercise: p.102 DFT over $-L/2$ to $L/2$ of boxcar from $-L/4$ to $L/4$
  (...)
  $$F_k = { \sin(\pi k/2) \sin(2\pi k/N) \over 2 N \sin^2(\pi k/N) }$$
  $$\cos\bigl({\pi k \over 2}\bigr) + 2 \Re \Bigl[{1 - \omega_N^{kN/4} \over 1 - \omega_N^k}\Bigr] - 1$$	
  $$\cos\bigl({2\pi k N \over 4}\bigr) + 2 \Re \Bigl[{ 1 - e^{i2\pi k N / 4} \over 1 - e^{i2\pi k/N} }\Bigr] - 1$$	
  


  SIDE: in thinking about this stuff I wonder if it is necessary to destinguish between Fourier coefficients in continuous v discrete settings...but my hunch is No, because the Fouerier coefficients are obtained in the same way in both cases.
  
\begin{figure}[H]
\centering
\begin{tikzpicture}
% Depiction of the sampling axis
\draw[->, thick]
	(-5,0) node[anchor=north, yshift=-1ex] {$0$} 
	-- (-5,0) node[] {$|$} 
	-- (-2,0) node[] {}
	-- (1,0) node[] {}
	-- (4,0) node[] {$|$}
	-- (4,0) node[anchor=north, yshift=-1ex] {$L$}
	-- (5,0) node[] {};
\node [anchor=west, align=left] (note) at (6,0) {(sampling axis)};

% note the sin and cos commands draw only a quarter sine/cos curve and the y coordinate of two points should be different.
\draw[ultra thick, black] (-5,0) sin (-2.75,1);
\draw[ultra thick, black] (-2.75,1) cos (-0.5,0);
\draw[ultra thick, black] (-0.5,0) sin (1.75,-1);
\draw[ultra thick, black] (1.75,-1) cos (4,0);

\filldraw[black] (-5,0)  circle (.1);
\filldraw[color=black, fill=white] (4,0) circle (.1);

\end{tikzpicture}
\caption{
  The fundamental mode, aka ``one-mode'' (red), which determines the grid spacing in the frequency domain $\triangle s = 1/L$.
  The dots indicate where this frequency is sampled during the present (black) and ensuing (hollow) sampling cycle.
}
\end{figure}
 

  All our knowledge comes from understanding simple examples.
  The DFT with even $N$ computed over $-L/2, L/2$ of the square pulse defined over $-L/4, L/4$
  $$ f(x) =
	\begin{cases}
		0 & \text{, $L/4 < |x| \le L/2$} \\
		1 & \text{, $|x| < L/4$} \\
    	1/2 & \text{, $x = \pm L/4$}
   	\end{cases}
  $$
  is given by
  $$F_k = {1\over N} \sum_{n=-N/2+1}^{N/2} f_n \omega_N^{-kn}$$
  It is easier to disect the sum with the factor of $N$ on the other side
  $$N F_k = {1\over 2} \biggl(\omega_N^{-kN/4} + \omega_N^{kN/4}\biggr) + \sum_{n=-N/4+1}^{N/4-1} \omega_N^{-kn}$$
  Collapsing the exponentials and folding the sum (minus 1 due to double counting the $n=0$ for $\pm n$)
  $$= \cos(\pi k/2) + \sum_{n=0}^{N/4-1} (\omega_N^{-kn} \omega_N^{kn}) - 1$$
  $$= \cos(\pi k/2) + 2 \text{Re} \sum_{n=0}^{N/4-1} (\omega_N^{-kn} \omega_N^{kn}) - 1$$
  Recognizing the geometric sum
  $$= \cos(\pi k/2) + 2 \text{Re} \Biggl({ 1 - \omega_N^{kN/4} \over 1 - \omega_N^k }\Biggr) - 1$$
  Briggs and Henson do not show their work but state from this expression the DFT simplifies to
  $$F_k = {\sin(\pi k/2) \sin(2\pi k/N) \over 2 N \sin^2(\pi k/N)}$$
  , which applies for $k$ from $-N/2+1$ to $N/2$ but not for $k=0$ for which the DFT is
  $$F_0 = {1\over 2}$$

  $$F_k = \cos(\pi k/2) + 2 \text{Re} \Biggl({ 1 - \omega_N^{kN/4} \over 1 - \omega_N^k }\Biggr) - 1$$
  $$F_k = \cos(\pi k/2) + 2 \text{Re} \Biggl({ 1 - e^{i2\pi kN/(4N)} \over 1 - e^{i2\pi k/N} }\Biggr) - 1$$
  $$= \cos(\pi k/2) + 2 \Biggl({ 1 - \cos(2\pi k N / 4 N) \over 1 - \cos(2\pi k / N) }\Biggr) - 1$$
  $$= \cos(\pi k/2) + 2 \Biggl({ 1 - \cos(\pi k / 2) \over 1 - \cos(2\pi k / N) }\Biggr) - 1$$
  $$= \cos(\pi k/2) + 2 \Biggl({ 1 - \cos(\pi k / 2) \over 1 - \cos(2\pi k / N) }\Biggr) - 1$$
  From here there are multiple ways to proceed.
  Simply proceeding algebraically without any trigonometry gives
  $$= \cos(\pi k/2)(1 - \cos(2\pi k / N)) + 2 \Biggl({ 1 - \cos(\pi k / 2) \over 1 - \cos(2\pi k / N) }\Biggr) - (1 - \cos(2\pi k / N))$$
  $$= {1 - \cos(\pi k / 2) + \cos(2\pi k / N) - \cos(\pi k/2) \cos(2\pi k / N) \over 1 - \cos(2\pi k / N)}$$
  $$= {(1 - \cos(\pi k / 2)) (1 + \cos(2\pi k / N)) \over 1 - \cos(2\pi k / N)}$$
  From here it is not obvious how Briggs and Henson arrive at their equation
  $${ \sin(\alpha) \sin(2\beta) \over 2 \sin^2(\beta) }$$
  This actually seems like an abuse of the identity
  $$1-\cos^2\theta = \sin^2\theta$$
  where they expand terms
  $$(1-\cos\theta)(1+\cos\theta) = \sin\theta\sin\theta$$
  but then simply replace one of $\theta$ values
  $$(1-\cos\theta)(1+\cos\phi) = \sin\theta\sin\phi$$
  but this is not math!
  %Maybe the half-angle trig identity for $\tan\theta$ makes this assertion a bit seductive
  %$$\tan({\theta\over 2}) = {\sin\theta \over 1 + \cos\theta} = {1-\cos\theta\over\sin\theta}$$
  %but again this does not overcome the need to randomly introduce the other angle.

  Use trigonometric identities
  $$= {(1 - \cos(\alpha)) (1 + \cos(2\beta)) \over 1 - \cos(2\beta)}$$
  $$= { (\cos^2(\alpha/2) + \sin^2(\alpha/2) - (\cos^2(\alpha/2) - \sin^2(\alpha/2))) \, (\cos^2(\beta) + \sin^2(\beta) + (\cos^2(\beta) - \sin^2(\beta))) \over \cos^2(\beta) + \sin^2(\beta) - (\cos^2(\beta) - \sin^2(\beta)) }$$
  $$= { (2 \sin^2(\alpha/2)) \, (2 \cos^2(\beta) ) \over 2 \sin^2(\beta) }$$
  $$= { 2 \sin^2(\alpha/2) \cos^2(\beta) \over \sin^2(\beta) }$$

  ***********************************************************************************

  THIS IS WHERE YOU COME IN AND FIX THINGS:

  Consider $f(x)$ to be the cyclic complex exponential function (this exercise is based on Briggs and Henson's section on leakage in chapter 6 of The DFT book\cite{BriggsHenson})
  $$f(x)= e^{+i2\pi k' x / L}$$
  ,a single mode in the Fourier series obtained from the $N$-point DFT, defined on the symmetric grid of width $L$.

\begin{center}
\begin{tikzpicture}
% horizontal labels
\draw[<->] (-7, 0)
	  -- (-6,0) node[] {$|$} node[anchor=north, yshift=-1.5ex] {$-L/2$}
	  -- (-6,0) node[] {$|$} node[anchor=north, yshift=5ex] {$-N/2$}
	  -- (-5,0) node[] {$|$}
	  -- (-4,0) node[] {$|$} 
	  -- (-3,0) node[] {$|$}
	  -- (-2,0) node[] {$|$}
	  -- (-1,0) node[] {$|$}
	  -- (0,0)  node[] {$|$} node[anchor=north, yshift=-1.5ex] {$0$}
	  -- (1,0)  node[] {$|$}
	  -- (2,0)  node[] {$|$}
	  -- (3,0)  node[] {$|$}
	  -- (4,0)  node[] {$|$} 
	  -- (5,0)  node[] {$|$}
	  -- (6,0)  node[] {$|$} node[anchor=north, yshift=-1.5ex] {$L/2$}
	  -- (6,0)  node[] {$|$} node[anchor=north, yshift=5ex] {$N/2$}
	  -- (7,0);
% f(x)
\draw[ultra thick, black] (-6,0) sin (-3,1);
\draw[ultra thick, black] (-3,1) cos (0,0);
\draw[ultra thick, black] (0,0) sin (3,-1);
\draw[ultra thick, black] (3,-1) cos (6,0);
\end{tikzpicture}
\end{center}

  In the case of leakage, the grid is potentially expanded such that the sampling extent is not a multiple of the full period of $f(x)$.

\begin{center}
\begin{tikzpicture}
% horizontal labels
\draw[<->] (-8,0)
	  -- (-7,0) node[] {$|$} node[anchor=north, yshift=-1.5ex] {$-pL/2$}
	  -- (-7,0) node[] {$|$} node[anchor=north, yshift=5ex] {$-N/2$}
	  -- (-5.75,0) node[] {$|$}
	  -- (-4.60,0) node[] {$|$}
	  -- (-3.45,0) node[] {$|$}
	  -- (-2.30,0) node[] {$|$}
	  -- (-1.15,0) node[] {$|$}
	  -- (0,0)	   node[] {$|$}
	  -- (0,0)	   node[] {$|$} node[anchor=north, yshift=-1.5ex] {$0$}
	  -- (1.15,0)  node[] {$|$}
	  -- (2.30,0)  node[] {$|$}
	  -- (3.45,0)  node[] {$|$}
	  -- (4.60,0)  node[] {$|$}
	  -- (5.75,0)  node[] {$|$}
	  -- (7.0,0)   node[] {$|$} node[anchor=north, yshift=-1.5ex] {$pL/2$}
	  -- (7.0,0)   node[] {$|$} node[anchor=north, yshift=5ex] {$N/2$}
	  -- (8,0);
\end{tikzpicture}
\end{center}

  To ensure the frequency associated with $f(x)$ is accessible on the expanded grid the number of sample points must also be scaled (the set of frequencies in the $N$-point DFT over $L$ and in the $pN$-point DFT over $pL$ must overlap) 

\begin{center}
\begin{tikzpicture}
% horizontal labels
\draw[<->] (-8,0)
	  -- (-7,0) node[] {$|$} node[anchor=north, yshift=-2.75ex] {$-pL/2$}
	  -- (-7,0) node[] {$|$} node[anchor=north, yshift=7ex] {$-pN/2$}
	  -- (-6,0) node[] {$|$}
	  -- (-5,0) node[] {$|$}
	  -- (-4,0) node[] {$|$}
	  -- (-3,0) node[] {$|$}
	  -- (-2,0) node[] {$|$}
	  -- (-1,0) node[] {$|$}
	  -- (0,0)  node[] {$|$}
	  -- (0,0)  node[] {$|$} node[anchor=north, yshift=-1.5ex] {$0$}
	  -- (1,0)  node[] {$|$}
	  -- (2,0)  node[] {$|$}
	  -- (3,0)  node[] {$|$}
	  -- (4,0)  node[] {$|$}
	  -- (5,0)  node[] {$|$}
	  -- (6,0)  node[] {$|$}
	  -- (7,0)  node[] {$|$} node[anchor=north, yshift=-2.75ex] {$pL/2$}
	  -- (7,0)  node[] {$|$} node[anchor=north, yshift=7ex] {$pN/2$}
	  -- (8,0);
\draw[ultra thick, black, dashed] (-9,-1) cos (-6,0);
\draw[ultra thick, black] (-6,0) sin (-3,1);
\draw[ultra thick, black] (-3,1) cos (0,0);
\draw[ultra thick, black] (0,0) sin (3,-1);
\draw[ultra thick, black] (3,-1) cos (6,0);
\draw[ultra thick, black, dashed] (6,0) sin (9,1);
\end{tikzpicture}
\end{center}

  Consider the $pN$-point DFT of $f(x)$ on the expanded $pL$ grid (ODD SYMMETRIC - simplest conceptual case)
  $$\mathcal{F}[k] = \sum_{n=-pN/2+1}^{pN/2}e^{-i2\pi k x / (pL)}$$
\begin{align*}
  (\mathcal{F}\!f)[k] &= \sum_{n=-pN/2}^{pN/2} e^{i2\pi k' x / L}\,e^{-i2\pi k x / (pL)}
  = \sum_{n=-pN/2}^{pN/2} \exp\biggl(-i2\pi \biggl( {k\over pL} - {k'\over L} \biggr) x \biggr)\\
  &= \sum_{n=-pN/2}^{pN/2} \exp\biggl(-i2\pi \biggl( {k\over pL} - {pk'\over pL} \biggr) x \biggr)
  = \sum_{n=-pN/2}^{pN/2} e^{-i2\pi (k - pk') x / (pL)}\\
\intertext{On the spatial (temporal) grid the position is $x = n \triangle x$, which on the expanded grid becomes $x = n {pL\over pN} = n L / N$ and (((UPDATE THESE FORMULAS...))) } 
  &= \sum_{n=-pN/2}^{pN/2} e^{-i2\pi (k - pk') {(n L)\over N}{1\over (pL)}}
  = \sum_{n=-pN/2}^{pN/2} e^{-i2\pi (k - pk') n / (pN)}\\
  &= \sum_{n=-pN/2}^{pN/2} \Bigl(e^{-i2\pi (k - pk') / (pN)}\Bigr)^n
  = { e^{-i2\pi (k - pk') (-pN/2) / (pN)} - e^{-i2\pi (k - pk') (pN/2+1)/ (pN)} \over 1 - e^{-i2\pi (k - pk') / (pN) } }\\
\intertext{\ul{Up to this point no conventions regarding end-points have been taken into account}.
  On first attempt, every possible convenience will be made.
  First, try asserting the end points are equal}
  &= { e^{i\pi (k - pk')} + e^{-i\pi (k - pk')} \over 2 } + 
  \sum_{n=-pN/2+1}^{pN/2-1} \Bigl(e^{-i2\pi (k - pk') / (pN)}\Bigr)^n\\
  &= \cos(\pi (k - pk')) + { e^{-i2\pi (k - pk') (-pN/2+1) / (pN)} - e^{-i2\pi (k - pk') (pN/2)/ (pN)} \over 1 - e^{-i2\pi (k - pk') / (pN) } }\\
\end{align*}
  Focusing on the term on the right and factoring the numerator and the denominator gives
  $${ e^{-i\pi (k - pk') / (pN)} \biggl( e^{-i2\pi (k - pk') (-pN/2+1) / (pN) + i\pi(k-pk')/(pN)} - e^{-i2\pi (k - pk') (pN/2) / (pN) + i\pi(K-pk')/(pN)} \biggr) \over e^{-i\pi (k - pk') / (pN)} \biggl( e^{i\pi (k - pk') / (pN)} - e^{-i\pi (k - pk') / (pN)} \biggr) }=$$
  $${ e^{-i{2\pi\over(pN)} (k - pk') \bigl(-{pN\over2}+1\bigr) + i{\pi\over(pN)}(k-pk')} - e^{-i{2\pi\over(pN)}(k - pk') ({pN\over2}) + i{\pi\over(pN)}(k-pk')} \over  e^{i\pi (k - pk') / (pN)} - e^{-i\pi (k - pk') / (pN)} }$$
  The denominator is $(i2)\sin(\pi(k-pk')/(pN))$.
  Focusing on the numerator
  $$e^{i{2\pi\over2pN} (k - pk') (pN - 2) + i{2\pi\over2pN}(k-pk')} - e^{-i{2\pi\over2pN}(k - pk') (pN) + i{2\pi\over2pN}(k-pk')}$$
  $$= e^{i{2\pi\over2pN} (k - pk') (pN - 2 + 1)} - e^{-i{2\pi\over2pN}(k - pk') (pN - 1)}$$
  $$= e^{i{\pi\over pN} (k - pk') (pN - 1)} - e^{-i{\pi\over pN}(k - pk') (pN - 1)}$$
  $$= (i2)\sin(\pi(k-pk')(pN-1)/(pN))$$
  
  Collecting these results
\begin{align*}
  \mathcal{F}[k] &= \cos(\pi (k - pk')) + {\sin(\pi(k-pk')(pN-1)/(pN)) \over \sin(\pi(k-pk')/(pN))}\\
  \intertext{Combining terms}
  &={ \cos(\pi(k-pk'))\sin(\pi(k-pk')/(pN)) + \sin\Bigl(\pi(k-pk')\Bigl(1-{1\over pN}\Bigr)\Bigr) \over \sin(\pi(k-pk')/(pN)) }\\
  \intertext{And using the trig identity $\sin(\theta-\phi) = \sin\theta\cos\phi - \cos\theta\sin\phi$}
		& = \Biggl( \cos\bigl(\pi(k-pk')\bigr)\sin\biggl({\pi\over pN}(k-pk')\biggr)
		+ \sin\bigl(\pi(k-pk')\bigr)\cos\biggl({\pi\over pN}(k-pk')\biggr)\\
		&\qquad -\cos\bigl(\pi(k-pk')\bigr)\sin\biggl({\pi\over pN}(k-pk')\biggr) \Biggr)
		\Bigg/ \sin\biggl({\pi\over pN}(k-pk')\biggr)\\
\end{align*}
  Say in words what this thing is... the DFT of the complex exponential defined (with fundamental mode of wavelength $L$) over the extent $L$ computed over the extent $pL$ (with modes with a fundamental wavelength of length $pL$)...say this better...
  $$\boxed{\mathcal{F}[k] = { \sin\bigl(\pi(k-pk')\bigr) \cos\bigl(\pi(k-pk')/(pN)\bigr) \over \sin\bigl(\pi(k-pk')/(pN)\bigr) }}$$

  %Something to check out: https://appliedacousticschalmers.github.io/scaling-of-the-dft/AES2020_eBrief/


  ***********************************************************************************

  AHEAD:

  an approach to deminishing leakage error is by adopting a different window, Hanning, Gaussian, etc. (p.609 \cite{Jerri}), to name a few.

  Jerri puts a ``conservative choice'' for the sampling duration to $10\times$ the wavelength of the fundamental frequency, or equivalently $10\times$ the bandwidth(recall $\Omega L = N$)\cite{Jerri}(4.H Deciding the Period for the Discrete Fourier Transform).
  Also: Chu also has advice on setting sampling duration (p.263, sec.8.6.2)\cite{Chu}.

  Jerri NOTES\cite{Jerri}:
  Jerri defines a bandlimited function as being bandlimited between $(-a, a)$, and the ''band limit'' to be $a$ (p.631 \cite{Jerri}).
  Note: Jerri points out that if the sampling duration is far from an integral multiple of the signal component period then the Hanning window helps less and less...
  ...Jerri's approach to dealing with leakage is to apply super-Gaussian (Jerri likes order equal to 6) windows (p.634 \cite{Jerri}), but even this approach is only moderately better.
  Also: Weaver p.134 on Super-Gaussian windows
 
  Brigham NOTES\cite{Brigham}:
  sec.9.2 techniques for reducing leakage.
 

  ***********************************************************************************


  ...
  AHEAD: adapt this for the digital context
  1. consider of expanded grid $pL$ to have expanded indices $pN$
 
  Consider how the indexes change when expanding the axis and the grid points along with it...
  Consider the function $f(x) = e^{i2\pi k' x / L}$
  And consider the DFT $\mathcal{F} = \sum_{n=0}^{N-1} e^{-i2\pi k n / N}$
  Indices $k'$, $k$, $n$, and L or N need to be adjusted to formulate
  $$\mathcal{F} f$$



  \section{Coding Preliminaries}

  Issues of Error, Accuracy, and Stability: floating-point, roundoff error, truncation error, stability, (Numerical Recipes section 1.1) math is hard ... ref\cite{NR}.

  Integer math is exact, division throws away the remainder.

  Machines implement floating-point in their own way
  processor floating-point representation specified in standards IEEE 754-1985, IEEE 854-1987, IEEE 754-2008,


  \subsection{Coding Preliminaries: Floating-point Math}
  %###################################################
  Resources: https://floating-point-gui.de/
  ''What Every Programmer Should Know About Floating-Point Arithmetic!?! or Why don’t my numbers add up?''
  %###################################################

  %###################################################
  rubinhlandau, Oregon State Univ., YouTube channel on Computational Physics
  single precision (32 bit): 6-7 decimal places of precision
  double precision (64 bit): 15-16 decimal places of precision 
  %###################################################

  %###################################################
  START: John D. Cook's blog - Anatomy of a floating point number (-doesn't use hyphen- not written as floating-point)

  Normalized scientific notation in base 10 is when the {\bf significand} is between $[1, 10)$, ie: $1.0\times 10^2 = 100$.
  Normalized scientific notation in base 2 is when the {\bf significand} is between $[1, 2)$.
  The 64 bit IEEE 754 double precision floating-point standard definition is
  $$(\pm) (1.\text{fraction})\times (2)^{\text{exponent}}$$
  $$= (-1^{(\text{s})}) (2 - 2^{-(f)})\times (2)^{(\text{e})-1023}$$
  Indexing from the left, bit 1 (s) sets the sign ($\pm$), bits 2-12 (e) provide an exponent, and the remaining 52 bits (f) set the precision.
  Given the significand for normalized scientific notation always starts with 1, this bit (pun intended) of information does not need to be stored!
  Just the fractional portion of the significand, (1.fraction), is represented in storage.
  As a result, 52 bits can effectively be used to encompass 53 bits worth of precision (can you demonstrate this?).

  *******
  MUST ALSO DISCUSS: Two's Compliment (read wikipedia page at least)
  *******

  The 11 (e) bits can represent $\text{e}^{11}$ integers, starting from 0 to $(\text{e}^{11}-1)$, not to over-count zero, $[0, 2047]$
  Giving (e) a bias of 1023 shifts this set of integers to between $[-1023, 1024]$.
  But these extrema of the set, (e) values corresponding to -1023 and 1024, 11 zeroes and 11 ones, respectively, are reserved for special use.
  This reduces the range of integer values (e) can take on to between $[-1022, 1023]$.
  Note, 0 cannot be accessed by $\pm (1.\text{f}\,) \times (2)^{\text{e}}$.
  The bitstring consisting of all zeros is reserved for zero.
\begin{verbatim}
julia> bitstring(0.0)
"0000000000000000000000000000000000000000000000000000000000000000"
\end{verbatim}

  In Julia the skeleton of a floating-point number can be output using
\begin{verbatimtab}[4]
# skeleton of a floating-point number
function float_skeleton(a)
	bs = bitstring(float(a))
	println("Floating-point definition: (s) (2)^(e-1023) (1.f) \n",                                                "sign-bit (s) = ", bs[1], "\n",
			"exponent-bit (e) = ", bs[2:12], "\n",
			"fraction-bit (f) = ", bs[13:64])
end
\end{verbatimtab}
 
  When (e) is maxed out, the bit-string consisting of 11 ones is reserved for the representation of Inf and NaN.
  In Julia, (e) of 11 ones, and (f) of 52 zeros is positive {\tt Inf}
\begin{verbatim}
julia> float_skeleton(Inf)
s = 0
e = 11111111111
f = 0000000000000000000000000000000000000000000000000000
\end{verbatim}

  In Julia, {\tt NaN} is positive, with (e) of 11 ones, and (f) with a leading bit of 1 followed by 51 zeros
\begin{verbatim}
julia> float_skeleton(NaN)
s = 0
e = 11111111111
f = 1000000000000000000000000000000000000000000000000000
\end{verbatim}

  %###########################################
  END: John D. Cook's blog - Anatomy of a floating point number (-doesn't use hyphen- not written as floating-point)
  %###########################################


  %###########################################
  START: Everything You Wanted to Know About Floating Point Numbers, And didn't know who to ask - Avik Sengupta
  %###########################################
 
  %###########################################
  END: Everything You Wanted to Know About Floating Point Numbers, And didn't know who to ask - Avik Sengupta
  %###########################################
  


  ...DEEPER INTO THE WEEDS OF FLOATING-POINT NUMBERS...
  when (e) is all zeros, is also used to represent ``denormalized'' floating point numbers???...  


  ...EVEN MORE DEEP INTO THE WEEDS OF (BEYOND) FLOATING-POINT NUMBERS
  Interval arithmetic.
  We know floating point numbers have ``gaps'' such that one is always estimating and dealing with the error associated.
  One approach to do math on intervals instead of points, this may seem like a drastic step to take but it is easier to make certain guarantees about intervals, as in the true answer lies within some range which can be made sufficiently small; and this is easier to work with in the end than to try and interrogate the validity of a floating-point number generated by a hugely complicated computation.
  %###################################################

 








  floating point numbers have been optimized for speed and efficiency - explain  

  ...the hard part about floats is the decimal value... 

\begin{verbatim}
  # binary (64 bit) representation of an Int
  julia> bitstring(1)
  "0011111111110000000000000000000000000000000000000000000000000000"

  # IEEE 64 bit (double precision) representation of a Float
  julia> bitstring(float(1))
  "0011111111110000000000000000000000000000000000000000000000000000"
\end{verbatim}

  floating point numbers use scientific notation in binary, explain:
  - factor number into a value between 1 to 2, and a power of 2.

  IEEE 754 64 bit double-precision binary floating-point format: 
  $$(-1)^{\text{sign bit}} (1 + \text{fraction}) \times 2^{\,\text{exponent} - \text{bias}}$$   
  The 64 available bits are distributed into 3 sections: sign (1 bit), exponent (11 bit), fraction (52 bit)
\begin{itemize}
  \item bit 1 (sign bit): determines the sign of the number
	\begin{itemize}
	  \item for positive numbers sign bit = 0
	  \item for negative numbers sign bit = 1
	\end{itemize}

  \item bits 2 to 12 (fraction):
  \begin{itemize}
	\item Fractions in binary are constructed like
		$$2^{-1} = {1\over 2}$$
		which in decimal notation is
		$$= 0.5$$
		but in binary notation is written as
		$$= 0.1_{\text{binary}}$$
	\item Construction of smaller fractions is done through taking successive powers
		$$2^{-2} = {1\over 2}{1\over 2} = 0.01_{\text{binary}} = 0.25_{\text{decimal}} $$
  \end{itemize}
	
  \item bits 13 to 64:
\end{itemize}








  
  
  Demystifying Floating Point - John Farrier, CppCon2015 
  "What Every Computer Scientist Should Know About Floating-Point Arithmetic" David Goldberg, 1991, Computing Survyes.
  Ensure every floating-point representation is unique (zero is a special case (why?)), and has a unique opposite, .
  Esure (which?) mathematical identities hold true (why are these the important ones?)
  $$x + y == y + x$$
  $$x + 0 == x$$
  $$\text{if} (x == y) , \text{then} x - y == 0$$
  $${x\over \sqrt{x^2 + y^2}} \le 1$$


  \subsection{Floating-point Error Correction}

  Section(5.6) in Numerical Recipes for how to handle round off error (ie: this is important for using the quadratic formula)


  \subsection{Floating-point Optimization: Julia}

  We want to write an implementation of the FFT for an embedded system...But first, some notes on floating point numbers and how operations on them are implemented by computers.


  Notes from ``Optimizing Floating Point Math in Julia'' by Oscar Smith in JuliaCon 2022 YouTube video.
  Computers store numbers in binary scientific notation
  $$x = s \cdot 2^e \cdot m$$
  where $s$ is the sign, the exponent $e$ is a positive or negative integer, and $m$ is a {\it mantisa} between 1 and 2.
  This format was standardized in IEEE 754 (in 1985).
  The two main sizes
\begin{itemize}
  \item 32 bit: 4 billion values, ``relative tolerance of $10e^{-7}$''(???)
  \item 64 bit: 16 quadrillion values, ``about 16 decimal places of accuracy''
\end{itemize}

  CPUs provide operations, ie: $+,-,*,/$
  A {\tt C} library exists called {\tt Libm} that provides access to other functions.
  Every operating system has its own implementation of the math library.
  Linux version is slow, Windows version is broken, Apple version is good.
  Julia version has better cross platform compatibility *consider if computer has fused multiply add operations*

  What the hard-ware provides is what is available...
  Avoid {\bf division} and {\tt if/else} (although you'll probably need 1 or 2), because they're slow.
  Core routine efficiently approximates functions and is fast on a modern computer.
  Polynomials are one approach, which can be evaluated quickly by Horner's rule
  $$a_0 + a_1 x + a_2 x^2 + a_3 x^3 = a_0 + x (a_1 + x (a_2 + a_3 x)) $$
  where an $n$-degree polynomial can be re-written as only $n$ additions and $n$ multiplications.
  Often only a few (relatively) terms of the polynomial are needed for approximation.
  Usually have exponential-ish convergence.

  Taylor Series
  $$f(x) \approx f(x_0) + x f'(x_0)^2 + \ldots$$
  Easy to compute coefficients (how?).
  
  To get the optimal polynomial use Minmax polynomials, which are the polynomials with the minimum maximal error.
  They can be generated using Remez algorithm provided by Remez.jl package.
  Significantly more accurate than Taylor polynomials.
  Not as fast (don't calculate at runtime).
  Let's you chose the range over which you're approximating.

  Rational approximations generally have better worse-case error (a measure of accuracy) than the polynomial approximation of the same degree, but require division (which is slow) so the rational approximation generally needs to be good at about 3 - 4 degrees lower than the polynomial approximation to be worth the computational overhead, and generally this is not the case. 
  But to approximate over a wide range the rational approximation may be more accurate.
  Remez.jl also provides rational functions, but proceed with caution.

  Fused-multiply-add {\tt fma} is your friend, computes $\text{fma}(a, b, c) = a\cdot b + c$, as fast as just a multiplication on most CPUs, and more accurately by only performing one rounding operation at the end.
  ...Really good for polynomials.
  {\tt fma} is also useful for error correction/ implementing more accurate arithmetic, to compensate for all the error in your calculations, ie:
  $\text{fma}(a, -b, a\cdot b) = \text{the error in computing $a\cdot b$} $ 

  Julia has {\tt fma}, and {\tt muladd}, which uses processor specific compiler heuristics chosen at initialization decide whether to fuse or not.
  To get the error properties of {\tt fma} use {\tt fma}.
  But if you only care about the performance then {\tt muladd}.
  Using the {\tt MuladdMacro.jl} package the compiler will turn all additions and multiplications into {\tt muladd}'s, so you don't have to!

  Next topic: Range reduction
  ...


  \section{Practical Implemention of the DFT - The FFT Algorithm}


  ``The Fast Fourier Transform [is] the most valuable numerical algorithm in our lifetime''(Gil Strang, p.495, Linear Algebra Ed.3)
  Gil Strang uses the $w_N=e^{i2\pi/N}$ notation, so I should use it too!

  when evaluating the algorithm think of time as the number or arithmetic operations, assm it takes constant time to multiply or add two real numbers.


  It is fair to say that any good FFT algorithm I write {\it should} use $N$ equal to some power of 2.
  This is almost sure to include some zero-padding...come back!




  ...This should be like a warm up, early encounter with the beast you're trying to tame...

  The DFT matrix for $N=4$ is a good starting example because it is easy to evaluate the matrix elements.


  ...building up the DFT matrix uisng $$w=e^{-i2\pi} = \cos(2\pi) - i\sin(2\pi)$$
  is evaluated at the points
\begin{center}
\begin{tikzpicture}
% left axis
\draw (-5,2) node{$\cos(2\pi x)$};
\draw[-] (-9,0) node[] {$|$} node[anchor=north, yshift=-1.5ex] {$0$}
	  -- (-7,0) node[] {$|$}
	  -- (-5,0) node[] {$|$} node[anchor=north, yshift=5ex] {$\pi$}
	  -- (-3,0) node[] {$|$}
	  -- (-1,0) node[] {$|$} node[anchor=north, yshift=-1.5ex] {$2\pi$};
% right axis
\draw (5,2) node{$-\sin(2\pi x)$};
\draw[-] (1, 0) node[] {$|$} node[anchor=north, yshift=5ex] {$0$}
	  -- (3,0)  node[] {$|$}
	  -- (5,0)  node[] {$|$} node[anchor=north, yshift=5ex] {$\pi$}
	  -- (7,0)  node[] {$|$} 
	  -- (9,0)  node[] {$|$} 
	  -- (9,0) node[anchor=north, yshift=-1.5ex] {$2\pi$};
% cos(x)
\draw[ultra thick, black] (-9,1) cos (-7,0);
\draw[ultra thick, black] (-7,0) sin (-5,-1);
\draw[ultra thick, black] (-5,-1) cos (-3,0);
\draw[ultra thick, black] (-3,0) sin (-1,1);
% sin(x)
\draw[ultra thick, black] (1,0) sin (3,-1);
\draw[ultra thick, black] (3,-1) cos (5,0);
\draw[ultra thick, black] (5,0) sin (7,1);
\draw[ultra thick, black] (7,1) cos (9,0);
% cos(x) points
\filldraw[black] (-9,1)  circle (.1);
\filldraw[black] (-7,0)  circle (.1);
\filldraw[black] (-5,-1)  circle (.1);
\filldraw[black] (-3,0)  circle (.1);
% sin(x) points
\filldraw[black] (1,0)  circle (.1);
\filldraw[black] (3,-1)  circle (.1);
\filldraw[black] (5,0)  circle (.1);
\filldraw[black] (7,1)  circle (.1);
\end{tikzpicture}
\end{center}

  with cycles repeating every $2\pi$ interval such that the $N=4$ DFT matrix is

\begin{align*}
W_4 &=
\begin{bmatrix}
  w^{-(0)(0)/4} & w^{-(0)(1)/4} & w^{-(0)(2)/4} & w^{-(0)(3)/4} \\
  w^{-(1)(0)/4} & w^{-(1)(1)/4} & w^{-(1)(2)/4} & w^{-(1)(3)/4} \\
  w^{-(2)(0)/4} & w^{-(2)(1)/4} & w^{-(2)(2)/4} & w^{-(2)(3)/4} \\
  w^{-(3)(0)/4} & w^{-(3)(1)/4} & w^{-(3)(2)/4} & w^{-(3)(3)/4} \\
\end{bmatrix}
\\
&=
\begin{bmatrix}
  1 & 1 & 1 & 1 \\
  1 & w^{-1/4} & w^{-2/4} & w^{-3/4} \\
  1 & w^{-2/4} & w^{-4/4} & w^{-6/4} \\
  1 & w^{-3/4} & w^{-6/4} & w^{-9/4} \\
\end{bmatrix}
=
\begin{bmatrix}
  1 & 1 & 1 & 1 \\
  1 & w^{-1/4} & w^{-1/2} 	& w^{-3/4} \\
  1 & w^{-1/2} & w^{-1} 	& w^{-(1+2/4)} \\
  1 & w^{-3/4} & w^{-(1+2/4)} & w^{-(2+1/4)} \\
\end{bmatrix}
\\
&=
\begin{bmatrix}
  1 & 1 & 1 & 1 \\
  1 & w^{-1/4} & w^{-1/2} 		& w^{-3/4} \\
  1 & w^{-1/2} & w^{-1}			& w^{-1} w^{-1/2} \\
  1 & w^{-3/4} & w^{-1} w^{-1/2} & w^{-2} w^{-1/4} \\
\end{bmatrix}
=
\begin{bmatrix}
  1 & 1 & 1 & 1 \\
  1 & w^{-1/4} & w^{-1/2} & w^{-3/4} \\
  1 & w^{-1/2} & w^{-1}	  & w^{-1/2} \\
  1 & w^{-3/4} & w^{-1/2} & w^{-1/4} \\
\end{bmatrix}
\\
&=
\begin{bmatrix}
  1 & 1		& 1 	& 1  \\
  1 & -i 	& -1 	& i \\
  1 & -1	& 1		& -1 \\
  1 & i		& -1 	& -i  \\
\end{bmatrix}
\end{align*}

  A check that this is indeed the correct matrix, note the columns should be linearly independent.
  The dot product of any two vectors should equal zero.
  Recall, for complex vectors the dot product is $\bar{a}^{T} b = a^{H} b$, as in $a$-transpose-complex-conjugate, or $a$-Hermitian dotted with $b$.

  Sadly the definition for the DFT that I have used is not the one that is used in modern implementation...
  For reasons that will become clear in what follows, the DFT matrix is built up by the elements $$w^{kn/N}=e^{i2\pi kn/N} = \cos\Bigl({2\pi kn\over N}\Bigr) + i\sin\Bigl({2\pi kn\over N}\Bigr)$$
  For $N=4$, the expression $w^{kn/N}$ gets evaluated at the grid points $0$, $\pi/2$, $\pi$, $3\pi/2$
\begin{center}
\begin{tikzpicture}
% left axis
\draw (-5,2) node{$\cos(2\pi x)$};
\draw[-] (-9,0) node[] {$|$} node[anchor=north, yshift=-1.5ex] {$0$}
	  -- (-7,0) node[] {$|$}
	  -- (-5,0) node[] {$|$} node[anchor=north, yshift=5ex] {$\pi$}
	  -- (-3,0) node[] {$|$}
	  -- (-1,0) node[] {$|$} node[anchor=north, yshift=-1.5ex] {$2\pi$};
% right axis
\draw (5,2) node{$\sin(2\pi x)$};
\draw[-] (1, 0) node[] {$|$} node[anchor=north, yshift=-1.5ex] {$0$}
	  -- (3,0)  node[] {$|$}
	  -- (5,0)  node[] {$|$} node[anchor=north, yshift=-1.5ex] {$\pi$}
	  -- (7,0)  node[] {$|$} 
	  -- (9,0)  node[] {$|$} 
	  -- (9,0) node[anchor=north, yshift=5ex] {$2\pi$};
% cos(x)
\draw[ultra thick, black] (-9,1) cos (-7,0);
\draw[ultra thick, black] (-7,0) sin (-5,-1);
\draw[ultra thick, black] (-5,-1) cos (-3,0);
\draw[ultra thick, black] (-3,0) sin (-1,1);
% sin(x)
\draw[ultra thick, black] (1,0) sin (3,1);
\draw[ultra thick, black] (3,1) cos (5,0);
\draw[ultra thick, black] (5,0) sin (7,-1);
\draw[ultra thick, black] (7,-1) cos (9,0);
% cos(x) points
\filldraw[black] (-9,1)  circle (.1);
\filldraw[black] (-7,0)  circle (.1);
\filldraw[black] (-5,-1)  circle (.1);
\filldraw[black] (-3,0)  circle (.1);
% sin(x) points
\filldraw[black] (1,0)  circle (.1);
\filldraw[black] (3,1)  circle (.1);
\filldraw[black] (5,0)  circle (.1);
\filldraw[black] (7,-1)  circle (.1);
\end{tikzpicture}
\end{center}

  with cycles repeating every $2\pi$ interval such that the $N=4$ DFT matrix is

\begin{align*}
W_4 &=
\begin{bmatrix}
  w^{(0)(0)/4} & w^{(0)(1)/4} & w^{(0)(2)/4} & w^{(0)(3)/4} \\
  w^{(1)(0)/4} & w^{(1)(1)/4} & w^{(1)(2)/4} & w^{(1)(3)/4} \\
  w^{(2)(0)/4} & w^{(2)(1)/4} & w^{(2)(2)/4} & w^{(2)(3)/4} \\
  w^{(3)(0)/4} & w^{(3)(1)/4} & w^{(3)(2)/4} & w^{(3)(3)/4} \\
\end{bmatrix}
\\
&=
\begin{bmatrix}
  1 & 1 & 1 & 1 \\
  1 & w^{1/4} & w^{2/4} & w^{3/4} \\
  1 & w^{2/4} & w^{4/4} & w^{6/4} \\
  1 & w^{3/4} & w^{6/4} & w^{9/4} \\
\end{bmatrix}
=
\begin{bmatrix}
  1 & 1 & 1 & 1 \\
  1 & w^{1/4} & w^{1/2} & w^{3/4} \\
  1 & w^{1/2} & w^{1}	& w^{1/2} \\
  1 & w^{3/4} & w^{1/2} & w^{1/4} \\
\end{bmatrix}
\\
&=
\begin{bmatrix}
  1 &  1 &  1 &  1 \\
  1 &  i & -1 & -i \\
  1 & -1 &  1 & -1 \\
  1 & -i & -1 &  i \\
\end{bmatrix}
\end{align*}

  In modern implementations this is the DFT in its entirety.
  Note, in the notation of Numerical Methods, the definition of the DFT
  (1) does not have a ($-$) negative sign associated with the complex exponential,
  (2) lacks a factor of $N^{-1}$,
  (and a minor detail is it) uses the index $j$ (I use the index $n$).
  The fundamental step involved in defining the FFT algorith, the Danielson and Lanczos {\it lemma} (Numerical Recipes\cite{NumericalRecipes}), is based on the recursive call to the DFT
\begin{align*}
  F[k, n, N] &= \sum_{n=0}^{N-1} f[n] e^{i2\pi k n / N} \qquad{\text{where } k, n \in \{0, 1, 2,..., N-1\}} \\
\intertext{, which can be broken up into two DFTs of half the length by separating the function/signal along the even and odd indices (after all, no DFT samples all points)}
  &= \sum_{n=0}^{N/2-1} f[2n] e^{i2\pi k ( 2n ) / N} + \sum_{n=0}^{N/2-1} f[ 2n + 1 ] e^{i2\pi k ( 2n + 1 ) / N} \\
\intertext{, where the $n$-index, and concomitantly the $k$-index both run over the restricted domain, from ie: $[0,1,2,...N/2-1]$ - indices involved in ``splitting'' the DFT are primed here to distinguish them from the originals, whilst referring to the same respective grids}
  &= \sum_{n'=0}^{N/2-1} f[2n'] e^{i2\pi k' ( 2n' ) / N} + e^{i2\pi k' / N} \sum_{n'=0}^{N/2-1} f[ 2n' + 1 ] e^{i2\pi k' ( 2n' ) / N} \\
  &= \sum_{n'=0}^{N/2-1} f[2n'] e^{i2\pi k' n' / (N/2)} + e^{i2\pi k' / N} \sum_{n'=0}^{N/2-1} f[ 2n' + 1 ] e^{i2\pi k' n' / (N/2)} \\
\end{align*}

  The $k'$ values of the $N/2$-point DFT
  $$ \Bigl\{ k' \,\Bigl|\, 0,\ldots,N/2-1 \Bigr\} $$
  can be extended to recover the latter half of the $k$ values
  $$ \Bigl\{ k \,\Bigl|\, N/2,\ldots,N-1 \Bigr\} $$ 
  of the original $N$-point DFT via
  $$k = k' + N/2$$

  Continuing the DFT computation for $\{k \, | \, N/2,\ldots,N-1 \}$ with $k = k' + N/2$
\begin{align*}
  F_{k' + N/2} &= \sum_{n'=0}^{N/2-1} f[2n'] e^{i2\pi (k' + N/2) n' / (N/2)} + e^{i2\pi (k' + N/2) / N} \sum_{n'=0}^{N/2-1} f[ 2n' + 1 ] e^{i2\pi (k' + N/2) n' / (N/2)} \\
\intertext{The complex exponentials of $N/2$-point DFTs are $N/2$-periodic in $k'$}
  &= \sum_{n'=0}^{N/2-1} f[2n'] e^{i2\pi k' n' / (N/2)} + e^{i2\pi (k' + N/2) / N} \sum_{n'=0}^{N/2-1} f[ 2n' + 1 ] e^{i2\pi k' n' / (N/2)} \\
\intertext{The complex exponential coefficient, $w^{i2\pi/N}$, on the odd DFT is $N$-periodic, so the $N/2$ additional rotation returns $-1$}
  &= \sum_{n'=0}^{N/2-1} f[2n'] e^{i2\pi k' n' / (N/2)} + e^{i2\pi (N/2) / N} e^{i2\pi k' / N} \sum_{n'=0}^{N/2-1} f[ 2n' + 1 ] e^{i2\pi k' n' / (N/2)} \\
  &= \sum_{n'=0}^{N/2-1} f[2n'] e^{i2\pi k' n' / (N/2)} + e^{i\pi} e^{i2\pi k' / N} \sum_{n'=0}^{N/2-1} f[ 2n' + 1 ] e^{i2\pi k' n' / (N/2)} \\
  &= \sum_{n'=0}^{N/2-1} f[2n'] e^{i2\pi k' n' / (N/2)} - e^{i2\pi k' / N} \sum_{n'=0}^{N/2-1} f[ 2n' + 1 ] e^{i2\pi k' n' / (N/2)} \\
\end{align*}
  where $\{k' \, | \, 0,\ldots,N/2-1\}$.

  In summary, if the number of samples, $N$, is even, and $n$ is indexed starting from from 0 (Using the definition of the DFT given in Numerical recipes, lacks the $1/N$ factor, and the complex exponential is positive rather than negative (probably all for reasons related to computer computation))
  $$\boxed{
	(\mathcal{F}f)[k] =
	\begin{cases}
		\text{If } k = k' \in [0,1,2,\ldots,N/2-1] \\
		 \qquad \sum\limits_{n'=0}^{N/2-1} f[2n'] e^{i2\pi k' n' / (N/2)} + e^{i2\pi k' / N} \sum\limits_{n'=0}^{N/2-1} f[2n'+1] e^{i2\pi k' n' / (N/2)} \\
		\\
		\text{If } k = k' + N/2 \in [N/2,\ldots,N-1] \\
  		\qquad \sum\limits_{n'=0}^{N/2-1} f[2n'] e^{i2\pi k' n' / (N/2)} - e^{i2\pi k' / N} \sum\limits_{n'=0}^{N/2-1} f[ 2n' + 1 ] e^{i2\pi k' n' / (N/2)} \\
	\end{cases}
	}
  $$
  where primed indices, $\Bigl\{ k', n' \, \Bigl|\, 0,1,2,\ldots,N/2-1\Bigr\}$, distinguish those involved in the splitting process from those, $\Bigl\{ k, n \,\Bigl|\, 0,1,2,\ldots,N-1 \Bigr\}$, of the original DFT.

  The splitting process continues recursively until the resulting series are of length 1, at which point we are done, ``since the DFT of a sequence of length 1 is itself''(p.382 The DFT Book, \cite{}) (...understand this final statement...maybe do $N=4$).

  The summation formulation makes the splitting process clear and intuitive, but is cumbersome to proceed with as the algorithm continues recursively.
  The fundamental step of splitting the DFT summation along even and odd index values can be written via factorization of, $W$, the DFT matrix
$$ W_{N} f =
  \biggl(
  B_{N}
  \begin{bmatrix}
	W_{N/2} & 0 \\
	0 & W_{N/2} \\
  \end{bmatrix}
  P_N 
  \biggr)
  f
$$
  And $P$ is a permutation matrix that partitions the signal along even and odd indices
  $$ P f =
	\begin{bmatrix}
	  f_{\text{even}} \\
	  f_{\text{odd}}
	\end{bmatrix}
  $$
  For example
  $$ P_4 f =
	\begin{bmatrix}
	  1 & 0 & 0 & 0 \\
	  0 & 0 & 1 & 0 \\
	  0 & 1 & 0 & 0 \\
	  0 & 0 & 0 & 1
	\end{bmatrix}
	\begin{bmatrix}
	  a \\ b \\ c \\ d
	\end{bmatrix}
	= 
	\begin{bmatrix}
	  a \\ c \\ b \\ d
	\end{bmatrix}
  $$
  And $B$ is often referred to as a butterfly matrix
$$
  B_{N} =
\begin{bmatrix}
  I_{N/2} & \Omega_{N/2} \\
  I_{N/2} & -\Omega_{N/2}
\end{bmatrix}
$$
  where $\Omega_{N/2}$ is the, $N/2\times N/2$, square matrix with the elements, ie: $\Bigl\{ w^{n / N} \Bigr\}_{n=0}^{n=N-1}$, along the diagonal.

  The DFT decomposition 
$$ W_N f =
  \biggl(
  B_N
  \begin{bmatrix}
	W_{N/2} & 0 \\
	0 & W_{N/2}
  \end{bmatrix}
  P_N 
  \biggr)
  f
$$
  can be recursively applied to the DFT matrices of the middle box diagonal matrix of DFTs.
  Let's look at an example before going for something more general.
  For example, $N=4$
\begin{align*}
  W_4 f =
  B_4
  \begin{bmatrix}
	W_2 & 0 \\
	0 & W_2
  \end{bmatrix}
  P_4
  \begin{bmatrix}
	f_0 \\ f_1 \\ f_2 \\ f_3
  \end{bmatrix}
  &=
  B_4
  \begin{bmatrix}
  \biggl( B_2
  \begin{bmatrix}
	W_1 & 0 \\
	0 & W_1
  \end{bmatrix} P_2 \biggr) & 0 \\
  0 & 
  \biggl(
  B_2
  \begin{bmatrix}
	W_1 & 0 \\
	0 & W_1
  \end{bmatrix}
  P_2
  \biggr)
  \end{bmatrix}
  \begin{bmatrix}
	f_0 \\ f_2 \\ f_1 \\ f_3
  \end{bmatrix} \\
  &=
  B_4
  \begin{bmatrix}
	B_{2} & 0 \\
	0 & B_{2}
  \end{bmatrix}
  \begin{bmatrix}
	W_1 & 0 & 0 & 0 \\
	0 & W_1 & 0 & 0 \\
	0 & 0 & W_1 & 0 \\
	0 & 0 & 0 & W_1
  \end{bmatrix}
  \begin{bmatrix}
	P_2 & 0 \\
	0 & P_2
  \end{bmatrix}
  \begin{bmatrix}
	f_0 \\ f_2 \\ f_1 \\ f_3
  \end{bmatrix} \\
  &=
  B_4
  \begin{bmatrix}
	B_{2} & 0 \\
	0 & B_{2}
  \end{bmatrix}
  \begin{bmatrix}
	W_1 & 0 & 0 & 0 \\
	0 & W_1 & 0 & 0 \\
	0 & 0 & W_1 & 0 \\
	0 & 0 & 0 & W_1
  \end{bmatrix}
  \begin{bmatrix}
	f_0 \\ f_2 \\ f_1 \\ f_3
  \end{bmatrix} \\
\intertext{The Fourier transform of length 1 - regardless of the DFT definition used - is the identity
  $$ \sum_{n=0}^{N-1} f[n] e^{i2\pi k n / N}
  = \sum_{n=0}^{1-1} f[n] e^{i2\pi k 0 / N}
  = \sum_{n=0}^{0} f[n] (1)
  = f[0] (1) $$
  Here, the signal/function is evaluated at an arbitrary point, $[n=0]$ simply refers to the index given.
  The $N=4$ DFT matrix decomposition simplified, or $N=4$ FFT, is}
  &=
  B_4
  \begin{bmatrix}
	B_{2} & 0 \\
	0 & B_{2}
  \end{bmatrix}
  \begin{bmatrix}
	f_0 \\ f_2 \\ f_1 \\ f_3
  \end{bmatrix}
\end{align*}

  Following this procedure, more generally
\begin{align*}
  W_N f &=
  B_N
  \begin{bmatrix}
	W_{N/2} & 0 \\
	0 & W_{N/2}
  \end{bmatrix}
  P_N f =
  B_N
  \begin{bmatrix}
	W_{N/2} & 0 \\
	0 & W_{N/2}
  \end{bmatrix}
  \begin{bmatrix}
    f_{\text{even}} \\
    f_{\text{odd}}
  \end{bmatrix}
  =
  B_N
  \begin{bmatrix}
	F_{\text{even}} \\
	F_{\text{odd}}
  \end{bmatrix} \\
\intertext{
  Taking this further - even/odd gets abreviated e/o - the DFT decomposition is applied to the DFTs along the signal/function at even and odd indices, respectively
  $$ F_{\text{e}} =
  B_{N/2}
  \begin{bmatrix}
	W_{N/4} & 0 \\
	0 & W_{N/4}
  \end{bmatrix}
  P_{N/2} f_{\text{e}}
  = 
  B_{N/2}
  \begin{bmatrix}
	W_{N/4} & 0 \\
	0 & W_{N/4}
  \end{bmatrix}
  \begin{bmatrix}
	f_{\text{ee}} \\
	f_{\text{eo}}
  \end{bmatrix}
  = 
  B_{N/2}
  \begin{bmatrix}
	F_{\text{ee}} \\
	F_{\text{eo}}
  \end{bmatrix}
  $$
  $$ F_{\text{o}} =
  B_{N/2}
  \begin{bmatrix}
	W_{N/4} & 0 \\
	0 & W_{N/4}
  \end{bmatrix}
  P_{N/2} f_{\text{o}}
  = 
  B_{N/2}
  \begin{bmatrix}
	W_{N/4} & 0 \\
	0 & W_{N/4}
  \end{bmatrix}
  \begin{bmatrix}
	f_{\text{oe}} \\
	f_{\text{oo}}
  \end{bmatrix}
  = 
  B_{N/2}
  \begin{bmatrix}
	F_{\text{oe}} \\
	F_{\text{oo}}
  \end{bmatrix}
  $$
}
\intertext{Using these formulas to perform the factorization}
  &=
  B_N
  \begin{bmatrix}
	B_{N/2} & 0 \\
	0 & B_{N/2}
  \end{bmatrix}
  \begin{bmatrix}
	W_{N/4} & 0 & 0 & 0 \\
	0 & W_{N/4} & 0 & 0 \\
	0 & 0 & W_{N/4} & 0 \\
	0 & 0 & 0 & W_{N/4}
  \end{bmatrix}
  \begin{bmatrix}
	P_{N/2} & 0 \\
	0 & P_{N/2}
  \end{bmatrix}
  P_N f \\
  &=
  B_N
  \begin{bmatrix}
	B_{N/2} & 0 \\
	0 & B_{N/2}
  \end{bmatrix}
  \begin{bmatrix}
	W_{N/4} & 0 & 0 & 0 \\
	0 & W_{N/4} & 0 & 0 \\
	0 & 0 & W_{N/4} & 0 \\
	0 & 0 & 0 & W_{N/4}
  \end{bmatrix}
  \begin{bmatrix}
	P_{N/2} & 0 \\
	0 & P_{N/2}
  \end{bmatrix}
  \begin{bmatrix}
	f_{\text{e}} \\
	f_{\text{o}}
  \end{bmatrix} \\
  &= 
  B_N
  \begin{bmatrix}
	B_{N/2} & 0 \\
	0 & B_{N/2}
  \end{bmatrix}
  \begin{bmatrix}
	W_{N/4} & 0 & 0 & 0 \\
	0 & W_{N/4} & 0 & 0 \\
	0 & 0 & W_{N/4} & 0 \\
	0 & 0 & 0 & W_{N/4}
  \end{bmatrix}
  \begin{bmatrix}
	f_{\text{ee}} \\
	f_{\text{eo}} \\
	f_{\text{oe}} \\
	f_{\text{oo}}
  \end{bmatrix} \\
  &=
  B_N
  \begin{bmatrix}
	B_{N/2} & 0 \\
	0 & B_{N/2}
  \end{bmatrix}
  \begin{bmatrix}
	F_{\text{ee}} \\
	F_{\text{eo}} \\
	F_{\text{oe}} \\
	F_{\text{oo}}
  \end{bmatrix}
\end{align*}
  This procedure proceeds until the middle box diagonal matrix of DFTs is the identity matrix and all that remains is a sequence of butterfly matrices to the left, and permutation matrices on the right.
  The complete DFT factorization that leads to the FFT is (notation for the function/signal is omitted to fit the page)
  $$ \boxed{ W_N \!=\!
	B_N
	\begin{bmatrix}
	  B_{N/2} & 0 \\
	  0 & B_{N/2}
	\end{bmatrix}
	\!\cdots\!
	\begin{bmatrix}
	  B_2 & \cdots & 0 & \cdots & 0 \\
	  0 & \ddots & 0 & \cdots & 0 \\
	  \vdots & 0 & B_2 & 0 & \vdots \\
	  0 & \cdots & 0 & \ddots & 0 \\
	  0 & \cdots & 0 & \cdots & B_2
	\end{bmatrix}
	I_N
	\begin{bmatrix}
	  P_2 & \cdots & 0 & \cdots & 0 \\
	  0 & \ddots & 0 & \cdots & 0 \\
	  \vdots & 0 & P_2 & 0 & \vdots \\
	  0 & \cdots & 0 & \ddots & 0 \\
	  0 & \cdots & 0 & \cdots & P_2
	\end{bmatrix}
	\!\cdots\!
	\begin{bmatrix}
	  P_{N/2} & 0 \\
	  0 & P_{N/2}
	\end{bmatrix}
	P_N }
  $$
  Note, $P_2$ on a vector of length 2 leaves it unchanged, so the block diagonal matrix of $P_2$'s is redundant.
   (...)
  This is the linear algebra behind the radix-2 FFT algorithm.
  But what does Radix-2 refer to??? 

  In pseudo code the algorithm can be broadly described in two stages, sometimes referred to as split and recombine
\begin{align*}
  &\text{\tt \# Stage 1: split } \\
  &x \leftarrow P^T x \\
  &\text{\tt \# Stage 2: combine} \\
  &\text{for $t$ = 1 : T} \\
  &\qquad x \leftarrow A_t x \\
  &\text{end}
\end{align*}

  \underline{\bf The splitting stage} involves sorting the signal in preparation for matrix multiplication in the next stage.
  % Resources:
  % https://graphics.stanford.edu/~seander/bithacks.html - Bit Twiddling Hacks

\begin{figure}[H]
\centering
\begin{tikzpicture}
\node[text width=2.5cm,align=center] at (-5.5,6) {Binary \\Representation of Signal Indices};
\draw[dashed, color=gray] (-6.5,-4.5) 
	-- (-6.5,4.5) -- (-4.5,4.5)
	-- (-4.5,-4.5) -- (-6.5,-4.5);
\draw[color=gray] (-6,-4) 
	-- (-6,4) -- (-5,4)
	-- (-5,-4) -- (-6,-4);
\draw[color=gray]
	(-6,3)  -- (-5,3)
	(-6,2)  -- (-5,2)
	(-6,1)  -- (-5,1)
	(-6,0)  -- (-5,0)
	(-6,-1) -- (-5,-1)
	(-6,-2) -- (-5,-2)
	(-6,-3) -- (-5,-3);
\node at (-5.5,+3.5) {000};
\node at (-5.5,+2.5) {001};
\node at (-5.5,+1.5) {010};
\node at (-5.5,+0.5) {011};
\node at (-5.5,-0.5) {100};
\node at (-5.5,-1.50) {101};
\node at (-5.5,-2.5) {110};
\node at (-5.5,-3.5) {111};

\node[text width=2.5cm,align=center] at (-2.5,5.5) {Signal Indices};
\draw[color=gray] (-3,-4) 
	-- (-3,4) -- (-2,4)
	-- (-2,-4) -- (-3,-4);
\draw[color=gray]
	(-3,3)  -- (-2,3)
	(-3,2)  -- (-2,2)
	(-3,1)  -- (-2,1)
	(-3,0)  -- (-2,0)
	(-3,-1) -- (-2,-1)
	(-3,-2) -- (-2,-2)
	(-3,-3) -- (-2,-3);
\node at (-2.5,+3.5) {0};
\node at (-2.5,+2.5) {1};
\node at (-2.5,+1.5) {2};
\node at (-2.5,+0.5) {3};
\node at (-2.5,-0.5) {4};
\node at (-2.5,-1.50) {5};
\node at (-2.5,-2.5) {6};
\node at (-2.5,-3.5) {7};

\draw[dashed, color=green] (-2,3.5)  -- (0,3.5);
\draw[dashed, color=green] (-2,1.5)  -- (0,2.5);
\draw[dashed, color=green] (-2,-0.5) -- (0,1.5);
\draw[dashed, color=green] (-2,-2.5) -- (0,0.5);
\draw[dashed, color=blue] (-2,2.5)  -- (0,-0.5);
\draw[dashed, color=blue] (-2,0.5)  -- (0,-1.5);
\draw[dashed, color=blue] (-2,-1.5) -- (0,-2.5);
\draw[dashed, color=blue] (-2,-3.5) -- (0,-3.5);

\node[text width=2.5cm,align=center] at (0.5,5.5) {1st Split};
\draw[color=gray] (0,-4) 
	-- (0,4) -- (1,4)
	-- (1,-4) -- (0,-4);
\draw[color=gray]
	(0,3)  -- (1,3)
	(0,2)  -- (1,2)
	(0,1)  -- (1,1);
\draw[color=red]
	(0,0)  -- (1,0);
\draw[color=gray]
	(0,-1) -- (1,-1)
	(0,-2) -- (1,-2)
	(0,-3) -- (1,-3);
\node at (0.5,+3.5) {0};
\node at (0.5,+2.5) {2};
\node at (0.5,+1.5) {4};
\node at (0.5,+0.5) {6};
\node at (0.5,-0.5) {1};
\node at (0.5,-1.50) {3};
\node at (0.5,-2.5) {5};
\node at (0.5,-3.5) {7};

\draw[dashed, color=green] (1,3.5) -- (3,3.5); 
\draw[dashed, color=green] (1,1.5) -- (3,2.5); 
\draw[dashed, color=green] (1,-0.5) -- (3,-0.5); 
\draw[dashed, color=green] (1,-2.5) -- (3,-1.5); 
\draw[dashed, color=blue] (1,2.5) -- (3,1.5); 
\draw[dashed, color=blue] (1,0.5) -- (3,0.5); 
\draw[dashed, color=blue] (1,-1.5) -- (3,-2.5); 
\draw[dashed, color=blue] (1,-3.5) -- (3,-3.5); 

\node[text width=2.5cm,align=center] at (3.5,5.5) {2nd Split};
\draw[color=gray] (3,-4) 
	-- (3,4)  -- (4,4)
	-- (4,-4) -- (3,-4);
\draw[color=gray]
	(3,3)  -- (4,3);
\draw[color=red]
	(3,2)  -- (4,2);
\draw[color=gray]
	(3,1)  -- (4,1);
\draw[color=red]
	(3,0)  -- (4,0);
\draw[color=gray]
	(3,-1) -- (4,-1);
\draw[color=red]
	(3,-2) -- (4,-2);
\draw[color=gray]
	(3,-3) -- (4,-3);
\node at (3.5,+3.5) {0};
\node at (3.5,+2.5) {4};
\node at (3.5,+1.5) {2};
\node at (3.5,+0.5) {6};
\node at (3.5,-0.5) {1};
\node at (3.5,-1.50) {5};
\node at (3.5,-2.5) {3};
\node at (3.5,-3.5) {7};

\draw[dashed, color=green] (4,3.5) -- (6,3.5); 
\draw[dashed, color=blue]  (4,2.5) -- (6,2.5); 
\draw[dashed, color=green] (4,1.5) -- (6,1.5); 
\draw[dashed, color=blue]  (4,0.5) -- (6,0.5); 
\draw[dashed, color=green] (4,-0.5) -- (6,-0.5); 
\draw[dashed, color=blue]  (4,-1.5) -- (6,-1.5); 
\draw[dashed, color=green] (4,-2.5) -- (6,-2.5); 
\draw[dashed, color=blue]  (4,-3.5) -- (6,-3.5); 

\node[text width=2.5cm,align=center] at (6.5,5.5) {Final Split};
\draw[color=gray] (6,-4) 
	-- (6,4)  -- (7,4)
	-- (7,-4) -- (6,-4);
\draw[color=red]
	(6,3)  -- (7,3)
	(6,2)  -- (7,2)
	(6,1)  -- (7,1)
	(6,0)  -- (7,0)
	(6,-1) -- (7,-1)
	(6,-2) -- (7,-2)
	(6,-3) -- (7,-3);
\node at (6.5,+3.5) {0};
\node at (6.5,+2.5) {4};
\node at (6.5,+1.5) {2};
\node at (6.5,+0.5) {6};
\node at (6.5,-0.5) {1};
\node at (6.5,-1.50) {5};
\node at (6.5,-2.5) {3};
\node at (6.5,-3.5) {7};

\node[text width=2.5cm,align=center] at (9.5,6) {Binary \\Representation \\of Sorted Indices};
\draw[dashed, color=gray] (8.5,-4.5) 
	-- (8.5,4.5)  -- (10.5,4.5)
	-- (10.5,-4.5) -- (8.5,-4.5);
\draw[color=gray] (9,-4) 
	-- (9,4)  -- (10,4)
	-- (10,-4) -- (9,-4);
\draw[color=gray]
	(9,3)  -- (10,3)
	(9,2)  -- (10,2)
	(9,1)  -- (10,1)
	(9,0)  -- (10,0)
	(9,-1) -- (10,-1)
	(9,-2) -- (10,-2)
	(9,-3) -- (10,-3);
\node at (9.5,+3.5) {000};
\node at (9.5,+2.5) {100};
\node at (9.5,+1.5) {010};
\node at (9.5,+0.5) {110};
\node at (9.5,-0.5) {001};
\node at (9.5,-1.50) {101};
\node at (9.5,-2.5) {011};
\node at (9.5,-3.5) {111};
\end{tikzpicture}
\caption{
  Depiction of the complete splitting process, from left to right, applied to a signal/function with \underline{$N=8$}, and $\bigl\{ n \, \bigl| \, 0,\ldots,N-1 \bigr\}$.
  The vertical columns represent the signal with samples organized from top to bottom.
  The $n$-index value is specified in each bin.
  {\color{red}Red} lines partition the signal into even (above) and odd (below) index sequences.
  {\color{green}Green} dashed lines indicate grouping of even indices.
  {\color{blue}Blue} dashed lines indicate grouping of odd indices.
  \underline{Notice, the third and final splitting step is redundant}.
  Binary representation of the original, and final sorted indices are shown at the borders, in dashed boxes.
}
\label{fig:splitting}% Now use \ref{}. Note: placement after caption
\end{figure}

  The fundamental splitting process amounts to multiplication by a permutation matrix, which is repeated until the signal remains unchanged by further splitting.
  This is performed a total of ($T$) times, where $(T)$ is given by $2^T = N$, but the last splitting step is redundant.

  In practice, the sorting process - successive grouping of even and odd sequences, preserving relative order - is efficiently handled by indexing samples in binary and using a procedure sometimes called {\it bit-reversal}.
  I'm not going to prove this, it's just amazing!
  Bit-reversal amounts to writing the binary representation with the reverse order of 1's and 0's.
  In figure \ref{fig:splitting}, the indicies when $N=8$ are written in binary in the dashed box on the left, as well as for the sorted (post splitting) indices in the dashed box on the right. 
  AHEAD: ref. Petr Rosel, Timing of some bit reversal algorithms, Signal Processing, Vol18, Issue4, 1989, 425-433.

  Implementing bit-reversal in Julia.
  In Julia use {\tt bitstring()} to get the full 64bit binary representation of a number.
  To perform bit reversal each binary index needs the same number of digits.
  The index value $N$ determines the left leading zero padding for the index values less than $N$.
  Remove the leading zeros from the {\tt bitstring()} representation so that all binary indices have the same number of digits.
  
\begin{verbatim}
function bitrev(N)
    skip = leading_zeros(N-1)+1 # no. left zero padding in bit-representation to omit

    for i in 0:N-1
        println( parse(Int, reverse(bitstring(i)[skip : end]), base=2) )
    end
end
\end{verbatim}


  \underline{\bf The combine stage} of the FFT algorithm performs the sequence of butterfly matrix multiplications 
  $$ B_N
	\begin{bmatrix}
	  B_{N/2} & 0 \\
	  0 & B_{N/2}
	\end{bmatrix}
	\!\cdots\!
	\begin{bmatrix}
	  B_2 & \cdots & 0 & \cdots & 0 \\
	  0 & \ddots & 0 & \cdots & 0 \\
	  \vdots & 0 & B_2 & 0 & \vdots \\
	  0 & \cdots & 0 & \ddots & 0 \\
	  0 & \cdots & 0 & \cdots & B_2
	\end{bmatrix}
  $$
  on the bit-reverse sorted sequence. 
  Recall the butterfly matrix is
$$
  B_{N} =
\begin{bmatrix}
  I_{N/2} & \Omega_{N/2} \\
  I_{N/2} & -\Omega_{N/2}
\end{bmatrix}
$$
  where $\Omega_{N/2}$ is the, $N/2\times N/2$, square matrix with the elements, $\Bigl\{ w^{n / N} \Bigr\}_n$, ie: $\Bigl\{ w^{n / N} \Bigr\}_{n=0}^{n=N-1}$, along the diagonal.

  The {\tt LinearAlgebra} package provides the identity matrix, {\tt I}, and the diagonal matrix {\tt Diagonal()} for constructing the butterfly matrix.
\begin{verbatim}
function butterfly(N)
    D = cispi.(2*[0:N/2-1...] / (N/2) )
    D = Diagonal(D)                                                                                         [I D; I -D]
	[I D; I -D]
end
\end{verbatim}

  To assemble the kaleidoscope (a group of butterflies) matrices 

  
  Looking at the algorith again from a birds eye view using this terminology
\begin{align*}
  &\text{\tt \# Stage 1: permutation} \\
  &x \leftarrow P^T x \\
  &\text{\tt \# Stage 2: kaleidoscope} \\
  &\text{for $t$ = 1 : T} \\
  &\qquad x \leftarrow A_t x \\
  &\text{end}
\end{align*}

  \subsection{Discrete Spectral Decomposition}

  Recall (sec. 2.1, Fourier Series Using Complex Exponentials), both $k$ and $-k$ are needed to describe the $k$-th mode.
\begin{align*}
  f(x) &= { A_0 \over 2 } + \sum_{k=1}^\infty \Bigl( A_k \cos ( k x ) + B_k \sin ( k x ) \Bigr) \\
  &= { A_0 \over 2 } + \sum_{ k \, = \, 1 }^\infty \biggl( A_k { e^{i \, k x}  + e^{- i \, k x} \over 2 } +  B_k { e^{i \, k x} - e^{ - i \, k x } \over 2 i } \biggr) \\
  &= { A_0 \over 2 } + {1\over2}\sum_{ k \, = \, 1 }^\infty \biggl( A_k - i B_k \biggr) e^{i \, k x} +  {1\over2}\sum_{ k \, = \, -1 }^{-\infty} \biggl( A_{_{-k}} + i B_{_{-k}} \biggr) e^{i \, k x}\\
  &= \sum_{ k \, = \, - \infty }^\infty C_k e^{i k x}
	\qquad \text{, where} \quad
    	\begin{cases}
			C_0 = A_0 / 2 & \text{, if $k = 0$} \\
			C_k = {1\over2}\bigl( A_k - i B_k \bigr) & \text{, if $k > 0$} \\
			C_k = {1\over2}\bigl( A_{_{-k}} + i B_{_{-k}} \bigr) & \text{, if $k < 0$}
    	\end{cases}\\
\end{align*}

  In the case where the index runs from 0 to $N$, the negative indices can be identified by the $N$-periodicity of the DFT.
  The index $k=N-1$ corresponds to $k=-1$, which pertains to $k=1$.
  The index $k=N-2$ corresponds to $k=-2$, which pertains to $k=2$.
  And so on.


  \subsection{Further notes on FFT implementation}
  done in-place
  - what does it mean for the FFT to be done in-place? ans. without the need for to instantiate additional arrays requiring additional storage.
  - this ultimately should avoid the reordering stage?
  - 1.6 The Cooley-Tukey Framework and 1.6.1 An In-Place Formulation
  
  (description of decimation in frequency...amenable to more general values of $N$, and can also be done in place, with same computational cost as the Cooley-Tukey FFT)p.384-385 The DFT Book\cite{BriggsHenson} 
  - Gentleman and Sande 1966 ref64 The DFT Book
  - Stockham ref38, Glassman ref66 The DFT Book

  HOW TO DO THIS??? p.386
  - if you are calculating both the DFT and the IDFT then you can avaoid the rearrangement step....!
  - by using decimation-in-frequency the for forward transform and decimation-in-time for the inverse transform
  

  Is Van Loan\cite{VanLoan} the best resource for the FFT?
  There is some linear algebra notation that gets introduced in the literature, the {\bf Kronecker product} between matrices $A_{pq}$ and $B_{rs}$, which produces the $$pr \times qs$$ matrix
$$
A \otimes B = 
\begin{pmatrix}
  a_{11} B & \cdots & a_{1q} B \\ 
  \vdots & \ddots & \vdots \\
  a_{p1} B & \cdots & a_{pq} B \\ 
\end{pmatrix}
$$
  This notation allows us to write the DFT of the signal $x$ as
  $$W_N x = (I_1 \otimes B_{N}) (I_2 \otimes B_{N/2}) \cdots (I_{N/2} \otimes B_2) P^T x$$
  where $N = 2^T$, $P^T$ is the permutation (sorting/bit-reversal) matrix, and $B$ is a butterfly matrix.

  The ``chirp transform algorithm'' by Oppenheim and Schafer 2010 is a FFT implementation for arbitrary $N$, ie: not a power of 2...But honestly not the place to start...zero padding and $N$-of some power of 2 is more practical implementation to begin with.

 
  \subsection{Zero Padding}
  % Resources:
  % https://graphics.stanford.edu/~seander/bithacks.html - Bit Twiddling Hacks

  COMEBACK: Jeremy Howard's ai tutorials on youtube...


  PRE: computer architecture bit widths, ie: 32, 64

  On 64 bit architecture the largest number that should be possible to represent is $2^{64}$.
  Let's check this in Julia
\begin{verbatim}
julia> 2^64
0
julia> 2^63
-9223372036854775808
\end{verbatim}

  Julia uses machine integers(Two's Complement)/overflow/wrap-around/native arithmetic, because it's fast; and SaferIntegers.jl is one package for working around this.

  Julias out the box uses machine integers aka Two's compliment: binary digit with greatest place value indicate positive or negative, this is called the ``most significant bit'' when 1 indicates negative.
because it's fast.
  To see the binary representation of a number in Julia
\begin{verbatim}
julia> bitstring(2^62)
"0100000000000000000000000000000000000000000000000000000000000000"

julia> bitstring(-2^62)
"1100000000000000000000000000000000000000000000000000000000000000"
\end{verbatim}

  Overflow - going beyond the max value - wraps around
\begin{verbatim}
julia> 2^63
-9223372036854775808

julia> -2^63
-9223372036854775808

julia> bitstring(2^64)
"0000000000000000000000000000000000000000000000000000000000000000"
\end{verbatim}

  To use this technique you must find the next highest power of 2.
  Consider: how your choice of definition of DFT affects this...

  ... perhaps one more thing to consider about the signal before we try to compute its DFT via the FFT is zero-padding, s.t. $N$ is a power of 2.

  the thing about zero-padding is that by modifying the signal you consequently modify the frequency grid also.

  zero padding is done at the end of a sequence\cite{Osgood}.

  Check out section13p1p1 in Numerical Methods\cite{NumericalMethods}
 
  zero padding on the assymmetric grid without $1/N$ in def of DFT (p.464, sec7p11p4 Osgood)
  zero padding on the symmetric grid with $1/N$ in def of DFT (p.90, The DFT Book), here you can see why the $1/N$ factor makes things complicated...

  
 
  \section{Julia Code}
  
  % check this out!!!: MIT 15.401 Finance Theory I, Fall 2008
  %JuliaCon 2019 | How We Wrote a Textbook using Julia - github.com/sisl/tufte_algorithms_book
  %Use:
%	- Pygments: custom lexer and style
%	- Pythontex...
  % Steven G. Johnson - fftw author, photonic crystal book (free, sensor topics),

  Why use Julia: math looks like math, real code is better than pseudo code

\begin{verbatim}
  core ideas:
  - the correct way to represent the signal is a vector
  - the correct way to represent the DFT is a matrix

  random signal
  - use random number generator
  - set N - length of vector
  - return vector of data points
  ie: N=10 signal of 32bit integers
  signal = rand(Int32, 10)

  box function of given width
  - set N - total no. data points
  - set N - no. of data points for box
  - set ON/OFF - set to 1/2 at discontinuities?
  - set position - default center
  - return a vector of data points.

  The boxcar function
  - N - set the number of samples 
  - b - the width of the box
  - default to a centered box (update later)
  - use fill() - equivalent to rep(0, 10) in R
  - use semicolon for vertical concatenation, ie: [10; 15; 21]
function boxcar(N, b)
    [fill(0, div(N-b), 2); fill(1, b); fill(0, div(N-b), 2)]
end
# ...more simply
boxcar(N, b) = [fill(0, div(N-b), 2); fill(1, b); fill(0, div(N-b), 2)]
# But...this is not good enough: cannot handle N odd
julia> boxcar(11, 2)
10-element Vector{Int64}:
 0
 0
 0
 0
 1
 1
 0
 0
 0
 0
# this picks a convention that is aligned with the index range [N/2+1: N/2]
julia> boxcar(N, b) = [fill(0, div(N-b, 2, RoundDown)); fill(1, b); fill(0, div(N-b, 2, RoundUp))]
boxcar (generic function with 1 method)
julia> boxcar(11, 2)
11-element Vector{Int64}:
 0
 0
 0
 0
 1
 1
 0
 0
 0
 0
 0
# *** MUST UPDATE *** boxcar() to use ones() and zeros() functions instead of fill ...	



#####################
# Calc DFT of cos(a*x) #
#####################
N = 4

# generate N samples of the function cos() along the interval 2pi: method 1
julia> samples = [range(start=0, stop=3pi/2, length=4)...]
4-element Vector{Float64}:
 0.0
 1.5707963267948966
 3.141592653589793
 4.71238898038469
# notice the floating-point error (not exactly 0 where it should be at samples 2 and 4)
julia> cos.(samples)
4-element Vector{Float64}:
  1.0
  6.123233995736766e-17
 -1.0
 -1.8369701987210297e-16

# Notice: Julia struggles most when the answer should be 0
julia> x = [range(start=0, stop=3pi/2, length=N)...]
4-element Vector{Float64}:
 0.0
 1.5707963267948966
 3.141592653589793
 4.71238898038469

julia> cos.(2x)
4-element Vector{Float64}:
  1.0
 -1.0
  1.0
 -1.0

# To generate the function samples without the floating-point errors
#... using the axis indices
julia> x = [0:N-1...]
4-element Vector{Int64}:
 0
 1
 2
 3
#...scale the cospi() function as needed
julia> cospi.(2x/N)
4-element Vector{Float64}:
  1.0
  0.0
 -1.0
  0.0
# Just to demonstrate this for cos() with a x-coefficient
julia> x = [0:N-1...]
4-element Vector{Int64}:
 0
 1
 2
 3
julia> cospi.(2*2*x/N)
4-element Vector{Float64}:
  1.0
 -1.0
  1.0
 -1.0
# NOTE: Perhaps the most disconcerting thing about floating-point errors for 0 is not being able to know what sign the answer will be...

# The DFT Matrix
# The complex exponential on pi intervals cispi() broadcast over the (k)x(n) matrix of indices
# Note the use of 0-based indexing convention
function dft_matrix(N)
    cispi.(2/N*[0:N-1...]*[0:N-1...]')
end

# Compute the DFT of a Signal
function dft_compute(f)
	dft_matrix(length(f))*f
end

#####################
# (OLD) Calc DFT of cos() #
#####################
# set N
julia> N = 5
5
# def signal
julia> s = cos.(2*pi/N*[0:N-1...])
5-element Vector{Float64}:
  1.0
  0.30901699437494745
 -0.8090169943749473
 -0.8090169943749475
  0.30901699437494723
# define DFT k,n-indices matrix
julia> W = [0:N-1...] * [0:N-1...]'
5x5 Matrix{Int64}:
 0  0  0   0   0
 0  1  2   3   4
 0  2  4   6   8
 0  3  6   9  12
 0  4  8  12  16
# define DFT exponents from indices matrix
julia> W*=-im*2*pi/N
5x5 Matrix{ComplexF64}:
 0.0-0.0im  0.0-0.0im      0.0-0.0im      0.0-0.0im      0.0-0.0im
 0.0-0.0im  0.0-1.25664im  0.0-2.51327im  0.0-3.76991im  0.0-5.02655im
 0.0-0.0im  0.0-2.51327im  0.0-5.02655im  0.0-7.53982im  0.0-10.0531im
 0.0-0.0im  0.0-3.76991im  0.0-7.53982im  0.0-11.3097im  0.0-15.0796im
 0.0-0.0im  0.0-5.02655im  0.0-10.0531im  0.0-15.0796im  0.0-20.1062im
# define DFT matrix by exponentiation of each element
julia> W=exp.(W)
5x5 Matrix{ComplexF64}:
 1.0-0.0im        1.0-0.0im             1.0-0.0im             1.0-0.0im             1.0-0.0im
 1.0-0.0im   0.309017-0.951057im  -0.809017-0.587785im  -0.809017+0.587785im   0.309017+0.951057im
 1.0-0.0im  -0.809017-0.587785im   0.309017+0.951057im   0.309017-0.951057im  -0.809017+0.587785im
 1.0-0.0im  -0.809017+0.587785im   0.309017-0.951057im   0.309017+0.951057im  -0.809017-0.587785im
 1.0-0.0im   0.309017+0.951057im  -0.809017+0.587785im  -0.809017-0.587785im   0.309017-0.951057im
# compute DFT of signal
julia> F_k = W*s
5-element Vector{ComplexF64}:
 -1.1102230246251565e-16 + 0.0im
                     2.5 - 5.551115123125783e-17im
   5.551115123125783e-17 - 3.0531133177191805e-16im
   3.608224830031759e-16 - 3.608224830031759e-16im
      2.4999999999999996 + 9.992007221626409e-16im
# select individual DFT component
julia> F_k[2]
2.5 - 5.551115123125783e-17im

\end{verbatim}

  This can be simplified using the builtin complex exponential function that has been optimized for evaluation at $\pi$ intervals.
\begin{verbatimtab}[4]
# The DFT matrix
function dft_matrix(N)
	W = [0:N-1...]*[0:N-1...]'
	cispi.(2*W/N)
end
\end{verbatimtab}

  That was the DFT implementation, which is impractical for industrial settings.

  The FFT algorithm: understand how to "break it down, then build it back up to construct the DFT"

  Julia code for decomposing the signal into even and odd sequences (once! the big question is how to use recurssion?)...BUT is this really necessary, can the computation be done on the singal "in place"? 
  select odd indices
  f[1:2:end]
  select even indices
  f[2:2:end]

  NOTE: the secret sauce in the FFTW package by Steven G Johnson to cater to the hardware internals.

  \section{The Discrete Time Fourier Transform (DTFT)}
  The DTFT transforms a discrete time signal to a continuous frequency representation (p.479 prob.7.27 \cite{Osgood})




  \section{Multi-taper Method}

  The multitaper method is a smoothing method 



  \section{Properties of the DFT}

  Introduce a compact notation that emphasizes important properties.
  View the complex exponentials as values from a discrete signal,
  the discrete (vector) complex exponential
  $$\boldsymbol{\omega} = (1, e^{2\pi i/N}, e^{2\pi i 2/N},\ldots, e^{2\pi i (N-1)/N})$$
  where the $m^{th}$ component is
  $$\boldsymbol{\omega}[m] = e^{2\pi i m/N}$$
  Define powers of $\boldsymbol{\omega}$ as
\begin{align*}
  \boldsymbol{\omega}^{(n)} &= \boldsymbol{\omega}^n = (1, e^{2\pi i n/N}, e^{2\pi i 2n/N},\ldots, e^{2\pi i (N-1)n/N}) \\
  \boldsymbol{\omega}^{(-n)} &= \boldsymbol{\omega}^{-n} = (1, e^{-2\pi i n/N}, e^{-2\pi i 2n/N},\ldots, e^{-2\pi i (N-1)n/N})
\end{align*}
  So the DFT is
  $$\mathscr{F}\!f = \sum_{n=0}^{N-1} f[n] \, \boldsymbol{\omega}^{(-n)}$$
  which operates on the index, $m$
  $$\mathscr{F}\!f[m] = \sum_{n=0}^{N-1} f[n] \, \boldsymbol{\omega}^{(-n)}[m]$$

  Properties of the DFT
\begin{enumerate}
  \item Periodicity of the inputs and outputs (Lect. 21) - difference between continuous and discrete cases.
  	\begin{itemize}
		\item ``The definition of the DFT compells you to regard the input $f$ and the output $F$ as not defined only on the interval $[0, N-1]$ but as periodic signals of period N''.
		\item Note: $\omega$ is naturally a periodic signal with period $N$.
  	\end{itemize}

  \item Orthogonality of (powers of) the complex exponentials, $\boldsymbol{\omega}^{(k)} \perp \boldsymbol{\omega}^{(\ell)}$
	$$
 		\boldsymbol{\omega}^{(k)} \cdot \boldsymbol{\omega}^{(\ell)} = 
	   		\begin{cases}
	    		0 & \text{, $k \ne \ell$} \\
	    		N & \text{, $k = \ell$}
     		\end{cases}
	$$
  \begin{itemize}
	\item Most interesting properties of DFT are related to this.
	\item Proof:
	  \begin{align*}
		\intertext{The inner product of $\boldsymbol{\omega}^{(k)}$ and $\boldsymbol{\omega}^{(\ell)}$ is}
		\boldsymbol{\omega}^{(k)} \cdot \boldsymbol{\omega}^{(\ell)} &= \sum_{n=0}^{N-1} \boldsymbol{\omega}^{(k)}[n] \,\overline{\boldsymbol{\omega}^{(\ell)}[n]} \\
		&= \sum_{n=0}^{N-1} e^{2\pi i k n/N} \,\overline{e^{2\pi i \ell n/N}} \\
		&= \sum_{n=0}^{N-1} \biggl(e^{2\pi i (k-\ell)/N}\biggr)^n \\
		\intertext{This is a geometric series, and if $k \ne \ell$}
		&= {1 - \biggl(e^{2\pi i (k-\ell)/N}\biggr)^N \over 1 - e^{2\pi i (k-\ell)/N}} \\
		&= {1 - e^{2\pi i (k-\ell)} \over 1 - e^{2\pi i (k-\ell)/N}} = 0 \\
		\intertext{If $k = \ell$}
		\boldsymbol{\omega}^{(k)} \cdot \boldsymbol{\omega}^{(\ell)} &= N
	  \end{align*}
  \end{itemize}

  \item The complex exponentials are orthogonal but \underline{not orthonormal}
		$$\left\Vert \boldsymbol{w}^{(n)} \right\Vert^2 = \boldsymbol{\omega}^{(u)} \cdot \boldsymbol{\omega}^{(n)} = N$$
  \begin{itemize}
	\item Source of a $N$ or $1/N$ factor in many formulas related to the DFT.
  \end{itemize}

	\item The inverse DFT (IDFT) has a factor of $1/N$, which arises because the vector complex exponentials are not orthonormal, which distinguishes it from its continuous analog
	\begin{align*}
	\mathscr{F}^{-1}\!f[m] &= {1\over N} \sum_{n=1}^{N-1} f[n]\, e^{2\pi i n m/N} \\
	&= {1\over N} \sum_{n=1}^{N-1} f[n] \, \boldsymbol{\omega}^{(n)} [m] \\
	\Aboxed{\mathscr{F}^{-1}\!f &= {1\over N} \sum_{n=1}^{N-1} f[n] \, \boldsymbol{\omega}^{(n)}}
	\end{align*}
	\begin{itemize}
	  \item Proof (show that, $\mathscr{F}^{-1}\mathscr{F}\!f[m] = f[m]$):
	  \begin{align*}
		\mathscr{F}^{-1}\mathscr{F}\!f[m]
		&= {1\over N} \sum_{n=0}^{N-1} \mathscr{F}\!f[n] \,e^{2\pi inm/N} \\	
		&= {1\over N} \sum_{n=0}^{N-1} \biggl(\sum_{k=0}^{N-1} f[k] \,e^{-2pi ikn/N}\biggr) e^{2\pi imn/N}
		\intertext{swap order of summation}
		&= {1\over N} \sum_{k=0}^{N-1} f[k] \sum_{n=0}^{N-1} e^{-2pi ikn/N} e^{2\pi imn/N} \\
		\intertext{Recall}
		\sum_{n=0}^{N-1} e^{-2pi ikn/N} e^{2\pi imn/N} = 
		  \begin{cases}
	    	0 & \text{, $k \ne m$} \\
	    	N & \text{, $k = m$}
		  \end{cases}
	  \end{align*}
	\end{itemize}
\end{enumerate}

  \subsection{Special Cases}: pick up here for updating {\bf boldsymbol} notation

  There are cases where the DFTs is analagous to the continuous Fourier transform.

  The Fourier transform of $f$ at $m=0$
  $$F[0] = \sum_{n=0}^{N-1} f[n]\, \omega^{-1}[0] = \sum_{n=0}^{N-1} f[n] $$
  which is analagous to the continuous case, where $s=0$
  $$(\mathscr{F}\!f)(0) = \int\limits_{-\infty}^\infty f(t) \,d t$$

  Two special discrete signals:

  The constant value function
  $${\bf 1} = (1,\ldots, 1)$$

  The discrete $\delta$-functions are given by
\begin{align*}
  \delta_0 &= (1, 0, 0, 0, \ldots) \\
  \delta_1 &= (0, 1, 0, 0, \ldots) \\
  \delta_2 &= (0, 0, 1, 0, \ldots) \\
  \vdots
\end{align*}

  The DFT of the delta-function at 0, $\delta_0$, is
\begin{align*}
  \mathscr{F}\delta_0 &= \sum_{n=0}^{N-1} \delta_0 [n] \, \boldsymbol{\omega}^{(-n)} \\
  \intertext{The $\delta_0$-function at $0$ pulls out the $0^{\text{th}}$ component of $\boldsymbol{\omega}^{(-n)}$, which is always equal to $1$ regardless of the power of $\boldsymbol{\omega}$.  This happens for every element in the summation, so}
  &= (1) \, \boldsymbol{\omega}^0 \\
  &= (1) (1, 1,\ldots, 1) \\
  &=(1, 1,\ldots, 1) = {\bf 1}
\end{align*}
  analagous to the continuous case.

  The DFT of the delta-function at $k$, $\delta_k$, is
\begin{align*}
  \mathscr{F}\delta_k &= \sum_{n=0}^{N-1} \delta_k [n] \, \omega^{-n} \\
  \intertext{Again, $\delta_k$ pulls out the $k^{\text{th}}$ component of $\boldsymbol{\omega}^{(-n)}$; performed for every element in the summation gives}
  &= (1)\,\boldsymbol{\omega}^{(-k)} \\
  &= \boldsymbol{\omega}^{(-k)}
\end{align*}
  analagous to the continuous case.

  The DFT of the vector of complex exponentials (has a factor of $N$) is
  $$\boxed{\mathscr{F}\boldsymbol{\omega}^{(k)} = N \delta_k}$$
\begin{align*}
  \intertext{Proof:}
  \mathscr{F}\boldsymbol{\omega}^{(k)} &= \sum_{n=0}^{N-1} \boldsymbol{\omega}^{(k)}[n]\,\boldsymbol{\omega}^{(-n)} \\
  &= \sum_{n=0}^{N-1} \boldsymbol{\omega}^{(k)}[n]\,\boldsymbol{\omega}^{(-n)}[m] \\
  &= \sum_{n=0}^{N-1} e^{2\pi ink/N} e^{-2\pi imn/N} \\
  \intertext{this is the inner product of $\boldsymbol{\omega}^{(k)}$ and $\boldsymbol{\omega}^{(m)}$, which is}
  &= \begin{cases}
		0 & \text{, $k \ne m$} \\	
		N & \text{, $k = m$}
	\end{cases} \\
  &=N \delta_k
\end{align*}
  
  The DFT can be viewed as an $\text{N}\times\text{N}$ matrix, which operates on the signal vector.
  Define the $N^{th}$ root of 1 (a scalar) as
  $$\omega = e^{2\pi i/N}$$
  to simplify references to matrix elements.
  Using this notation, the DFT becomes
  $$\mathscr{F}f[m] = \sum_{n=0}^{N-1} \omega^{-nm} f[n]$$
  which suggests matrix multiplication

%(check)
$$
\begin{pmatrix}
  1 & 1 & \ldots & 1 \\
  1 & \omega^{-1} & \ldots & \omega^{-(N-1)} \\
  1 & \omega^{-2} & \ldots & \omega^{-2(N-1)} \\
  1 & \vdots 	& \ddots 	& \vdots \\
  1 & \omega^{-(N-1)} & \ldots & \omega^{-(N-1)^2} \\
\end{pmatrix}  
\begin{pmatrix}
  f[0]   \\
  f[1]   \\
  \vdots \\
  f[N-1] \\
\end{pmatrix}  
= 
\begin{pmatrix}
  \mathscr{F}\!f[0]   \\
  \mathscr{F}\!f[1]   \\
  \vdots \\
  \mathscr{F}\!f[N-1] \\
\end{pmatrix}  
$$

  A usefull mnemonic is for the DFT matrix is
  $$(\mathscr{F})_{nm} = (\omega^{-nm})$$
  Note the DFT is symmetric but not unitary, the adjoint $\mathscr{F}^*$ is not equal to the conjugate transpose of $\mathscr{F}$
  $$\mathscr{F}^*\mathscr{F} = \mathscr{F}\mathscr{F}^* = N \mathbb{I}_{NN} \ne \mathbb{I}$$
  This demonstrates that the IDFT is
  $$\mathscr{F}^{-1} = {1\over N} \mathscr{F}^*$$

  To compute $\mathscr{F}\!f$ requires $N^2$ operations, $\mathcal{O}(N^2)$.
  The Fast Fourier Transform (FFT) reduces this to $\mathcal{O}(N log(N))$.

  (More/Back to general properties of the DFT)

  Periodicity

  An important difference between the Fourier transform and the DFT is both the inputs and the outputs must be \underline{periodic}, because the fundamental building bock of the DFT, the complex exponential, is periodic.
  $$\boldsymbol{\omega}[m] = e^{2\pi im/N}$$
  $$\boldsymbol{\omega}[m+N] = e^{2\pi i(m+N)/N}$$
  $$= e^{2\pi im/N} e^{2\pi iN/N}$$
  DON'T FOLLOW THIS...
  $$ = \omega[m]$$
  Likewise, powers of $\boldsymbol{\omega}^{(k)}$ are also periodic of period N.

  The DFT is a linear combination of periodic signals all with period $N$.
  The output of the DFT is also periodic with period $N$.
  $$\sum_{n=0}^{N-1} f[n] \, \underline{\omega}^{(-n)}$$
  The output of the IDFT is also periodic with period $N$.
  $${1\over N}\sum_{n=0}^{N-1} f[n] \, \underline{\omega}^{(n)}$$
  Therefore the {\em signal must be regarded as periodic} with period $N$.
  This is a feature of the DFT, which differs from the continuous Fourier transform.
  Furthermore, asserting periodicity where it may not be present is an issue in practice...(PICK UP).
  A consequence of periodicity is any set of consecutive $N$ indices will suffice, ie:
  $$\mathscr{F}\!f = \sum_{n=1}^N f[n] \boldsymbol^{(-n)}$$
  in this case, since
  $$f[N] = f[0]$$

  Negative indices can be used provided the cyclic reference is clear, ie:
  $$f[-1] = f[N-1]$$
  $$f[-2] = f[N-2]$$

  Another common convention for $N$ even is
  $$\mathscr{F}\!f = \sum_{n={-1\over N}+1}^{N\over 2} f[n] \boldsymbol^{(-n)}$$
  for $N$ odd is
  $$\mathscr{F}\!f = \sum_{n={-1\over N}}^{N\over 2} f[n] \boldsymbol^{(-n)}$$
  which puts the $0^{\text{th}}$ component in the middle of the spectrum.
  

  Duality results using the reverse signal
  $$f^-[n] = f[-n]$$

  Duality for the DFT
  $$\mathscr{F}\!f^- = (\mathscr{F}f)^-$$
  Duality for the DFT which differs from the continuous case
  $$\mathscr{F}\mathscr{F}\!f = N f^-$$
 

  Do convolution of DFT...(not done in lectures!)
  

  \section{The Discrete Fourier Transform (DFT), Pt. II}

  There are two things you can control, the length / duration (space or time) to sample, (I want to just call this $L$ but it's not standard), and $N$, the number of samples.

  The reciprocity relations provide a conceptual reference when considering the DFT.
  It is useful to start from the temporal/spacial and frequency grids as a foundation for thinking about the DFT.
  These grids suggest just how intimately the DFT is connected with how much time or space can be captured as a single frame, and the number of times you can sample within that domain.
  (Would it be useful to use $T$, or $L$, as the period of time, or length of space, despite notation conventions, for clarity of concepts?)

  There are multiple routes to arrive at the DFT but rather than focus on one let's consider multiple to see what insights different approaches reveal.

  \subsection{Preliminaries: The Number of (Sub-) Intervals v. (Sample) Indices}

  Subtle (important) issue regarding the number of subintervals used to subdivide the spatial/temporal interval and the number of data points sampled.
\begin{center}
  \begin{tabular}{| c | c |}
	\hline
	No. intervals & No. data points \\ [0.5ex]
	\hline
	N	& N + 1 \\
	N-1 & N \\
	\hline
  \end{tabular}
\end{center}
  For $N$ intervals
  $$N\cdot \triangle x = L$$ 
  the width of the frequency interval is
  $$\triangle \omega = {1\over L}$$
  but there are $N+1$ sample points ($x_n$), each with an associated frequency ($\omega_k$).
  Each frequency considered in the DFT is obtained by
  $$n \cdot \triangle\omega = \omega_n$$
  so there are more data points than frequncies to match.
  ***The way to avoid this ambiguity in the meaning of $N$ is to assume/assert that the first and last data points are equal, ie: that the signal is periodic.
  By this construction the first data point technically the last data point, the one associated with the lowest frequency.


  \subsection{Preliminaries: The Symmetric v. Asymmetric Inteval}

  Briggs and Henson like the symmetric interval $x \in [-L/2, L/2]$;
  specifically, the case where the number of subintervals ($N$) is even,
  and the indices run from $n : [-N/2 + 1: N/2] \in \mathbb{Z}$
  because the range of frequencies is $[-{\Omega\over2+1}, {\Omega\over2}]$
  necessarily (not by construction).
  Now there is no ambiguity when specifying the associated $\omega_m = (m)(\triangle\omega)$ as to what is meant by a negative frequency since it is the index not the frequency that carries the negative sign.

  While this convention aleviates the issue of ``negative frequencies'' in teh spatial domain, this convention introduces the concept of ``negative time'' when translating the spatial domain into the temporal domain.



  \subsection{The Fast Fourier Transform (FFT)}

  There are a variet of FFT algorithms that are case specific.
  The basics of treatment by Tukey proceeds as follows.
  The DFT as an $\text{N}\times\text{N}$ matrix is
  $$\mathscr{F} = (\omega^{-nm}) $$
  where
  $$\omega = e^{2\pi i/N}$$
  indexed from $0$ to $N-1$
  which operates on a signal regarded as a column vector.
  This algorithm is of order $\mathcal{O}(N^2)$.

  Side: the popular approach is to use the symmetry of the matrix and properties of complex exponential to factor the DFT matrix into simpler matrices that require fewer multiplications.

  Proceeding simply based on the DFT
  (Notation is becoming ambiguous, note that $\boldsymbol{\omega}^{(-N)}[m] = \omega^{-nm}$)
  $$\mathscr{F}\!f[m] = \sum_{n=0}^{N-1} f[n]\, \omega^{-nm}$$
  Write the complete sum over even and odd indices, to write the order $N$ DFT as a combination of two DFTs of order $N/2$, and then iterate.
  To start requires $N$ is even. 
  To iterager requires $N$ is a power of 2 (zero padding is a technique for dealing with when this assumption is not met).
  Introduce notation to keep track of powers of the complex exponential
  $$\omega[p, q] = e^{2\pi i q / p}$$
  such that
  $$\omega[p, q_1 + q_2] = e^{2\pi i (q_1 + q_2) / p}$$
  $$= \omega[p, q_1] \omega[p, q_2] $$
  $$\omega[p, -nm] = \omega^{-nm} = e^{2\pi i (q_1 + q_2) / p}$$

  For even powers
  $$\omega[N/2, -nm] = e^{-2\pi inm/(N/2)}$$
  $$= e^{-2\pi i (2nm)/N}$$
  $$= \omega[N, -2nm]$$
  
  For odd powers
  $$\omega[N, -(2n+1)m] = \omega[N, -2nm-m]$$
  $$= \omega[N, -2nm] \omega[N, -m]$$
  $$= \omega[N/2, -nm] \omega[N, -m]$$

  Including these results into the DFT
  $$\mathscr{F}\!f[m] = \sum_{n=0}^{N-1} f[n]\, \omega[N, -nm]$$
  write the sum over even and odd indices
  $$= \sum_{n=0}^{(N/2)-1} f[2n]\, \omega[N, -2nm] + \sum_{n=0}^{(N/2)-1} f[2n+1]\, \omega[N, -(2n+1)m]$$
  $$= \sum_{n=0}^{(N/2)-1} f[2n]\, \omega[N/2, -nm] + \sum_{n=0}^{(N/2)-1} f[2n+1]\, \omega[N/2, -nm] \omega[N, -m]$$
  $$= \sum_{n=0}^{(N/2)-1} f[2n]\, \omega[N/2, -nm] + \omega[N, -m] \sum_{n=0}^{(N/2)-1} f[2n+1]\, \omega[N/2, -nm]$$
  which is ``almost''
  $$\mathscr{F}_N\!f = \mathscr{F}_{N/2} f_{even} + \omega[N, -m] \mathscr{F}_{N/2} f_{odd}$$
  however, the right hand side consists of $N/2$-tuples...
  This formula holds for indices $0$ to $N/2 - 1$.
  To get the remaining indices $N/2$ to $N-1$, use the indexing scheme
  $$m + {N\over2}\quad\text{where} m=(0, 1,\ldots, {N\over2}-1)$$
  $$\mathscr{F}_N\!f[m+N/2] =
  \sum_{n=0}^{N/2-1} f[2n]\, \omega[N/2, -n(m+N/2)]
  + \omega[N, -(m+N/2)] \sum_{n=0}^{N/2-1} f[2n+1]\omega[N/2, -(m+N/2)n]$$
  (min 30) come back if you dare!...  

  (Lect 22: min 46)
  ...for indices $m = (0,\dots, N/2-1)$
  $$\mathscr{F}_N f[m] = \mathscr{F}_{N/2} f_{even}[m] + \omega[N, m] \mathscr{F}_{N/2} f_{odd}[m]$$
  ...for indices $m = (0,\dots, N/2-1)$
  $$\mathscr{F}_N f[m+N/2] = \mathscr{F}_{N/2} f_{even}[m] - \omega[N, -m] \mathscr{F}_{N/2} f_{odd}[m]$$
  Now Iterate!
  How to see the reduction in computational complexity $\mathcal{O}(Nlog(N))$?


  \section{(Introduction to) Linear Systems} %Lect 23 

  Major Topics: impulse response, and the transfer function, complex exponentials as eigen functions of linear time invariant systems, *analysis of linear systems in terms of the Fourier transform,

  Def. of linear system: a mapping/function from inputs to outputs that satisifies the principles of super position
  $$L(v_1 + v_2) = L v_1 + L v_2$$
  $$L(\alpha v) = \alpha L v$$
  The extension to infinite sums (and to integrals) 
  $$L (\sum_{n=0}^\infty v_n) = \sum_{n=0}^\infty L v_n$$
  $$L (\sum_{n=0}^\infty \alpha v_n) = \sum_{n=0}^\infty L(\alpha_n v_n) = \sum_{n=0}^\infty \alpha_n L (v_n)$$
  generally requires additional assumptions on the operator (ie: assume ``continuity'' of $L$).

  The quintessential (only!(B. Osgood)) example of a linear system is direct proportionality
  $$L(v_1 + v_2) = \alpha(v_1 + v_2)$$
  $$= \alpha v_1 + \alpha v_2$$
  $$= L v_1 + L v_2$$
  $$L(\alpha v) = \alpha L v$$
  (all linear systems are in some way similar to this case).
  Note, $\alpha$ need not be a constant; multiplication is a linear system.
  $$L v(t) = \alpha(t) v(t)$$
  eg: flipping on a switch for a duration $a$
  $$L v(t) = \Pi_a(t) v(t)$$
  eg: sampling
  $$L v(t) = \Sh_p(t) v(t)$$

  A (slight but important) generalization of ``direct proportionality'' is to include summing up the results, ie: matrix multiplication
  $$A_{m\times n} v_n = w_m$$
  $$(A x)_i = \sum_{j=1}^n a_{i,j} v_j = w_i$$
  Linearity of matrix multiplication
  $$A(v + w) = A v + A w$$
  $$A(\alpha v) = \alpha A v$$

  Common problem
  $$\dot{x} = A v$$
  Initial conditions
  $$x(0) = v$$
  Solution
  $$x(t) = e^{t A} v$$

  special cases: linear systems with special properties derive from special properties of the matrix.
  Square matrices...

  Symmetrix (self adjoint) system
  $$A = A^T$$

  Hermitian (the complex case)
  $$A = A^*$$
  where $A^*$ is the conjugate transpose.

  Unitary
  $$A^* A = \mathbb{I}$$
  
  Often look for eigen vectors and eigen values of $A$.
  A vector, $v \ne 0$, is an eigen vector if
  $$A v = \lambda v$$
  If have eigen vectors
  $$v_1,\ldots, v_n$$
  with corresponding eigen values
  $$\lambda_1,\ldots, \lambda_n$$,
  that form a basis for all the inputs,
  then can analyze $A$ easily,
  any input $v$ can be writen as 
  $$v = \sum_{i=1}^n \alpha_i v_i$$
  $$A v = \sum_{i=1}^n A (\alpha_i v_i)$$
  $$A v = \sum_{i=1}^n  \alpha_i A v_i$$
  $$= \sum_{i=1}^n \alpha_i \lambda_i v_i$$

  When do linear systems have a basis of eigen vectors (can use special properties)?
  The spectral theorem in finite dimensions for matrices states if $A$ is Hermitian then there is an orthonormal basis of eigen vectors.

  (One more property)
  ``It's not just that (matrix) multiplication is a good and natural example of finite dimensional linear systems, it's the only example''.
  Any finite dimensional linear system can be written as matrix multiplication.

  Ex: Inputs are polynomials of degree $\le n$
  $$a_0 + a_1 x + a_2 x^2 + \cdots + a_n x^n$$ 
  And the linear system is given by
  $$L = {d \over \, dx}$$
  Then find $L$ (hint: can be written as an $(n+1)\times(n+1)$ matrix).

  ``There is an analagous statement for the infinite dimensional continuous case''...
  The linear system that generalizes matrix multiplication is integration against a kernel.
  The inputs are functions $v(x)$.
  The kernel is a function $k(x, y)$, which defines the linear system.
  Integration against a kernel is
  $$L v(x) = \int\limits_{-\infty}^\infty k(x, y) v(y) \, dy$$
  is linear because integration is linear.
  This is the infinite dimensional continuous analog of matrix multiplication.
  Think of $v$ as an infinite column vector.
  Think of $k(x, y)$ as a matrix with $x$ the row index and $y$ the column index.
  And integration is the summation.

  Special linear systems arise by extra assumptions on the kernel $k(x, y)$.
  The analog for the kernel of the transpose of a matrix is the symmetry (self adjoint) condition
  $$k(x, y) = k(y, x)$$
  (self adjoint linear system).
  Hermitian symmetry in the case of a complex kernel is
  $$k(x, y) = \overline{k(y, x)}$$
  Orthogonality...

  Examples of linear systems given by integration against a kernel, ie:
  Ex: the Fourier transform, 
  $$\mathscr{F}\!f(s) = \int\limits_{-\infty}^\infty e^{-2\pi ist} f(t) \,d t$$
  where
  $$k(s, t) = e^{-2\pi ist}$$
  is symmetric...

  Ex: convolution
  $$L v = h * v$$ 
  $$L v(x) = \int\limits_{-\infty}^\infty h(x-y) v(y) \,d y$$
  Note, for a linear system given by convolution the kernel does not depend on $x$ and $y$ separately but on their difference.
  This property leads to convolution being a linear shift/time invariant system, ie:
  $$x \longrightarrow x - a$$
  $$y \longrightarrow y - a$$
  $$x-y \longrightarrow x-a - (y - a) = x-y$$

  ``It's not just that integration against a kernel is a good example of continuous infinite dimensional linear systems, it's the \underline{only} example!''...requires some assumptions...That is, any linear system can be written as integration against a kernel.

  %lect 24
  linear systems continued
  in the discrete finite dimensional case any linear system is given by a matrix.
  in the continuous infinite dimensional case, where the inputs and outputs are functions, any linear system is give by integration against a kernel.
  $$L v(x) = \int\limits_{-\infty}^\infty k(x, y) v(y) \,d y$$

  Digression on cascading(engineering)/composing(math) linear system
\begin{figure}[H]
\centering
\begin{tikzpicture}
% Linear systems L and M
\draw[draw=black] (-1, 0) rectangle (0, 1) node[pos=.5] {L};
\draw[draw=black] (2, 0) rectangle (3, 1) node[pos=.5] {M};

% Connections
\draw[->] (-2,0.5) node[anchor=south] {$v$}
	-- (-1,0.5);
\draw[->] (0,0.5)
	-- (2,0.5);
\draw[->] (3,0.5)
	-- (4,0.5) node[anchor=south] {$w$};

\end{tikzpicture}
\end{figure}
  If $L$ and $M$ are each linear then
  $$M L v = w$$
  so is their cascade.

  When $L$ and $M$ are given by integration against a kernel
  $$L v(x) = \int\limits_{-\infty}^\infty k(x, y) v(y) \,d y$$
  Note, subscript $x$ on $M_x$ suggests how $M$ operates on $k$
  $$M L v(x) = \int\limits_{-\infty}^\infty M_x k(x, y) v(y) \,d y$$

  Proof:
  $$\int\limits_{-\infty}^\infty k(x, y) v(y) \,d y \approx \sum_i k(x, y_i) v(y_i) \triangle y_i$$
  $$M \Biggl( \sum_i k(x, y_i) v(y_i) \triangle y_i\Biggr)$$
  $$= \sum_i M\Biggl(k(x, y_i) v(y_i) \triangle y_i\Biggr)$$
  Note, for $M$ operates on the $x$ variable while is $y$ fixed.
  $$= \sum_i M_x(k(x, y_i)) v(y_i) \triangle y_i$$
  $$\approx \int\limits_{-\infty}^\infty M_x (k(x, y)) v(y) \,d y$$
  $$M L v(x) = \int\limits_{-\infty}^\infty M(k(x, y)) v(y) \,d y$$
  A rigorous proof requires more work.
  This is further support for the idea that any linear system can be represented as integration against some kernel.
  
  Continuous analog of expressing a vector as the sum of its components
  $$v(x) = \int\limits_{-\infty}^\infty \delta(x-y) v(y) \,d y$$

  If $L$ is a linear system then $L v$ is given by applying $L$ to the integral
  $$L v(x) = L \Biggl( \int\limits_{-\infty}^\infty \delta(x-y) v(y) \,d y\Biggr)$$
  $$= \int\limits_{-\infty}^\infty L_x (\delta(x-y)) v(y) \,d y$$
  Set $$k(x, y) = L_x (\delta(x-y))$$
  then
  $$L v(x) = \int\limits_{-\infty}^\infty k(x, y) v(y) \,d y$$
  $\delta$ is called an ``impulse''
  when you feed an impulse into a system the system response, so $k(x, y)$ is called the ``impulse response''.
  The system $L$ responds to an impulsive input namely $\delta(x-y)$.
  Note: $h$ is typically used instead of $k$.
  
  The Schwartz Kernel Theorem
  (The deepest fact in the theory of distributions is used daily by every electrical engineers)
  $L$ is a linear operator on distributions (assumptions on continuity and boundedness) then there is a unique kernel $k$ which is another distribution such that 
  $$L v =  \langle k, v\rangle$$

  Examples:

  Ex 1:
  What is the impulse response of the Fourier transform viewed as a linear system?
  Multiple approaches.
  (PickUP: Lect24: 24min)

  $$\mathscr{F}(\delta(x-y)) = e^{2\pi ixy}$$
  In addition
  $$\mathscr{F}\!f(x) = \int\limits_{-\infty}^\infty e^{-2\pi ixy} f(y) \,d y$$
  the Fourier tranform as a linear system can be viewed as integration against a kernel ($h$, traditional notation)
  $$h(x, y) = e^{-2\pi ixy}$$
  Schwartz Theorem says this kernel (the ``impulse response'') is unique.  
  ...Important point: If you can express a linear operator as integration you have found the impulse response.

  Ex 2: (Prove this to yourself)
  ...What about the finite dimensional discrete case?
  Any finite dimensional linear system is given by multiplication by a matrix.
  What is the impulse response?
  The impulse response is the sytem itself, ie: the matrix.
  
  Ex 3: The switch
  $$L v = \Pi v$$ 
  What is the impulse response?
  $L$ operating on the $x$ variable.
  $$L (\delta(x-y)) = \Pi(x) \delta(x-y)$$
  using the sampling property of the $\delta$ function
  $$= \Pi(y) \delta(x-y)$$
  Then the impulse response is
  $$h(x, y) = \Pi(y) \delta(x-y)$$
  Check this
  $$\int\limits_{-\infty}^\infty h(x, y) v(y) \,d y$$
  $$= \int\limits_{-\infty}^\infty \Pi(y) \delta(x-y) v(y) \,d y$$
  $$= \int\limits_{-\infty}^\infty \delta(x-y) \Pi(y) v(y) \,d y$$
  $$= \Pi(x) v(x)$$
  which is the system.

  Special case where the system is given by convolution
  $$L v = h * v$$
  where $h$ is fixed.
  $$L v(x) = L v(x) = \int\limits_{-\infty}^\infty h(x-y) v(y) \,d y$$
  Here, can appeal to the Schwartz kernel theorem: the system is given by integration against a kernel then the impulse response is the kernel.
  Conclusion:
  $$L (\delta(x-y) = h(x-y)$$
  Can also check this directy from the uniqueness aspect of the defition of the Schewartz kernel Theorem.

  Recall, there is a special property of convolution that makes it especially important for linear systems.
  There is a (simple) relationship between convolution and delay (shift).
  Write the delay operator as
  $$\tau_z v(x) = v(x-a)$$
  Is the delay operator a linear operator?
  What is its impulse response? 
  (do this, Show:)
  Convolution of a delayed signal is the delay of the convolution of the signal, ie:
  $$h * (\tau_a v) = \tau_a (h * v)$$ 
  Reinterpreted in terms of linear systems, this says if the system is given by convolution
  $$L v = h * v$$
  $$w = L v = h * v$$
  Delay of an input
  $$v \longrightarrow v(x-a)$$
  causes an identical delay of the output.

\begin{figure}[H]
\centering
\begin{tikzpicture}

\draw [->] (-2,2) node[anchor=south] {$v$}
		-- (-2,0) node[anchor=east] {$L $}
		-- (-2,0) node[anchor=west] {$ h * v$}
		-- (-2,-2) node[anchor=east] {$w$};

\draw [->] (-2,2)
		-- (0, 2) node[anchor=south] {$\tau_a v$}
		-- (2, 2) node[anchor=west] {$v(t-a)$};

\draw [->] (2, 2)
		-- (2, 0) node[anchor=west] {$\boxed{h * (\tau_a v) = \tau_a(h * v)}$}
		-- (2, 0) node[anchor=east] {$L (\tau_a v)$}
		-- (2, -2);

\draw [->] (-2, -2)
		-- (0, -2) node[anchor=north] {$\tau_a w$}
		-- (2, -2) node[anchor=west] {$w(t-a)$};

\end{tikzpicture}
\end{figure}

  Convolution is a time/shift invariant system
  $$w = L v$$
  $$w(x-a) = L(v(x-a))$$
  $$L(\tau_a v) = \tau_a (L v)$$
  Or $L$ commutes with shifts
  $$L\tau_a = \tau_a L$$
  \underline{\em The converse is also true}!
  If the system is time invariant then it must be given by convolution.
  (Note: ``the switch'' is not time invariant.)
  ...But, why is this true?
  We know that for any linear system
  $$L v(x) = \int\limits_{-\infty}^\infty L(\delta(x-y)) v(y) \,d y$$
  is given by the impulse response integrated against the function.
  What is $L(\delta(x-y))$ ?
  Let
  $$L\delta(x) = h(x)$$
  then
  $$L\delta(x-y) = L(\tau_y \delta)$$
  But for a time invariant system the shifted input goes to the shifted output, so
  $$\tau_y h(x) = h(x-y)$$
  So
  $$L v(x) = \int\limits_{-\infty}^\infty h(x-y) v(y) \,d y$$
  $$h(x) = L(\delta(x))$$
  $$h(x-y) = L(\delta(x-y))$$

  Back to ``the switch''
  $$w = \Pi v$$
  $$L v = \Pi v$$
  $$h(x, y) = \Pi(y) \delta(x-y)$$
  which is not of the form $f(x-y)$ alone. 

  Recap:
  1. any linear system is integration against a kernel, and the kernel is the impulse response.
  - special case: a time invariant system is also integration against a kernel but the integration is given by convolution, the ``impulse response'' is given by the difference of the variables not the variables independently.
  2. a system is time invariant iff it is given by convolution.
  - ``any time the convolution comes into the picture, the Fourier transform cannot be far behind''(BOsgood)

  %Lect 25
  \subsection{Linear Time Invariant (LTI) Systems and the Fourier Transform}
  
  General structure of a linear system $L$ in terms of the impulse response
  $$h(x, y) = L(\delta(x-y))$$

  The Schwartz Kernel Theorem: 
  the output of a linear system is given by integrating the input agaist the impulse resonse, ie:  
  if 
  $$w(x) = L v(x)$$
  then
  $$w(x) = \int\limits_{-\infty}^\infty h(x, y) v(y) \,d y$$

  In the special case of an LTI system:
  a delay in the input results in a corresponding delay of the output, ie:
  if
  $$w(x) = L v(x)$$
  then
  $$w(x-y) = L(v(x-y))$$
  Furthermore, a linear system is time invariant {\em iff} if it is given by convolution.
  The impulse response 
  $$h(x) = L(\delta(x))$$
  and by time invariance
  $$h(x-y) = L(\delta(x-y))$$
  and the action of the system is given by convolution
  $$w(x) = \int\limits_{-\infty}^\infty h(x-y) v(y) \,d y$$

  The same considerations hold for linear finite discrete systems
  $$L v(x) = A v$$
  given by matrix multiplication.
  $L$ is LTI iff $$w = h * v$$
  where $h$ is the impulse response, $v$ is the input vector, $w$ is the output vector.
  $$h = L\delta$$
  $$h[n-m] = L\delta_m$$

  If 
  $$w = A v$$
  then $A$ has a special form for time invariant systems
  eg:
  $$h = (1, 2, 3, 4) = 
\begin{pmatrix}
  1 \\
  2 \\
  3 \\
  4 \\
\end{pmatrix}$$
  $$w = A v = h * v$$
  What is $A$?
  Columns of $A$ are 
  $$A \delta_0 = h * \delta_0$$
  $$A \delta_1 = h * \delta_1$$
  $$A \delta_2 = h * \delta_2$$
  $$A \delta_3 = h * \delta_3$$

  $$(h * \delta_1)[m] = h[m-1]$$

  $$(h * \delta_1) =
\begin{pmatrix}
  4 \\
  1 \\
  2 \\
  3 \\
\end{pmatrix} =
(h * \delta_1)[0] = h[-1] = h[3] = 4$$
  
  $$(h * \delta_2) =
\begin{pmatrix}
  3 \\
  4 \\
  1 \\
  2 \\
\end{pmatrix}$$

  $$(h * \delta_3) =
\begin{pmatrix}
  2 \\
  3 \\
  4 \\
  1 \\
\end{pmatrix}$$

  So the matrix $A$ is
  $$ A = \begin{pmatrix}
  1 & 4 & 3 & 2 \\
  2 & 1 & 4 & 3 \\
  3 & 2 & 1 & 4 \\
  4 & 3 & 2 & 1 \\
\end{pmatrix}$$
  is a ``circulant'' (triplet) matrix.
  Notice,
  1. the elements along the diagonal are constant, 
  and 2. each column is obtained from the previous column by a shift.
  
  The Fourier transform for LTI systems.

  For a LTI system...
  $$w = h * v$$ 
  Take the Fourier transform and use the convolution theorem
  $$\mathscr{F} w = (\mathscr{F} h) (\mathscr{F} v)$$
  $$W(s) = H(s) V(s)$$
  The ``impulse response'' is $h$ ($h(x)$ or $h(x-y)$, depending on situation).
  The ``transfer/system function'' is $H(s)$.
  In the frequency domian the system is given by direct proportion (multiplication).
  In the time domian the system is ... convolution.
  For LTI systems the complex exponentials are eigen functions...
  (...)
  Input is $$v(x) = e^{2\pi i \nu x}$$
  What is $L v(x)$ (sometimes called the ``frequency response'')?
  What is the F.T.?
  Work in the frequency domain
  $$\mathscr{F} (e^{2\pi i \nu x}) = \delta(x - \nu)$$
  Then the output is
  $$W(s) = H(s) \delta(s-\nu)$$
  $$W(s) = H(\nu) \delta(s-\nu)$$
  Go back to the time domain (take the inverse Fourier transform)
  $$w(x) = H(\nu) e^{2\pi i \nu x}$$
  ie: 
  $$L (e^{2\pi i \nu x}) = H(\nu) e^{2\pi i \nu x}$$
  This says, $e^{2\pi i \nu x}$ is an eigen function(/vector) for {\em any} LTI system with eigen value $H(\nu)$.
  The complex exponentials form a basis of eigen functions...can diagonalize the operator associated with a LTI system.
  Note, however, in general, $\sin(x)$ and $\cos(x)$ themselves are not eigen functions of LTI systems, ie:
  Given
  $$v(x) = \cos(2\pi\nu x)$$
  What is $L v(x)$?
  $$L (\cos(2\pi\nu x) = L\Biggl({e^{2\pi i\nu x} + e^{-2\pi i\nu x}\over 2}\Biggr)$$
  $$= {1\over 2} \Bigl( L(e^{2\pi i\nu x}) + L(e^{-2\pi i\nu x}) \Bigr)$$
  $$= {1\over 2} \Bigl( H(\nu) e^{2\pi i\nu x} + H(-\nu) e^{-2\pi i\nu x} \Bigr)$$
  Without further assumptions this cannot be simplified.
  If $h$ is real, then $h$ has symmetry 
  $$H(-\nu) = \overline{H(\nu)}$$
  $$L(cos(2\pi \nu x)$$
  $$= {1\over 2} \Bigl( H(\nu) e^{2\pi i\nu x} + \overline{H(\nu)} \overline{e^{2\pi i\nu x}} \Bigr)$$
  which is the real part
  $$Re(H(\nu) e^{2\pi i \nu x})$$
  still stuck, this is not an eigen function.
  Writing $H(\nu)$ in terms of the magnitude and the phase
  $$H(\nu) = |H(\nu)| e^{i\phi}$$
  then
  $$H(\nu) e^{2\pi i \nu x} = |H(\nu)| e^{2\pi i(\nu x + \phi)}$$
  $$Re(H(\nu) e^{2\pi i \nu x}) = |H(\nu)| cos(2\pi\nu x + \phi)$$
  which is still not an eigen function but as close as you can get.
  With additional assumptions (which?) can these be made eigen functions?

  The discrete version:
  $$w = L v = h * v$$
  $$W[m] = H[m] V[m]$$
  Discrete complex exponentials are eigen vectors.
  ie: 
  $$v = \boldsymbol{\omega}^k$$
  $$\mathscr{F}(\boldsymbol{\omega}^k) = N \delta_k$$
  What is $L (\boldsymbol{\omega}^k)$?
  In the frequency domain
  $$W = H N \delta_k$$
  $$= H[k] N \delta_k$$
  In the time domain
  $$w = H[k] \boldsymbol{\omega}^k$$  
  $$ L(\boldsymbol{\omega}^k) = H[k] \boldsymbol{\omega}^k$$  
  Recall, powers of $\boldsymbol{\omega}^k$ cycle.
  So
  $$({\bf 1}, \boldsymbol{\omega}, \boldsymbol{\omega}^2,\ldots, \boldsymbol{\omega}^{N-1})$$
  form a basis (not orthonormal) of eigen vectors for {\em any} LTI system.

  Ex: $$w = h * v$$
  $$h = (1, 2, 3, 4)$$
  Given by $$w = A v$$
  where
  $$ A = \begin{pmatrix}
  1 & 4 & 3 & 2 \\
  2 & 1 & 4 & 3 \\
  3 & 2 & 1 & 4 \\
  4 & 3 & 2 & 1 \\
\end{pmatrix}$$
  So eigen vectors of the system are eigen vectors of $A$
  The eigen values are given by the values of the transfer function
  $$H[0], H[1], H[2], H[3]$$
  $$H = \mathscr{F} h = \sum_{k=0}^3 h[k] \boldsymbol{\omega}^{(-k)}$$
  $$= \sum_{k=0}^3 (k+1) \boldsymbol{\omega}^{(-k)}$$
  write this out...you get
  $$\begin{pmatrix}
  10 \\
  -2 + 2i\\
  -2 \\
  -2 - 2i \\
\end{pmatrix}$$
  So the eigen values of $A$ are $(10, -2+2i, -2, -2-2i)$. 


  \section{Higher Dimensional Fourier Transform}
  
  ie: Fourier transforms of functions of several variables $(x_1, x_2,\ldots)$.
  Applications: spectral analysis/signal processing for imaging (where, ie: $f(x_1, x_2)$ is intensity from black to white at point $(x_1, x_2)$ is the is the spatial description of an image)

  2D Fourier transform
  component parts are the 2D complex exponentials.
  think in terms of vectors.
  
  Recall, the 1d Fourier transform
  $$\mathscr{F}\!f(s) = \int\limits_{-\infty}^\infty e^{-2\pi ist} f(t) \,d t$$
  where $f$ is a function of $t$ and $\mathscr{F}\!f$ is a function of $s$.

  Then in 2d
  $$x = (x_1, x_2)$$ spatial variables.
  $$\xi = (\xi_1, \xi_2)$$ frequency variables.
  The function is $$f(x) = f(x_1, x_2)$$.
  The Fourier transform is $$\mathscr{F}\!f(\xi) = \mathscr{F}\!f (\xi_1, \xi_2)$$.
  The $st$ in the complex exponential becomes $x \dot \xi = x_1 \xi_1 + x_2 \xi_2$, ie:
  $$e^{-2\pi i (x \dot \xi)} = e^{-2\pi i (x_1 \xi_1 + x_2 \xi_2)}$$
  The Fourier transform is
  $$\mathscr{F}\!f(\xi) = \int\limits_{\mathbb{R}^2} e^{-2\pi i (x \dot \xi)} f(x) \,d x$$
  $$\mathscr{F}\!f(\xi_1, \xi_2) =
  \int\limits_{-\infty}^\infty\int\limits_{-\infty}^\infty e^{-2\pi i(x_1 \xi_1 + x_2 \xi_2)} f(x_1, x_2) \,d x_1 \,d x_2$$

  In $n$-dimensions (skipping the mathematical motivation)
  $$x = (x_1,\ldots, x_{n})$$
  $$\xi = (\xi_1,\ldots, \xi_{n})$$
  $$x \cdot \xi = x_1 \xi_1 + \cdots + x_n \xi_n$$
  $x$ has units of length, so the $\xi_i$ have dimension $1/length$.
  The Fourier transform is
  $$\mathscr{F}\!f(\xi) = \int\limits_{\mathbb{R}^{^n}} e^{-2\pi i (x \cdot \xi)} f(x) \,d x$$
  The inverse Fourier transform is
  $$\mathscr{F}^{-1}\!g(\xi) = \int\limits_{\mathbb{R}^{^n}} e^{2\pi i (x \cdot \xi)} g(\xi) \,d \xi$$
  Again there is a reciprocal relationship between the spatial domain and the frequency domain.


  To consider $$e^{\pm 2\pi i (x\cdot\xi)}$$ geometrically

  first consider the 1d case
  $$e^{2\pi ist}$$
  fixing the frequency $s$, then it is periodic with period $1/s$
  $$e^{2\pi is (t + {1\over s})}$$
  $$= e^{2\pi ist} e^{2\pi is {1\over s}}$$
  $$= e^{2\pi ist}$$
  will be equal to 1 at the points $t = 0, \pm {1\over s}, \pm {2\over s},\ldots$ spaced $1\over s$ apart.
  
  now consider the 2d case
  fixing $\xi_1$ and $\xi_2$ when is the function
  $$e^{-2\pi i (x_1 \xi_1 + x_2 \xi_2)}$$
  equal to $1$?
  $$x_1 \xi_1 + x_2 \xi_2 = n$$
  $$n = 0, \pm 1, \pm 2,\ldots$$
  Gives a family of evenly spaced parallel lines spaced $1/||\xi||$ apart.
  ie: 
  $$x_1 \xi_1 + x_2 \xi_2 = 0$$
  Is (recall analytic vector geometry) a line through the origin with normal vector $\xi$.
  $$x_1 \xi_1 + x_2 \xi_2 = 1$$
  Is a line not through the origin, parallel to the previous, with normal vector $\xi$.
  How far apart are these lines?
  $$|| x || \cos\theta$$
  $$x\cdot\xi = ||x|| ||\xi|| \cos\theta$$
  $$||x||\cos\theta = {x\cdot\xi\over ||\xi||}$$
  $$ = {1\over ||\xi||}$$
  (Figure at min 45 Lect26)
  so
  $$e^{-2\pi i (x_1 \xi_1 + x_2 \xi_2)} = 1$$
  along each of these lines
  , \underline{is periodic in the direction $\xi$ with period $1/||\xi||$}.
  High frequency (fast oscillation of the complex exponential (in a specific direction)) means the magnitude of $||\xi||$ is large, so $1/||\xi||$ is small.
  Low frequency (slow oscillation) means the magnitude of $||\xi||$ is small, so $1/||\xi||$ is large.
  
  \subsection{The Fourier Transform of Separable Functions}
  %Lect 27:
 
  Special case(s):
  Class of examples for which a higher dimesional transform can be computed via lower (typically 1d) dimensional transforms, these are separable functions.
  $$f(x_1,\ldots, x_n) = f(x_1)\cdot\cdots\cdot f(x_n)$$
  $$\mathscr{F}\!f(\xi_1,\ldots,\xi_n) =
	\mathscr{F}\!f(\xi_1)\cdot\cdots\cdot\mathscr{F}\!f(\xi_n)$$

  Example: The 2d boxcar function $\Pi(x_1, x_2)$ (a box of height 1 coming out of the plane)
  $$\Pi(x_1, x_2) = 
     \begin{cases}
	     1 & \text{, $|x_1| < 1/2$} \\
	     1 & \text{, $|x_2| < 1/2$} \\
	     0 & \text{otherwise}
     \end{cases}$$
  which can be written as the product of 1d boxcar functions
  $$\Pi(x_1) \Pi(x_2)$$
  $$\mathscr{F}\!f(\xi_1, \xi_2) = \int\limits_{-\infty}^\infty\int\limits_{-\infty}^\infty e^{-2\pi i (x_1\xi_1 + x_2\xi_2)} \Pi(x_1, x_2) \,d x_1 \,d x_2$$
  $$= \int\limits_{-\infty}^\infty \int\limits_{-\infty}^\infty e^{-2\pi i x_1\xi_1} e^{-2\pi i x_2\xi_2} \Pi(x_1) \Pi(x_2) \,d x_1 \,d x_2$$

  $$= \int\limits_{-\infty}^\infty e^{-2\pi i x_1\xi_1} \Pi(x_1) \,d x_1 
      \int\limits_{-\infty}^\infty e^{-2\pi i x_2\xi_2} \Pi(x_2) \,d x_2$$
  $$\sinc(\xi_1)\sinc(\xi_2)$$
  
  Example: 2d Gaussian
  $$g(x_1, x_2) = e^{-\pi(x_1^2 + x_2^2)}$$
  is separable
  $$= e^{-\pi x_1^2} e^{-\pi x_2^2}$$
  The Fourier transform is
  $$\mathscr{F}\!g(\xi_1,\xi_2) = \mathscr{F}\!g(\xi_2) \mathscr{F}\!g(\xi_2)$$
  $$= e^{-\pi(x_1^2 + x_2^2)}$$

  The central limit theorem also holds in higher dimensions.


  \subsection{The Fourier Transform of Radial Functions}

  Radial functions have symmetry in the radial or polar coordinates, which makes them more natural to work with in polar coordinates, ie: those that only depend on the distance $r$ from the origin.
  \underline{The Fourier transform of a radial function is also a radial function}.

  The Gaussian is an example of a ``radial'' function, 
  Introduce polar coordinates 
  $$f = \sqrt{x_1^2 + x_2^2}$$
  $$\theta = \tan^{-1}\biggl({x_2\over x_1}\biggr)$$
  $$x_1 = r \cos\theta$$
  $$x_2 = r \sin\theta$$
  $$e^{-\pi(x_1^2 + x_2^2)} = e^{-\pi r^2}$$
  depends only on $r$ not on $x_1$ and $x_2$ indipendently.

  Convert the Fourier transform to a ``radial'' form.
  Notice, the Fourier transform and the DFT are cartesian by nature.
  (However there are many cases where the function is naturally described in polar form)
  Assume that $f(x)$ is a radial function.
  $$\int\limits_{-\infty}^\infty\int\limits_{-\infty}^\infty e^{-2\pi i(x_1 \xi_1 + x_2 \xi_2)} f(x_1, x_2) \,d x_1 \,d x_2$$
  Convert to polar coordinates
  $$x_1 = r \cos\theta$$
  $$x_2 = r \sin\theta$$
  $$\xi_1 = \rho\cos\phi$$
  $$\xi_2 = \rho\sin\phi$$
  Will see: If $f$ is a functio of $r$ then the Fourier transform is a function of $\rho$.
  $$f(x_1, x_2) \longrightarrow f(r)$$
  $$\,d x_1 \,d x_2 \longrightarrow r \,d r \,d \theta$$
  How to perform inner product (what is meant by the F.T. being cartesian)?
  $$x_1 \xi_1 + x_2 \xi_2$$
  $$= r\cos\theta\rho\cos\phi + r\sin\theta\rho\sin\phi$$
  $$= r\rho(\cos\theta\cos\phi + \sin\theta\sin\phi$$
  $$= r\rho(\cos(\theta - \phi))$$
  $$\mathscr{F}\!f(x) = \int\limits_0^\infty \int\limits_0^{2\pi} e^{-2\pi i r\rho\cos(\theta - \phi)} f(r) r \,d r \,d \theta$$
  Note: 
  $$\int\limits_0^{2\pi} e^{-2\pi i r\rho\cos(\theta - \phi)} \,d \theta$$
  $$= \int\limits_0^{2\pi} e^{-2\pi i r\rho\cos\theta} \,d \theta$$
  is independent of $\phi$, given the integration is over a complete period.
  However, this is not an elementary integral, there is no formula for the antiderivative that enables you to evaluate the integral in closed form.
  ...So define
  $$J_0(a) = {1\over 2\pi} \int\limits_0^{2\pi} e^{-ia\cos\theta} \,d \theta \quad\text{where $a$ is real}$$
  is the zeroth order Bessel function of the first kind.
  Bessel functions always come up in context of radial functions.
  So we have
  $$2\pi J_0(2\pi r\rho)$$
  and
  $$F(\rho) = 2\pi \int\limits_0^\infty f(f) J_0(2\pi r\rho) r \,d r$$
  this is called the zeroth order Hankel transform.

 
  \subsection{convolution in n-dim case}
  $$(f * g) (x) = \int\limits_{\mathbb{R}^{^n}} f(y) g(x-y) \,d y$$
  In the 2d case
  $$(f * g) (x_1, x_2) = \int\limits_{-\infty}^\infty \int\limits_{-\infty}^\infty f(y_1, y_2) g(y_1 - x_1) g(y_2 - x_2) \,d y_1 \,d y_2$$
  all formulas, formulas, interpretations continue to apply.
  Importantly, the convolution theorem still holds
  $$\mathscr{F}(f * g) = (\mathscr{F}\!f) (\mathscr{F}\!g)$$
  $$\mathscr{F} \mathscr{F}(f * g) = (\mathscr{F}\!f) * (\mathscr{F}\!g)$$ (???)
  (B.Osgood uses this in the derivation of the sampling theorem, but I rejected this derivation...might have to go back to that...Prove this claim and check above derivation! Try to find derivation of convolution theorem for Fourier transforms)

  \subsection{Shift Theorem in $\mathbb{R}^{^n}$}

  Recall, 1d version
  $$f(t) \longleftrightarrow F(s)$$
  $$f(t-b) \longleftrightarrow e^{-2\pi isb}F(s)$$

  In 2d, shift each varible independently.
  $$x_1 \longrightarrow x_1 - b_1$$
  $$x_2 \longrightarrow x_2 - b_2$$
  $$f(x_1, x_2) \longrightarrow f(x_1 - b_1, x_2 - b_2)$$
  $$\int\limits_{-\infty}^\infty \int\limits_{-\infty}^\infty
	e^{-2\pi i (x_1 \xi_1 + x_2 \xi_2)} f(x_1 - b_1, x_2 - b_2) \,dx_1\,dx_2$$
  (easy) change of variables
  $$u_1 = x_1 - b_1$$
  $$u_2 = x_2 - b_2$$
  $$\,du_1\,du_2 = \,dx_1\,dx_2$$
  $$\int\limits_{-\infty}^\infty \int\limits_{-\infty}^\infty
	e^{-2\pi i ((u_1+b_1)\xi_1 + (x_2+b_2)\xi_2)} f(u_1, u_2) \,du_1\,du_2$$
  $$= \int\limits_{-\infty}^\infty \int\limits_{-\infty}^\infty
	e^{-2\pi i (u_1+b_1)\xi_1}\, e^{-2\pi i (x_2+b_2)\xi_2} \,f(u_1, u_2) \,du_1\,du_2$$
  $$= e^{-2\pi i(b_1\xi_1 + b_2\xi_2)}
	\int\limits_{-\infty}^\infty \int\limits_{-\infty}^\infty
	e^{-2\pi i u_1\xi_1}\, e^{-2\pi i b_2\xi_2} f(u_1, u_2) \,du_1\,du_2$$
  $$= e^{-2\pi i(b_1\xi_1 + b_2\xi_2)} \mathscr{F}\!f(\xi_1, \xi_2)$$
  $$b = (b_1, b_2)$$
  $$\xi = (\xi_1, \xi_2)$$
  $$f(x_1-b_1, x_2-b_2 = f(x-b)$$
  $$f(x) \longleftrightarrow F(\xi)$$
  $$f(x-b) \longleftrightarrow e^{-2\pi i \xi \cdot b} F(\xi)$$

  \subsection{Scaling Theorem in $\mathbb{R}^{^n}$}

  1d case
  $$f(t) \longleftrightarrow F(s)$$
  reciprocal relationship
  $$f(at) \longleftrightarrow {1\over |a|} \, F\Bigl({s\over a}\Bigr)$$

  2d case
  $$f(x_1, x_2)$$
  scale $x_1$ and $x_2$ independently, reciprocal relationship
  $$f(a_1x_1, a_2x_2) \longleftrightarrow {1\over |a_1|}{1\over |a_2|} \, F\Bigl({\xi_1\over a_1}, {\xi_2\over a_2}\Bigr)$$
  extends naturally to higher dimension.

  More general notion of scaling 
  $$x_1 \rightarrow a x_1 + b x_2$$
  $$x_2 \rightarrow c x_1 + d x_2$$
  ie: Matrix multiplication
$$\begin{pmatrix}
  x_1 \\
  x_2 \\
\end{pmatrix}
\longrightarrow
\begin{pmatrix}
  a & b \\
  c & d \\
\end{pmatrix}
\begin{pmatrix}
  x_1 \\
  x_2 \\
\end{pmatrix}$$

  So $$f(x) \longrightarrow f(Ax)$$ what happens to the Fourier transform?
  The Fourier transform of $f(Ax)$ is
  $$\mathscr{F}\!f(Ax) = \int\limits_{\mathbb{R}^{^n}} e^{-2\pi i (x\cdot\xi)} f(Ax) \,dx$$
  change variable $$u = Ax$$
  variables are not independent (are coupled)
  a linear change of variables the volume scales by the absolute value of the determinant
  $$d\vec{u} = |det(A)| \,d\vec{x}$$
  What about the complex exponential?
  What happens to $x\cdot\xi$?
  $$u = A x$$
  $$x\cdot\xi = \bigl(A^{-1} u\bigr)\cdot\xi$$
  There is a general relationship between matrix multiplication and the inner product.
  $$B x \cdot y = x\cdot B^T y$$
  (eg EE263: Reference to another course by B.Osgood)
  Write the inverse transpose as
  $$(A^{-1})^T = A^{-T}$$
  $$A^{-1} u\cdot\xi = u\cdot A^{-T}\xi$$
  Plug into the Fourier transform
  $$\int\limits_{\mathbb{R}^{^n}} e^{-2\pi i(u\cdot A^{-T}\xi)} f(u) {1\over |det(A)|}\,du$$
  $$= {1\over |det(A)|} \int\limits_{\mathbb{R}^{^n}} e^{-2\pi i(u\cdot A^{-T}\xi)} f(u) \,du$$
  $$= {1\over |det(A)|} \mathscr{F}\!f\bigl(A^{-T}\xi\bigr)$$
  The stretch theorem
  $$f(x) \longrightarrow F(\xi)$$
  $$f(a x) \longrightarrow {1\over |det(A)|} F(A^{-T} \xi)$$

  Special cases
  
  Scaling the coodinates
  $$x_1 \longrightarrow a_1 x_1$$
  $$x_2 \longrightarrow a_2 x_2$$
  the matrix is given by 
$$A = \begin{pmatrix}
  a_1 & 0 \\
  0   & a_2 \\
\end{pmatrix}$$
$$A^{-1} = \begin{pmatrix}
  1/a_1 & 0 \\
  0   & 1/a_2 \\
\end{pmatrix}$$
$$A^{-T} = \begin{pmatrix}
  1/a_1 & 0 \\
  0   & 1/a_2 \\
\end{pmatrix}$$
  leads to earlier results.

  coodinates under rigid rotation in 2d
$$ A = \begin{pmatrix}
  \cos\theta & -\sin\theta \\ 
  \sin\theta & \cos\theta \\ 
\end{pmatrix}$$
  is orthogonal
  $$A^T A = I$$
  because
  $$\det(A^T A) = det(I) = 1$$
  $$(det(A^T)) (det(A))$$
  $$(det(A)^2)$$
  So
  $$|(det(A))| = 1$$
  $A$ preserves volumns.
  Volumes are distorted under a linear transformation by the absolute value of the determinant.
  $$A^T A = I$$
  $$A^T = A^{-1}$$
  $$(A^{-1})^T = A$$
  $$A^{-T} = A$$
  Plut into formula
  $$f(A x) \longrightarrow {1\over |det(A)|} F (A^{-T} \xi$$
  $$= F(A \xi)$$
  $$f(A x) \longrightarrow F(A\xi)$$
  rotation of the spatial domain corresponds to the same rotation of the requency domain.
  Physical interpretation: rotating an image $\sim$ is the same as rotating its spectrum.
  $$f(x) \longleftrightarrow F(\xi)$$
  $$f(A x) \longleftrightarrow {1\over |det(A)|} F(A^{-T} \xi)$$
  In higher dimensions, reciprocal relationship is reinterpreted as inverse transponse, $A^{-T}$.
  
  Higher order delta Functions
  $$\langle\delta, \varphi\rangle = \varphi(0)$$
  $$\varphi(x_1,\ldots, x_n)$$
  $$0 = (0,\ldots, 0)$$
  The Fourier transform is
  $$\mathscr{F}\delta = 1$$

  Shifted delta function
  $$\delta(x-b) = \delta_b$$
  $$\langle\delta_b, \varphi\rangle = \varphi(b)$$
  $$b = (b_1,\ldots, b_n)$$
  The Fourier transform is
  $$\mathscr{F}\delta_b = e^{-2\pi i b\cdot\xi}$$

  $$f \delta = f(0) \delta$$
  $$f \delta_b = f(b) \delta_b$$

  Scaling properties of Delta
  The 1-d case
  $$\delta(a x) = {1\over |a|} \delta(x)$$

  The n-d case
  $$\delta(A x) = {1\over |det(A)|} \delta(x)$$
  $A$ a matrix.
  

  %Lect29
  $\Sh$'s, lattices, and crystallograhy.
  Physical example of where ``reciprocal'' means inverse transpose.
  Transform coodinates by a nonsingular matrix $A$
  $$\mathscr{F}(f(A x)) = {1\over |det(A)|} \mathscr{F}\!f(A^{-T} \xi)$$

  \subsection{The Higher-Order $\Sh$ Function}

  Recall, for integer spacing in 1-d
  $$\mathscr{F}\Sh = \Sh$$

  And, for for non-integer spacing
  $$\Sh_p(x) = \sum_{k=-\infty}^\infty \delta(x - kp)$$
  the Fourier transform is
  $$\mathscr{F}\Sh_p = {1\over p} \Sh_{1/P}$$
  important reciprocal relation.

  The 2-d $\Sh$ function.
  For evenly spaced points in $\mathbb{R}^2$ consider a grid in the plane with points at ingeter coordinates, aka the ingeger lattice, $\mathbb{Z}^2$.
  $$\Sh_{\mathbb{Z}^2} (x) = \sum_{k \in \mathbb{Z}^2} \delta(x - k)$$
  where $k = (k_1, k_2)$ and $k_1$, $k_2$ are integers.
  The Fourier transform follows in the same way as the 1-d case
  $$\mathscr{F}\Sh_{\mathbb{Z}^2} = \Sh_{\mathbb{Z}^2}$$

  For non-integer spacing the 2-d $\Sh$ function is
  Spacing is modified by applying a (non-singular) $2\times 2$ matrix to the integer lattice.
  Integer combinations of $e_1$, $e_2$ under linear transformation by $A$ to go integer combination s of basis vectors of $v_1$, $v_2$.
  The result of $A(\mathbb{Z}^2)$ is an oblique lattice
  $$\mathfrak{L} = \{ k_1 v_1 + k_2 v_2 \big| k_1, k_2 \in \mathbb{Z}\}$$
  where
  $$v_1 = A e_1 \quad\text{,} v_2 = A e_2$$
  ...speak about th area of the fundamental cell (parallelogram given by the basis vectors) in the lattice.
  $$Area(\mathfrak{L} = \text{area of parallelogram}$$
  The $\Sh$ function on an oblique lattice is
  $$\Sh_{\mathfrak{L}}(x) = \sum_{p \in \mathfrak{L}} \delta(x - p)$$
  The Fourier transform is
  $$\mathscr{F} \Sh_{\mathfrak{L}} = $$

  $$\mathfrak{L} = A(\mathbb{Z}^2)$$
  The reciprocal lattice
  $$\mathfrak{L}^* = A^{-T}(\mathbb{Z}^2)$$
  The area of $\mathfrak{L}^*$ is the reciprocal of the area of the other lattice.
  $$Area(\mathfrak{L}^*) = {1\over Area(\mathbb{Z})}$$

  The Fourier transform
  $$\mathscr{F}\Sh_{\mathfrak{L}} = {1\over Area(\mathfrak{L})} \Sh_{\mathfrak{L}^*}$$
  involves the dual lattice

  In higher dimensions...(USE THE GENERALIZED STRETCH THEOREM)
  $$\mathscr{F}\Sh_{\mathfrak{L}} = {1\over |det(?)|} \Sh_{\mathfrak{L}^*}$$
  involves the dual lattice

  For a crystal
  Recall, it is the {\em electron density} for the atoms in a crystal that perform the measurement in an X-ray diffraction experiment.
  periodized version via convolve with $\Sh$
  $$\rho_{\mathfrak{L}}(x) = \rho(x) * \Sh_{\mathfrak{L}}$$
  $$= \sum_{p \in \mathfrak{L}} \rho(x - p)$$
  The X-ray diffraction experiment produces the (magnitude) of the Fourier transform of $\rho$ the lattice
  $$\mathscr{F}(\rho_{\mathfrak{L}}) = (\mathscr{F}\rho) (\mathscr{F} \Sh_{\mathbb{L}})$$
  $$= (\mathscr{F}\rho) ({1\over |\det(\mathfrak{L})|} \Sh_{\mathbb{L^*}})$$
  $$= \sum_{p^* \in \mathfrak{L}^*} (\mathscr{F}\rho)(s) \, \delta(s - p^*)$$
  $$= \sum_{p^* \in \mathfrak{L}^*} (\mathscr{F}\rho)(p^*) \, \delta(s - p^*)$$
 
  \subsection{Fundamental Problem of Tomography} 

  Application to medical imaging (not MRI).

  Density on 2-d surface/slice/whatever
  $$\mu(x_1, x_2)$$
  Apply X-ray through a region along a line and measure what 
  know the intensity of X-ray upon entry $I_0$ and measure the intensity at exit $I$
  (derived in text)
  $$I = I_0 e^{-\int\limits_L \mu}$$
  where 
  $$\int\limits_L \mu$$
  is the line integral along the line $L$.
  You don't know $\mu$ or $\int\limits_L \mu$...
  Given many measurements one obtains values for all possible lines $L$.
  To recover $\mu$ consider the measurement values as given by a transform of $\mu$ evaluated along the the line $L$.
  {\bf The Radon transform} of $mu$ is
  $$(\mathcal{R})(\mu)(L) = \int\limits_{L} \mu$$
  get $\mu$ by taking the inverse Radon transform.
  Answer in terms of the 2-d Fourier transform.

  \subsection{Tomography and inverting the Radon transform} 
  %Lect30
 
  Can you find $\mu$ given all values of its transform $\mathcal{R}_\mu (L)$?
  aka can we invert $\mathscr{R}$

  Introduce coodinates that describe a family of lines in the plane.
  eg: given the equation of a line
  $$y = mx + b$$
  $(m, b)$ describes the line
  So the set of coodinates $(m, b)$
  $$-\infty < b < \infty$$
  $$m \le m < \infty$$
  is no good since it omits the vertical lines.
  A better set of coodiantes is one that describes families of parallel lines, ie: all with the same normal vector
  $$\hat{n} = (\cos\phi, \sin\phi)$$
  $$0 \le \phi < \pi$$
  where $\phi$ is the angle wrt the abscisa.
  So a line is determined by the angle $\phi$ and the (minimum) signed distance from the origin $\rho$.
  $$-\infty < \rho < \infty$$
  So a set of parallel lines is given by a fixed $\phi$ while $\rho$ may vary
  (these are not polar coordinates in the plane).
  The cartesian equation of a particular line in these coordinates is in vector form
  $$x\cdot\hat{n} = \rho$$
  where
  $$n = (\cos\phi, \sin\phi)$$
  In coodinate form
  $$\rho = x_1 \cos\phi + x_2 \sin\phi$$
  
  Use this to evaluate a line integral $$\int\limits_L \mu$$ via a $\delta$-function.
  Fact:
  consider a $\delta$ function along a line parametrized as described as
  $$\delta(\rho - x_1 \cos\phi - x_2 \sin\phi) = 
		\begin{cases}
			0 & \text{, off the line} \\
			\infty & \text{, on the line} \\
		\end{cases}$$ 
  a line-impulse, a $\delta$ function along the line.
  $$\int\limits_L \mu = \int\limits_{\mathbb{R}^{^2}} \mu(x_1, x_2) \delta(\rho - x_1 \cos\phi - x_2 \sin\phi) \,dx_1 \,dx_2$$
  needs some proof but analagous to the 1-d case.

  Use this to invert the Radon transform $\mathscr{R}\mu$
  $$\int\limits_{-\infty}^\infty \int\limits_{-\infty}^\infty
	\mu(x_1, x_2) \delta(\rho - x_1 \cos\phi - x_2 \sin\phi) \,dx_1 \,dx_2$$
  Consider passing a family of parallel lines to the integral via $\rho$ with $\phi$ fixed.
  Then $\mathscr{R}\mu(\rho, \phi)$ becomes a function of $\rho$ with $\phi$ fixed.
  ...Take the 1-d Fourier transform wrt $\rho$
  $$\mathscr{F}_\rho \mathscr{R}\mu(\rho, \phi) = \mathscr{F}_\rho(\mathscr{R}\mu(\rho, \phi))$$
  $$= \int\limits_{-\infty}^\infty e^{-2\pi i r \rho} \mathscr{R}\mu(\rho, \phi) \,d\rho$$
  $$= \int\limits_{-\infty}^\infty e^{-2\pi i r \rho}
	\mathscr{R}\mu(\rho, \phi) \,d\rho$$
  $$= \int\limits_{-\infty}^\infty
	e^{-2\pi i r \rho}
	\Biggl(\:
		\int\limits_{-\infty}^\infty \int\limits_{-\infty}^\infty
		\mu(x_1, x_2) \delta(\rho - x_1 \cos\phi - x_2 \sin\phi) \,dx_1 \,dx_2
	\Biggr) \,d\rho$$
  Change order of integration
  $$= 
	\int\limits_{-\infty}^\infty \int\limits_{-\infty}^\infty
	\mu(x_1, x_2) 
	\Biggl(\:
		\int\limits_{-\infty}^\infty
		e^{-2\pi i r \rho}
		\delta(\rho - x_1 \cos\phi - x_2 \sin\phi) \,d\rho
	\Biggr) \,dx_1 \,dx_2$$
  $$= 
	\int\limits_{-\infty}^\infty \int\limits_{-\infty}^\infty
	\mu(x_1, x_2) 
	\Biggl(\:
		\int\limits_{-\infty}^\infty
		e^{-2\pi i r (x_1\cos\phi + x_2\sin\phi)} \,d\rho
	\Biggr) \,dx_1 \,dx_2$$
  $$= 
	\int\limits_{-\infty}^\infty \int\limits_{-\infty}^\infty
	\mu(x_1, x_2) 
	\Biggl(\:
		\int\limits_{-\infty}^\infty
		e^{-2\pi i (x_1 r\cos\phi + x_2 r\sin\phi)} \,d\rho
	\Biggr) \,dx_1 \,dx_2$$
  Introduce dual variables to $x_1$ and $x_2$, $\xi_1 = r\cos\phi$ and $\xi_2 = r\sin\phi$.
  $$= 
	\int\limits_{-\infty}^\infty \int\limits_{-\infty}^\infty
	\mu(x_1, x_2) 
	\Biggl(\:
		\int\limits_{-\infty}^\infty
		e^{-2\pi i (x_1\xi_1 + x_2\xi_2)} \,d\rho
	\Biggr) \,dx_1 \,dx_2$$
  In sum
  $$\mathscr{F}_\rho(\mathscr{R} \mu(\rho, \phi) =
	\int\limits_{-\infty}^\infty \int\limits_{-\infty}^\infty
	\mu(x_1, x_2) e^{-2\pi i (x_1\xi_1 + x_2\xi_2)} \,dx_1 \,dx_2$$
  $$= \mathscr{F}\mu(\xi_1, \xi_2)$$
  where this last Fourier transform is a genuin 2-d Fourier transform (recall, $\phi$ is being held fixed in the initial statement of the Fourier transform wrt $\rho$).

  Recal:
  Fix $\phi$.
  Let $\rho$ vary.
  Take 1-d Fourier transform of the Radon transform wrt $\rho$.
  This gives the 1-d Fourier transform of $\mu$. 
  Think: measurements return values of
  $$\mathscr{F}_\rho \mathscr{R} \mu(\rho, \phi)$$
  Making the change of variables
  $$\xi_1 = r\cos\phi$$
  $$\xi_2 = r\sin\phi$$
  results in a function of
  $$G(\xi_1, \xi_2) = \mathscr{F} \mu$$
  To get $\mu$ take the inverse Fourier transform
  $$\mu = \mathscr{F}^{-1} G$$

  Practical implementation is done via a discrete version.
  Come back to issues and trade offs of cartesian v. polar representations.

  ***FIX: turn these into subsections... 


  \section{Distributions Continued}

  Excerpts from Robert Strichartz, ``A Guide to Distribution Theory and Fourier Transforms''.

  The thermometer analogy to distributions:
  Rather than measuring the temperature at a point in space (and time), it is more meaningful to consider
  $$\int f(x) \varphi(x) \,dx$$
  where $f(x)$ represents temperature, and $varphi(x)$ corresponds to the device that performs the measurement, which is ``concentrated'' within the local vicinity and tends to zero at sufficient distance from the measuring device.

  The measurement can be thought of as an average in the sense that
  $\varphi(x) \ge 0$
  and the integral over all spaces is
  $$\int \varphi(x) \,dx = 1$$
  Note: this is not a restriction to nonnegative values.
  The difference of two thermometers with arbitrary coefficients
  $$\int f(x) (a_1 \varphi_1(x) - a_2 \varphi_2(x)) \,dx$$
  can take on any value.

  def. {\em generalized functions}, denoted by
  $$\langle f, \varphi \rangle \in \mathbb{R} \text{or} \mathbb{C}$$
  can be real or complex.
  Require ``something akin to $\int f(x) \varphi(x) \,dx$ exists'' for a suitable ``test function''.
  Linearity
  $$a_1 \langle f, \varphi_1 \rangle - a_2 \langle f, \varphi_2 \rangle = \langle f, a_1 \varphi_1 - a_2 \varphi_2 \rangle$$
  Note if $\varphi_1$, $\varphi_2$ are test functions then so is $a_1 \varphi_1 + a_2 \varphi_2$.
  Remaining questions: what constitutes a test function, and what is the continuity hypothesis?
  
  Denote the space of test functions leading to the theory of distributions as, $\mathcal{D}$.
  The ``underlying point set'' is $\mathbb{R}^n$ (or an open subset, $\Omega \subset \mathbb{R}^n$) with elements, $x = (x_1,..., x_n)$, ie:
  ``every point $x \in \Omega$ is surrounded by a ball ${y : |x-y| < \epsilon}$ contained in $\Omega$, where $\epsilon$ depends on $x$, and''  
  $$|x-y| = \sqrt{(x_1 - y_1)^2 + \cdots + (x_n - y_n)^2}$$ 

  ``The class of test functions $\mathcal{D}$ consists of all functions $\varphi(x)$ defined in $\Omega$, vanishing outside a bounded subset of $\Omega$ that stays away from the boundary of $\Omega$, and such that all partial derivatives of all orders of $\varphi$ are continuous.''\cite{Strichartz}
  For $n=1$, $\Omega = {0 < x < 1}$
 

\begin{figure}[H]
\centering
\begin{tikzpicture}
% Axis
\draw[-] (-2,0) node[anchor=north] {0} 
	  -- (2,0)  node[anchor=north] {1};
% Curve
\draw (-2,0) parabola bend (0,2) (2,0);
\end{tikzpicture}
\caption{Is not in $\mathcal{D}$ because it does not vanish near the boundaries.}
\end{figure}

\begin{figure}[H]
\centering
\begin{tikzpicture}
% Axis
\draw[-] (-2,0) node[anchor=north] {0} 
	  -- (2,0)  node[anchor=north] {1};
% Curve
\draw[-] (-1,0) -- (0,1) -- (1,0);
\end{tikzpicture}
\caption{Is not in $\mathcal{D}$ because it is not differentiable at three points.}
\end{figure}

  No analytic function except $\varphi \equiv 0$ can be in $\mathcal{D}$ due to the vanishing requirement.
  Any formula for $\varphi$ must be writen pieces wise, ie: in $\mathbb{R}$
  $$\psi(x) = \begin{cases}
	e^{-1/x^2} & x > 0 \\
	0 & x > 0
  \end{cases}$$
  has continuous derivatives of any order
  $$\Biggl({\,d \over \,dx}\Biggr)^k e^{-1/x^2} = {\text{polynomial in $x$}\over\text{polynomial in $x$}} e^{-1/x^2}$$
  and $\psi(x) \rightarrow 0$ as $x \rightarrow 0$ because $e^{-1/x^2}$ goes to 0 faster than the pole ${1\over\text{polynomial in $x$}}$.
  So, any order derivative of $\varphi(x) = \psi(x) \psi(1-x)$ is continuous, $\varphi(x)$ is $C^\infty$.
  $\varphi(x)$ vanishes outside $0 < x < 1$, so $\varphi \in \mathcal{D}(a < x < b)$ so long as $a < 0$ and $b > 1$ (why not $a \le 0$ and $b \ge 1$? DO THIS?)
  Given one example, others can be generated using normal function modifications
\begin{itemize}
\item Horizontal shift $\varphi(x + x_0)$.
\item Vertical scaling $a \varphi(x)$.
\item Horizontal scaling $\varphi(a x)$.
\item Linear combination $a_1 \varphi_1(x) + a_2 \varphi_2(x)$.
\item Products to generate higher dimensional versions $\varphi(x_1)\varphi(x_2)...\varphi(x_n) = \varphi_1(x_1,\ldots,x_n)$
\end{itemize}
 
  Def. the class of distributions on $\Omega$ is all continuous linear functionals on $\mathcal{D}(\Omega)$, denoted by $\mathcal{D}'(\Omega)$.
  Notation for the {\em functional}
  $$\langle f, \varphi\rangle$$
  {\em Linear} functional
  $$a_1 \langle f, \varphi_1\rangle + a_2 \langle f, \varphi_2\rangle
  = \langle f, a_1 \varphi_1 + a_2 \varphi_2\rangle$$
  Temporary definition of {\em continuous}:
  if $\varphi_1$ is close enough to $\varphi$ then $\langle f, \varphi_1\rangle$ is close to $\langle f, \varphi\rangle$.
  ``linear functionals tend to be continuous...a certain amount of continuity is built into linearity...all linear functionals on $\mathcal{D}(\Omega)$ will be continuous'' (yay!)
  Continuity argument (not proof):
  $$\varphi_1 - \varphi = \varphi_2$$
  $$\varphi_1 = \varphi + \varphi_2$$
  Move $\varphi_1$ closer to $\varphi$ via $\varphi + t \varphi_2$
  By linearity
  $$\langle f, \varphi + t \varphi_2\rangle = \langle f, \varphi\rangle + t \langle f, \varphi_2\rangle$$
  , which as $t$ gets small
  $$= \langle f, \varphi\rangle$$

  AT: 1.2 Examples of distributions 

  Any function can specify a distribution
  $$\langle f, \varphi\rangle = \int_\Omega f(x) \varphi(x) \,dx$$
  provided the integral exists.
  



  \section{Begin Wavelets}

  Strichartz, Robert S "A Guide to Distribution Theory and Fourier Transforms" (2003)

  The Gabor transform, using thge Gaussian as a prototype, can generate all functions frequency and space via ``translations'' in frequency and space.
  Wavelets are a different approach to expand a function in terms of simple component functions that are localized in frequency and space.
  Translations and ``dilations'' of 

  The simplest wavelet is the Haar function
  $$\psi(x) = \begin{cases}
	1  & \text{if}\quad 0 < x \le {1\over 2} \\
	-1 & \text{if}\quad {1\over 2} < x \le 1 \\
	0  & otherwise.
  \end{cases}$$
  The support of $\psi$ is $[0, 1]$.
  Dilate $\psi$ by powers of 2.
  $psi(2^jx)$ is supported on $[0, 2^{-j}$ where $j$ is an integer.
  And translate by $2^{-j}$ times an integer
  $$\psi(2^j (x - 2^{-j} k)) = \psi(2^j x - k)$$
  is supported on $[2^{-j} k, 2^{-j} (k+1)]$.
  With the normalization factor is $2^{j/2}$ the family of functions $2^{j/2}\psi(2^j x - k) = \psi_{j,k}(x)$ is orthonormal
  $$\int \psi_{j,k}(x)\psi_{j',k'}(x)\,dx = \begin{cases}
	1 & \text{if}\quad j = j' \quad\text{and}\quad k = k'\\
	0 & \text{otherwise}
  \end{cases}$$
  If $j=j'$ but $k\ne k'$ then $\psi_{j,k}$ and $\psi_{j',k'}$ have disjoint support, so $(\psi_{j,k})(\psi_{j',k'}) = 0$.
  If $j\ne j'$ but $k=k'$ then $\psi_{j,k}$ and $\psi_{j',k'}$ may have overlapping support, but the integral of $(\psi_{j,k})(\psi_{j',k'})$ is still $0$ via cancelation.

  An arbitrary function can be expanded via the {\em Haar series expansion}
  $$f = \sum_{j=-\infty}^\infty \sum_{k=-\infty}^\infty \langle f, \psi_{j,k}\rangle \psi_{j,k}$$
  Define the system as {\em complete} if the expansion is valid.
  The usual caveats from Fourier series apply
\begin{itemize}
  \item The acceptable functions are restricted to those such that
	$$\int |f(x)|^2 \,dx < 0$$
  \item The infinite series may not converge in the pointwise sense.

  The Haar functions are complete...(p.142)
\end{itemize}










\section{NOTES:}
  (MOVE THIS...)
  A lack of smoothness in the function, or in any of its derivatives, being approximated by a Fourier series forces an infinite number of terms.
  A finite number of terms will be smooth, as it is infinitely differentiable.

  Greater generality requires a different point of view.
  The important condition is integrability of the function.
  If a function is periodic, and square integrable, $ f \in L^2([0, 1])$, if
  $$ \int\limits_0^1 | f(t) |^2 \, d t < \infty $$
  ($f$ has finite energy).
  $$ \int\limits_0^1 \biggl| f(x) - \sum_{k = - n}^n \hat{f}(k) exp( 2 \pi i k t ) \biggr|^2 \, d t \to 0 \; \mathtt{as} \; n \to \infty $$ 
  Can only get such convergence results with a generalization of the integral due to Lebesgue (where the $L$ in $L^2$ comes from, above).

  The transition from Fourier series to the Fourier transform involves the transition from periodic phenomena to non-periodic phenomena.
  View non-periodic function as a limiting case of a periodic function as the period tends to infinity.
  Recall there are two aspects: analysis and sysnthesis
  Analysis: for periodic $f (t)$ then the Fourier coefficients are 
  $$ \hat{f}(K) = \int\limits_0^1 \, d t \exp( - 2 \pi i k t ) f ( t ) $$
  Synthesis: recover $ f ( t ) $ from its constituent components
  $$ f ( t ) = \sum_{ k = - \infty }^\infty \, d t \hat{ f }( k ) \exp ( 2 \pi i k t ) $$
  Fourier transform is the generalization (limiting case, as the period tends to infinity) of the Fourier coefficients. 
  Inverse Fourier transform is the limiting case of Fourier series. 
  Need a setup when $ f ( t ) $ is periodic of period $ T $ and let $ T $ tend to infinity.
  The building blocks for $f( t )$ with period $T$ are
  $$ \exp ( 2 \pi i k t / T ) $$
  such that the Fourier series take the form
  $$ f ( t ) = \sum_{ k = - \infty}^\infty C_k \exp ( 2 \pi i k t / T ) $$
  and the coefficients are got by 
  $$ C_k  = {1 \over T} \sum\limits_0^T \exp ( - 2 \pi i k t / T ) f ( t ) $$
  $$ = {1 \over T} \sum\limits_{-T/2}^{T/2} \exp ( - 2 \pi i k t / T ) f ( t ) $$
  and the spacing between Fourier coefficients (the frequency spectrum) is $1 / T$
  Note: the reciprocal relationship between the period (time domain) and the frequencies.
  As the period $T$ tends to infinity the spectrum becomes continuous.
  Recall we are attempting to allow the period to tend to infinity and transition from periodic to non-periodic phenomena.
  BUT, assume the function goes to zero within the the specified boundary $T$, then the Fourier coefficients go to zero due to the factor of $1/T$! ie:
  $$ C_k \to 0 $$
  How to save this?
  Scale up by $T$.
  $$ \mathfrak{f} f \bigl( {k \over T} \bigr) = \int\limits_{-T/2}^{T/2} \, d t \exp ( - 2 \pi i ( k / T ) t ) f ( t ) $$
  Fourier series looks like


  
  $$ f ( t ) = \sum_{k = - \infty}^{infty} \mathfrak{f} f \bigl( {k \over T} \bigr) \exp ( 2 \pi i ( k / T ) t ) {1 \over T} $$


\begin{thebibliography}{12}
\bibitem{DJG}%dynamic indexing using \cite{cite_key}
  Griffiths, D.J.  
  \emph{Introduction to Quantum Mechanics}, Ed. 2.
  Prentice Hall, New Jersey,
  2005.

\bibitem{B&B}
  Bekefi, G. \& Barrett A.H.
  \emph{Elecromagnetic Vibrations, Waves, and Radiation}.
  MIT Press, Massachusetts
  1977.

\bibitem{Folland}
  Folland, G.B.
  \emph{Fourier Analysis and its Applications}.
  Wadsworth \& Brooks/ Cole Advanced Books in Software, California,
  1992.

\bibitem{Osgood}
  Osgood, B.
  \emph{Lecture on the Fourier Transform and its Applications}
  AMS,
  2019.

% best technical/practical DFT ref
\bibitem{BriggsHenson}
  Briggs, W.L, \& Henson V.E.
  \emph{The DFT, An Owner's Manual}
  siam,
  1995.

\bibitem{HensonPhD}
  Henson, V.E.
  \emph{Fourier Methods of Image Reconstruction}
  PhD thesis, University of Colorado at Denver,
  1990.

\bibitem{HensonReport}
  Henson, V.E.
  \emph{DFTs on irregular grids: The anterpolated DFT}
  Technical Report NPS-MA-92-006, Naval Postgraduate School, Monterey, CA, 
  1992.

\bibitem{HensonReport}
  Henson, V.E.
  \emph{An extended Poisson sum formula for the generalized integral transform and aliasing error bound for the sampling theorem}
  J. Appl. Anal, 26 (1988) 
  1988.

\bibitem{Bracewell}
  Bracewell, R.N.
  \emph{The Fourier Transform and its Applications, Ed. 3}.
  McGraw-Hill,
  2000.

\bibitem{Bracewell2}
  Bracewell, R.N.
  \emph{Fourier Analysis and Imaging}.
  Springer,
  2003.

\bibitem{Hamming}
  Hamming, R.W.
  \emph{Digital Filters, Ed. 3}.
  Dover,
  1997.

\bibitem{OppenheimSchafer}
  Oppenheim, A.V, \& Schafer R.E.
  \emph{Discrete-Time Signal Processing, Ed. 3}.
  Pearson,
  2010.

\bibitem{PercivalWalden1993}
  Percival, D.B, \& Walden A.T.
  \emph{Spectral Analysis for Physical Applications (Multitaper And Conventional Univariate Techniques)}.
  Cambridge University Press,
  1993.

\bibitem{PercivalWalden2000}
  Percival, D.B, \& Walden A.T.
  \emph{Wavelet Methods for Time Series Analysis}.
  Cambridge University Press,
  2000.

\bibitem{PercivalWalden2020}
  Percival, D.B, \& Walden A.T.
  \emph{Spectral Analysis for Univariate Time Series}.
  Cambridge University Press,
  2020.

% Amazing ref for DSP signals + systems, notes by MIT TA
\bibitem{Karu}
  Karu, Z.Z
  \emph{Signals and Systems Made Ridiculously Simple}.
  ZiZi Press,
  1995.

% Highly recommended for understanding the fundamentals
\bibitem{SteinShakarchi}
  Stein, E.M, \& Shakarchi R.
  \emph{Fourier Analysis An Introduction}.
  Princeton University Press,
  2011.

% vital reference difficult to get...
\bibitem{Jerri}
  Jerri, A.J.
  \emph{Integral and Discrete Transforms with Applications and Error Analysis}.
  CRC Press,
  1992.

\bibitem{Jerri2011}
  Jerri, A.J.
  \emph{Advances in the Gibbs Phenomenon}.
  Sampling Publishing,
  2011.

% What the title says...
\bibitem{Strichartz}
  Strichartz, R.S.
  \emph{A Guide to Distribution Theory and Fourier Transforms}.
  CRC Press,
  1994.

\bibitem{Papoulis}
  Papoulis, A. \& Pillai, S.U.
  \emph{Probability, Random Variables, and Stochastic Processes}.
  McGraw Hill,
  2002.

\bibitem{Lighthill}
  Lighthill, M.J.
  \emph{An Introduction to Fourier Analysis and Generalised Functions}.
  Cambridge Monographs on Mechanics,
  1958.

\bibitem{VanLoan}
  Van Loan, C.
  \emph{Computational Frameworks for the Fast Fourier Transform}.
  SIAM,
  1992.


\bibitem{Weaver} % might be a good ref. but haven't seen it.
  Weaver, H.J.
  \emph{Applications of Discrete and Continuous Fourier Analysis}.
  Wiley-Interscience, New York,
  1983.

\bibitem{Brigham}
  Brigham, E.
  \emph{The Fast Fourier Transform and it's Applications}.
  Prentice Hall,
  1988.

\bibitem{Chu}
  Chu, E.
  \emph{Discrete and Continuous Fourier Transforms: Analysis, Applications and Fast Algorithms}.
  CRC Press,
  1988.

\bibitem{Mallet}
  Mallet, S.G.
  \emph{A wavelet tour of signal processing: the sparse way}.
  
\bibitem{wikipedia}
  Wikipedia.
  \emph{Nyquist–Shannon sampling theorem},
  https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem,
  accessed 4/15/2022.

\bibitem{NumericalRecipes} % might be a good ref. but haven't seen it.
  Press, W.H., Teukolsky, S.A., Vetterling, W.T. \& Flannery B.P.
  \emph{Numerical Recipes: The Art of Scientific Computing, 3rd Ed}.
  Cambridge University Press,
  2007.

\end{thebibliography}

\end{document}  
